[
  {
    "path": "posts/2021-03-09-julia-mle/",
    "title": "Maximum Likelihood Estimation with Julia",
    "description": {},
    "author": [
      {
        "name": "Hok Chio (Mark) Lai",
        "url": {}
      }
    ],
    "date": "2021-03-09",
    "categories": [
      "Julia",
      "Statistics"
    ],
    "contents": "\n\nContents\nLoading Packages\nSimple Bernoulli Example\nGrid Search\nGradient Descent/Ascent\nLog-Likelihood\n\nNewton’s Method\nUsing Established Algorithms with Optim.jl\nFit MLE\n\nRegression\nSample Data\nWith Optim.jl\nUsing JuMP.jl\nSufficient Statistics\n\n\n\nFor a statistical model of data \\(\\mathbf{y}\\) written as \\(f(\\mathbf{y}; \\boldsymbol{\\mathbf{\\theta}})\\) with parameters \\(\\boldsymbol{\\mathbf{\\theta}}\\), the likelihood function, \\(\\mathcal{L}(\\boldsymbol{\\mathbf{\\theta}}; \\mathbf{y})\\), is the probability of the data given the parameter values.\nLoading Packages\nusing Plots, Distributions, Random\nusing Optim\nusing NLSolversBase, ForwardDiff, FiniteDifferences\nusing HypothesisTests\nusing Printf\nusing LinearAlgebra\nSimple Bernoulli Example\nAs a simple example, consider a model where \\(Y\\) is assumed to follow a Bernoulli distribution with probability of success equals \\(\\theta\\). Assume that there are three observed values \\(\\mathbf{y} = [0, 1, 0]'\\). You can think about flipping a coin (which may or may not be fair) three times, and call it a success if the coin shows a head. The model further assumes that each coin flip is independent. Therefore, if the success probability is \\(\\theta\\), then the probability of observing \\(\\mathbf{y} = [0, 1, 0]'\\) is \\[\\mathcal{L}(\\theta; y_1 = 0, y_2 = 1, y_3 = 0) = (1 - \\theta) \\times \\theta \\times (1 - \\theta) = \\theta(1 - \\theta)^2,\\] which is our likelihood function. Note that \\(\\theta \\in [0, 1]\\) We can write the function in Julia\n# Note that it returns zero when theta is out of range\nlik(θ) = (0 <= θ <= 1) ? θ * (1 - θ)^2 : 0\n# Try theta = 0.5\nlik(0.5)\n# Try theta = 0.3\nlik(0.3)\n# Try theta = -0.1\nlik(-0.1)\n0\nWe can plot the likelihood function [need help to annotate it]\nplot(lik, 0, 1)\n\nThe maximum point of the curve is the maximum likelihood estimate (MLE), usually denoted as \\(\\hat \\theta\\). From eyeballing we can see \\(\\hat \\theta \\approx 1 / 3\\). As you probably can guess, the MLE is the sample mean.\nGrid Search\nLet’s write a Julia program to find the MLE. One brute force method is to try discrete values, say 0, 0.001, 0.002, up to 1:\n# Write a function that returns the maximum likelihood value and the estimate\n# Need to make the lower bound, the upper bound, and the grid size as parameters\nmy_find_mle = function(lik, grid_size)\n    thetas = 0:grid_size:1\n    lik_vals = [lik(i) for i in thetas]\n    max_lik, max_idx = findmax(lik_vals)\n    return (lik = max_lik, mle = thetas[max_idx])\nend\nmy_find_mle(lik, .00001)\n(lik = 0.148148148137037, mle = 0.33333)\nGradient Descent/Ascent\nAn algorithm to minimize a function \\(f\\) is to iteratively do \\[\\theta_{s + 1} = \\theta_s + \\gamma f'(\\theta_s),\\] where \\(\\gamma\\) is a step size/learning rate. We need to first to find the derivative. It can be done numerically, like\nlik_prime(θ; h = 1e-5) = (lik(θ + h) - lik(θ)) / h\n# lik_prime(0.5)\nlik_prime (generic function with 1 method)\n\nNote: One can also use the central difference method, which is more accurate\n\nstep_size = 0.6\nθ₀ = 0.5\nlik0 = lik(θ₀)\ndel0 = lik_prime(θ₀)\nθ₁ = θ₀ + step_size * del0\ndel1 = lik_prime(θ₁)\nθ₂ = θ₁ + step_size * del1\n# TODO: Write a loop/function to find the MLE\nfunction ga(f, θ; step_size = 0.6, tol = 1e-10, max_iter = 100)\n    f_prime(θ; h = 1e-5) = (f(θ + h) - f(θ)) / h\n    del = f_prime(θ)\n    iter = 0\n    while (iter < max_iter && abs(del) > tol)\n        θ += step_size * del\n        del = f_prime(θ)\n        iter += 1\n    end\n    println(\"converged after \", iter, \" iterations.\")\n    θ\nend\nga (generic function with 1 method)\nLog-Likelihood\nThe log-likelihood function is \\[\\ell(\\theta; y_1 = 0, y_2 = 1, y_3 = 0) = \\log \\theta + 2 \\log(1 - \\theta).\\]\nThe second derivative (or Hessian if \\(\\theta\\) is a vector), is useful to obtaining approximate standard errors for the estimate. From the theory of maximum likelihood estimation, it can be shown that the standard error of the MLE is approximately \\(\\sqrt{- 1 / \\ell''(\\theta)}\\). This is usually called the asymptotic standard error, or ase.\nll(θ) = log(lik(θ))\n# Numerical first derivative\nll_prime(θ; h = 1e-5) = (ll(θ + h) - ll(θ)) / h\n# Numerical second-order derivative (2nd order forward)\nll_prime2(θ; h = 1e-5) = (ll(θ + 2h) - 2ll(θ + h) + ll(θ)) / h^2\n# Asymptotic standard error\nase(θ) = sqrt(- 1 / ll_prime2(θ))\nase (generic function with 1 method)\nSo the ase is 0.27.\nNewton’s Method\nSee https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization\nθ₀ = 0.5\nll0 = ll(θ₀)\ndel0 = ll_prime(θ₀)\ndeldel0 = ll_prime2(θ₀)\nθ₁ = θ₀ - del0 / deldel0\ndel1 = ll_prime(θ₁)\ndeldel1 = ll_prime2(θ₁)\nθ₂ = θ₁ + del1 / deldel1\n# TODO: Write a loop/function to find the MLE (credit to Winnie Tse)\nfunction newt(θ; step_size = .6, tol = 1e-10, max_iter = 10)\n    del = ll_prime(θ)\n    deldel = ll_prime2(θ)\n    iter = 0\n    while (iter < max_iter && abs(del) > tol)\n        θ += (-1)^(iter+1) * del / deldel\n        del = ll_prime(θ)\n        deldel = ll_prime2(θ)\n        iter += 1\n    end\n    println(\"converged after \", iter, \" iterations.\")\n    θ\nend\nnewt(.5)\nconverged after 5 iterations.\n0.33332833334790385\nUsing Established Algorithms with Optim.jl\nSee https://julianlsolvers.github.io/Optim.jl/stable/\n# Find minimum on -LL\nopt = optimize(x -> -ll(x), 0, 1)  # Brent's Method\n# Second-derivative (5th order central method)\nFiniteDifferences.central_fdm(5, 2)(ll, opt.minimizer)\n-13.499999822735278\nFit MLE\n# From Distributions.jl\nfit_mle(Bernoulli, [0, 1, 0])\nDistributions.Bernoulli{Float64}(p=0.3333333333333333)\nRegression\n\\[Y = \\beta_0 + \\beta_1 X + e\\]\n\\[e \\sim N(0, \\sigma)\\]\n\\[f(Y; \\beta_0, \\beta_1) \\overset{d}{=} N(\\beta_0 + \\beta_1 X, \\sigma)\\]\n\\[f(Y_1, Y_2, \\ldots; \\beta_0, \\beta_1) \\overset{d}{=} \\Pi_{i = 1}^n N(\\beta_0 + \\beta_1 X_i, \\sigma)\\]\n\\[\\ell(\\beta_0, \\beta_1, \\sigma; y_1, y_2, \\ldots, y_n) = \\sum_{i = 1}^n \\ell(\\beta_0, \\beta_1, \\sigma; y_i)\\]\n\\[\\ell(\\beta_0, \\beta_1, \\sigma; y_1, y_2, \\ldots, y_n) =  -\\frac{n}{2} \\log(2 \\pi) - n \\log \\sigma - \\frac{\\sum_{i = 1}^n(y_i - \\beta_0 - \\beta_1 x_i)^2}{2 \\sigma^2}\\]\n\nSample Data\n# From the `lm()` example in R\nctl = [4.17, 5.58, 5.18, 6.11, 4.50, 4.61, 5.17, 4.53, 5.33, 5.14]\ntrt = [4.81, 4.17, 4.41, 3.59, 5.87, 3.83, 6.03, 4.89, 4.32, 4.69]\n# Likelihood function\nfunction ll(θ; yc = ctl, yt = trt)\n    σ = θ[3]\n    if σ < 0 \n        return -Inf\n    end\n    β₀ = θ[1]\n    β₁ = θ[2]\n    sum(vcat(logpdf(Normal(β₀, σ), yc), \n             logpdf(Normal(β₀ + β₁, σ), yt)))\nend\nll (generic function with 1 method)\nPlot the conditional log-likelihood functions\n# Just beta1\nplot(x -> ll([5, x, 1]), -2, 2)\n# Both beta1 and sigma\ncontour(-1:0.01:0, 0.5:0.01:1, (x, y) -> ll([5, x, y]), fill = true)\n\nWith Optim.jl\nopt = optimize(x -> -ll(x), [4.0, 1.0, 1.0])    # Nelder-Mead\nopt.minimizer\n3-element Vector{Float64}:\n  5.03198824016666\n -0.37098861906446434\n  0.6606616771567159\nOr use Optim.maximize\nresult = maximize(ll, [4.0, 1.0, 1.0])\nresult.res.minimizer\n3-element Vector{Float64}:\n  5.03198824016666\n -0.37098861906446434\n  0.6606616771567159\nBox Constraints\nWhen some of the parameter(s) are constrained in just a subset of the real line.\nfunction ll2(θ; yc = ctl, yt = trt)\n    sum(vcat(logpdf(Normal(θ[1], θ[3]), yc), \n             logpdf(Normal(θ[1] + θ[2], θ[3]), yt)))\nend\n# Unconstrained for beta, [0, infinity) for sigma\nopt_box = optimize(x -> -ll2(x), \n                   [-Inf, -Inf, 0], [Inf, Inf, Inf], \n                   [4.0, 1.0, 1.0])    # Fminbox with L-BFGS\nopt_box.minimizer\n3-element Vector{Float64}:\n  5.032000000048675\n -0.3710000000898218\n  0.6606530860112547\nTo obtain the Hessian (to be used to compute standard errors, use\nhess = ForwardDiff.hessian(x -> ll(x), opt.minimizer)\n# Asymptotic covariance matrix\n- inv(hess)\n3×3 Matrix{Float64}:\n  0.0436474   -0.0436474   -3.88479e-7\n -0.0436474    0.0872948    3.75962e-7\n -3.88479e-7   3.75962e-7   0.0109123\nCompare to \\(t\\) test\nEqualVarianceTTest(ctl, trt)\nTwo sample t-test (equal variance)\n----------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          0.371\n    95% confidence interval: (-0.2833, 1.0253)\n\nTest summary:\n    outcome with 95% confidence: fail to reject h_0\n    two-sided p-value:           0.2490\n\nDetails:\n    number of observations:   [10,10]\n    t-statistic:              1.191260381848704\n    degrees of freedom:       18\n    empirical standard error: 0.3114348514002032\nWith Analytic Derivatives\nfunction g!(G, θ; yc = ctl, yt = trt)\n    n = size(ctl, 1) + size(trt, 1)\n    G[1] = - (sum(yc .- θ[1]) + sum(yt .- θ[1] .- θ[2])) / θ[3]^2\n    G[2] = - sum(yt .- θ[1] .- θ[2]) / θ[3]^2\n    G[3] = n / θ[3] - sum(vcat(yc .- θ[1], yt .- θ[1] .- θ[2]) .^ 2) / θ[3]^3\nend\ng! (generic function with 1 method)\nopt2 = optimize(x -> -ll(x), g!, [4.0, 1.0, 1.0], GradientDescent())\n* Status: success\n\n * Candidate solution\n    Final objective value:     2.008824e+01\n\n * Found with\n    Algorithm:     Gradient Descent\n\n * Convergence measures\n    |x - x'|               = 1.92e-09 ≰ 0.0e+00\n    |x - x'|/|x'|          = 3.82e-10 ≰ 0.0e+00\n    |f(x) - f(x')|         = 0.00e+00 ≤ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 0.00e+00 ≤ 0.0e+00\n    |g(x)|                 = 6.96e-08 ≰ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    88\n    f(x) calls:    224\n    ∇f(x) calls:   224\nWith Automatic Differentiation (AD)\nobj_func = TwiceDifferentiable(x -> -ll(x), [4.0, 1.0, 1.0]; \n                               autodiff = :forward)\nopt3 = optimize(obj_func, [4.0, 1.0, 1.0])  # Newton's method\nNLSolversBase.hessian!(obj_func, opt3.minimizer)\n3×3 Matrix{Float64}:\n 45.823        22.9115       -3.52496e-14\n 22.9115       22.9115       -3.34732e-14\n -3.52496e-14  -3.34732e-14  91.6459\nUsing JuMP.jl\nusing JuMP\nimport Ipopt\nmodel = Model(Ipopt.Optimizer)\n@variable(model, β[1:2])\nsetvalue(β[1], 4.0)\nsetvalue(β[2], 1.0)\n@variable(model, σ >= 0.0, start = 1.0)\n@NLobjective(\n    model,\n    Max,\n    -10 * log(σ) - sum((ctl[i] - β[1])^2 for i = 1:10) / (2 * σ^2) + \n    -10 * log(σ) - sum((trt[i] - β[1] - β[2])^2 for i = 1:10) / (2 * σ^2)\n)\n# @NLconstraint(model, β₀ == 10σ)\noptimize!(model)\nJuMP.value.(β)\nJuMP.value.(σ)\n***************************************************************************\n***\nThis program contains Ipopt, a library for large-scale nonlinear optimizati\non.\n Ipopt is released as open source code under the Eclipse Public License (EP\nL).\n         For more information visit https://github.com/coin-or/Ipopt\n***************************************************************************\n***\n\nThis is Ipopt version 3.13.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation\n).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:        6\n\nTotal number of variables............................:        3\n                     variables with only lower bounds:        1\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_p\nr  ls\n   0 -1.0264350e+01 0.00e+00 6.93e+00  -1.0 0.00e+00    -  0.00e+00 0.00e+0\n0   0\n   1 -3.6504365e+00 0.00e+00 3.23e+01  -1.0 7.91e+00   0.0 1.00e+00 1.55e-0\n1f  2\n   2 -2.0124548e+00 0.00e+00 8.88e+00  -1.0 9.54e-02    -  8.95e-01 1.00e+0\n0f  1\n   3 -1.7225159e+00 0.00e+00 1.75e+00  -1.0 5.56e-02    -  1.00e+00 1.00e+0\n0f  1\n   4 -1.7094835e+00 0.00e+00 1.07e-01  -1.0 1.67e-02    -  1.00e+00 1.00e+0\n0f  1\n   5 -1.7094719e+00 0.00e+00 6.82e-05  -2.5 4.64e-04    -  1.00e+00 1.00e+0\n0f  1\n   6 -1.7094718e+00 0.00e+00 6.89e-07  -3.8 4.45e-05    -  1.00e+00 1.00e+0\n0f  1\n   7 -1.7094718e+00 0.00e+00 2.10e-09  -5.7 2.45e-06    -  1.00e+00 1.00e+0\n0f  1\n   8 -1.7094718e+00 0.00e+00 3.50e-13  -8.6 3.04e-08    -  1.00e+00 1.00e+0\n0f  1\n\nNumber of Iterations....: 8\n\n                                   (scaled)                 (unscaled)\nObjective...............:   1.7094718195406751e+00   -1.7094718195406751e+0\n0\nDual infeasibility......:   3.4991254363551114e-13    3.4991254363551114e-1\n3\nConstraint violation....:   0.0000000000000000e+00    0.0000000000000000e+0\n0\nComplementarity.........:   2.5059884083328424e-09   -2.5059884083328424e-0\n9\nOverall NLP error.......:   2.5059884083328424e-09    3.4991254363551114e-1\n3\n\n\nNumber of objective function evaluations             = 14\nNumber of objective gradient evaluations             = 9\nNumber of equality constraint evaluations            = 0\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 0\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 8\nTotal CPU secs in IPOPT (w/o function evaluations)   =      2.174\nTotal CPU secs in NLP function evaluations           =      1.901\n\nEXIT: Optimal Solution Found.\n0.6606530860101113\nFor more discussion on these packages and other packages in Julia, this page https://julia.quantecon.org/more_julia/optimization_solver_packages.html would be helpful. Also check out the documentation of JuMP.jl and Optim.jl.\nSufficient Statistics\nIt can be shown that the likelihood function depends only on \\(\\sum_{i = 1}^n y_i^2\\) and \\(\\sum_{i = 1}^n y_i\\). This helps speed up the optimization.\n# Likelihood function with sufficient statistics\nfunction ll_suff(θ; sum_yc = sum(ctl), sum_yt = sum(trt), \n                    sum_ycsq = sum(ctl .^ 2), sum_ytsq = sum(trt .^ 2))\n    β₀ = θ[1]\n    β₁ = θ[2]\n    σ = θ[3]\n    μₜ = β₀ + β₁\n    - 10 * log(2pi) - 20 * log(σ) - \n    (sum_ycsq + sum_ytsq - 2 * (β₀ * sum_yc + μₜ * sum_yt) + \n     10 * (β₀^2 + μₜ^2)) / 2σ^2\nend\n# Unconstrained for beta, [0, infinity) for sigma\nopt_box = optimize(x -> -ll_suff(x), \n                   [-Inf, -Inf, 0], [Inf, Inf, Inf], \n                   [4.0, 1.0, 1.0])    # Fminbox with L-BFGS\nopt_box.minimizer\n3-element Vector{Float64}:\n  5.032000000229699\n -0.3710000002832418\n  0.6606530859921219\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-14T15:04:58-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-09-julia-regression/",
    "title": "Regression with Julia!",
    "description": {},
    "author": [
      {
        "name": "Winnie Wing-Yee Tse",
        "url": {}
      }
    ],
    "date": "2021-02-11",
    "categories": [
      "Julia",
      "Regression",
      "Visualization"
    ],
    "contents": "\n\nContents\nSetup\nBrief Introduction to the Package Pipe\nBrief Introduction to the Package RDatasets\nBrief Introduction to the Package DataFrames\n\nLinear Regression\nExtracting Model Information\n\nMultiple Regression\nProbit Regression\nLogistic Regression\nVisualization\n\nSetup\nBefore we start, I encourage you to uncomment the codes below and install the required packages that you do not have for this tutorial.\nusing Plots, StatsPlots\n#= uncomment below to install packages\nusing Pkg\nPkg.add([\"RDatasets\", \"Pipe\", \"DataFrames\", \"GLM\",\n        \"Plots\", \"StatPlots\"])\n=#\nPrior to running regression analyses, let me briefly go over three essential packages, Pipe, RDatasets, DataFrames, in handling data frames in Julia.\nBrief Introduction to the Package Pipe\nThe package Pipe improves the base pipe operator (|>) in Julia. It allows you to represent the object that is piped from with an underscore _ in the functions that follow. For example, sqrt(sum([1, 2, 3])) involve two commands – one applied to [1, 2, 3] and the other one applied to sum([1, 2, 3]). We can pipe these commands, stemming from the [1, 2, 3] object, using @pipe together with |>, as illustrated below.\nusing Pipe\n@pipe [1, 2, 3] |> sum(_) |> sqrt(_)\n2.449489742783178\nThe above code might have overkilled this simple task of taking a sqaure root of a sum, but piping will become very useful in handling a long series of commands. For more details of Pipe, please read the julia tutorial on [pipe] (https://syl1.gitbook.io/julia-language-a-concise-tutorial/useful-packages/pipe).\nBrief Introduction to the Package RDatasets\nThe package RDatasets provides most of the base R datasets for Julia users to play around. We will be using the dataset bfi from the R package psych in this tutorial to explore regression analyses in Julia.\nLet’s read in the dataset bfi and drop out missing values.\nusing RDatasets\n# show(RDatasets.datasets(\"psych\"), allrows=true)\nbfi = @pipe dataset(\"psych\", \"bfi\") |> dropmissing(_)\nprintln(names(bfi))\n[\"Variable\", \"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"E\n1\", \"E2\", \"E3\", \"E4\", \"E5\", \"N1\", \"N2\", \"N3\", \"N4\", \"N5\", \"O1\", \"O2\", \"O3\",\n \"O4\", \"O5\", \"Gender\", \"Education\", \"Age\"]\nFor a complete R documentation of this dataset, please refer to [this] (https://www.personality-project.org/r/html/bfi.html).\nBrief Introduction to the Package DataFrames\nFor simplicity, let’s wrangle our data and sum over the scores on the columns for agreeableness (A1 to A5) and for contienciousness (C1 to C5). Gender is a numeric variable in the original dataset, while it should better be treated as a binary variable. Let’s also convert it into a categorical array using categorical.\ntransform() has similar function as mutate() in R, with which we can transform a column and save it into a new column in a dataframe. The operator => is pretty intuitive. In the first transform statment down below, we are basically asking Julia to take the columns between A1 and A5 in bfi, transform them by summing scores on the same row (=> ByRow(+)), and then save these transformed data into a new column called Asum.\nNote that to call a column in a dataframe, we need to place an colon : before the column name.\nusing DataFrames\nbfi = @pipe bfi |> \n            transform(_, Between(:A1, :A5) => ByRow(+) => :Asum) |> \n            transform(_, Between(:C1, :C5) => ByRow(+) => :Csum) |>\n            transform(_, :Gender => categorical => :Gender_bin)\ndescribe(select(bfi, Between(:Asum, :Gender_bin)))\n3×7 DataFrame\n Row │ variable    mean     min  median  max  nmissing  eltype             \n    ⋯\n     │ Symbol      Union…   Any  Union…  Any  Int64     DataType           \n    ⋯\n─────┼─────────────────────────────────────────────────────────────────────\n─────\n   1 │ Asum        21.1632  5    22.0    30          0  Int64              \n    ⋯\n   2 │ Csum        19.0501  5    19.0    30          0  Int64\n   3 │ Gender_bin           1            2           0  CategoricalValue{In\nt64\n                                                                1 column om\nitted\ndescribe() provides summary statistics of the selected columns of our newly created variables from Asum to Gender_bin. As can be seen in the summary table, Gender_bin has a data type of CategoricalValue{Int64,UInt32}. Through that way in our following regression analysis, gender will be treated as a categorical variable and automatically dummy coded.\nFor data wrangling in Julia using the package Dataframes, a [cheatsheet] (https://ahsmart.com/assets/pages/data-wrangling-with-data-frames-jl-cheat-sheet/DataFramesCheatSheet_v0.22_rev1.pdf) may come in handy here. I am a beginner Julia and Dataframes user. If you have a better way to optimize my codes, please let me know and I will be more than happy to learn it!\nLinear Regression\nBelow are some summary statistics and a snapshot of the variables of interest for the following analyses.\ndescribe(select(bfi, Between(:Education, :Gender_bin)))\n5×7 DataFrame\n Row │ variable    mean     min  median  max  nmissing  eltype             \n    ⋯\n     │ Symbol      Union…   Any  Union…  Any  Int64     DataType           \n    ⋯\n─────┼─────────────────────────────────────────────────────────────────────\n─────\n   1 │ Education   3.19141  1    3.0     5           0  Int64              \n    ⋯\n   2 │ Age         29.5103  3    26.0    86          0  Int64\n   3 │ Asum        21.1632  5    22.0    30          0  Int64\n   4 │ Csum        19.0501  5    19.0    30          0  Int64\n   5 │ Gender_bin           1            2           0  CategoricalValue{In\nt64 ⋯\n                                                                1 column om\nitted\nfirst(select(bfi, Between(:Education, :Gender_bin)), 5)\n5×5 DataFrame\n Row │ Education  Age    Asum   Csum   Gender_bin\n     │ Int64      Int64  Int64  Int64  Cat…\n─────┼────────────────────────────────────────────\n   1 │         3     21     28     22  2\n   2 │         2     19     14     15  1\n   3 │         1     21     24     17  1\n   4 │         1     17     14     19  1\n   5 │         5     68     23     18  1\nIn our sample, do older people tend to be more agreeable than younger people? We can fit a linear model with Age as the predictor and Asum, the sum scores of agreeableness, as the outcome variable, and determine whether there is a positive association between these two variables.\nThe package GLM provides convenient ways that are analogous to the R functions lm() and glm() to specify linear models in Julia.\nTo build a linear model, we use lm(), specify the model equation within @formula(), and indicate the dataset after the formula.\nusing GLM\nlm1 = lm(@formula(Asum ~ Age), bfi)\nStatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1\n}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2\n}}}},Array{Float64,2}}\n\nAsum ~ 1 + Age\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(>|t|)  Lower 95%   Upper 95%\n───────────────────────────────────────────────────────────────────────────\n(Intercept)  20.4382     0.225823    90.51    <1e-99  19.9954    20.8811\nAge           0.0245685  0.00719702   3.41    0.0007   0.010455   0.0386821\n───────────────────────────────────────────────────────────────────────────\nUnlike R, we do not need to additionally summarize lm1 to get a summary table of the outputs.\nEquivalent to lm(), we can use the fit() function with the LinearModel argument.\nfit1 = fit(LinearModel, @formula(Asum ~ Age), bfi)\nStatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1\n}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2\n}}}},Array{Float64,2}}\n\nAsum ~ 1 + Age\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(>|t|)  Lower 95%   Upper 95%\n───────────────────────────────────────────────────────────────────────────\n(Intercept)  20.4382     0.225823    90.51    <1e-99  19.9954    20.8811\nAge           0.0245685  0.00719702   3.41    0.0007   0.010455   0.0386821\n───────────────────────────────────────────────────────────────────────────\nlm(...) and fit(LinearModel, ...) do not differ in their type, output, and functionality. A quick check on their type:\ntypeof(lm1)\nStatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1\n}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2\n}}}},Array{Float64,2}}\ntypeof(fit1)\nStatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1\n}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2\n}}}},Array{Float64,2}}\nwhich are essentially the same.\nExtracting Model Information\nWe can use coef(), stderror(), and vcov() to extract coefficient estimates and standard errors of the coefficents, and the estimated variance-covariance matrix of the coefficient estimates, respectively.\ncoef(lm1)\n2-element Array{Float64,1}:\n 20.43821402633636\n  0.024568514618654096\nstderror(lm1)\n2-element Array{Float64,1}:\n 0.22582269777999403\n 0.007197020298763872\nvcov(lm1)\n2×2 Array{Float64,2}:\n  0.0509959   -0.00152855\n -0.00152855   5.17971e-5\nTo obtain statistics for model fit, we can use r2 for R^2 and deviance() for the weighted residual sum of squares.\nr2(lm1)\n0.005189310057134855\ndeviance(lm1)\n29411.99403563621\npredict() computes the predicted values of each individual in our sample.\npredict(lm1)\n2236-element Array{Float64,1}:\n 20.9541528333281\n 20.905015804090787\n 20.9541528333281\n 20.85587877485348\n 22.10887302040484\n 21.10156392104002\n 20.880447289472134\n 20.92958431870944\n 21.69120827188772\n 21.248975008751945\n  ⋮\n 21.15070095027733\n 21.248975008751945\n 21.003289862565406\n 20.978721347946752\n 20.978721347946752\n 21.02785837718406\n 21.15070095027733\n 21.199837979514637\n 21.666639757269067\n!!! note “Follow-up Practice (1)” Let’s try to run a linear model to investigate whether contiouenciousness is predicted by education. How much variance is explained by this model?\nMultiple Regression\nWe may suspect that the difference in agreeableness over years of age depend on someone’s education level. To investigate this, we can add an interation between Age and Education on Asum by Age & Education. Typically when we add an interaction term to a model, we include also the main effects of the variables. A shorthand of specifying both the main effects and interaction between the variables is Age * Education.\n# the models below are equivalent\nlm(@formula(Asum ~ Age + Education + Age & Education), bfi)\nStatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1\n}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2\n}}}},Array{Float64,2}}\n\nAsum ~ 1 + Age + Education + Age & Education\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n──────\n                      Coef.  Std. Error      t  Pr(>|t|)   Lower 95%    Upp\ner 95%\n───────────────────────────────────────────────────────────────────────────\n──────\n(Intercept)      19.4241      0.651563   29.81    <1e-99  18.1463     20.70\n18\nAge               0.0831532   0.0211002   3.94    <1e-4    0.0417752   0.12\n4531\nEducation         0.273547    0.200684    1.36    0.1730  -0.119999    0.66\n7093\nAge & Education  -0.016346    0.0061294  -2.67    0.0077  -0.0283659  -0.00\n432607\n───────────────────────────────────────────────────────────────────────────\n──────\nlm(@formula(Asum ~ Age * Education), bfi)\nStatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1\n}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2\n}}}},Array{Float64,2}}\n\nAsum ~ 1 + Age + Education + Age & Education\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n──────\n                      Coef.  Std. Error      t  Pr(>|t|)   Lower 95%    Upp\ner 95%\n───────────────────────────────────────────────────────────────────────────\n──────\n(Intercept)      19.4241      0.651563   29.81    <1e-99  18.1463     20.70\n18\nAge               0.0831532   0.0211002   3.94    <1e-4    0.0417752   0.12\n4531\nEducation         0.273547    0.200684    1.36    0.1730  -0.119999    0.66\n7093\nAge & Education  -0.016346    0.0061294  -2.67    0.0077  -0.0283659  -0.00\n432607\n───────────────────────────────────────────────────────────────────────────\n──────\n!!! note “Follow-up Practice (2)” Let’s fit a model to explore whether there is an interaction between education and gender (Gender_bin) on contienciousness. How much did the variance explained by the model increasecompared to the model without the interaction (in reference to Practice (1)) increase?\nProbit Regression\nFor the illustrative purpose, I arbitrarily define that a sum score above 20 out of 25 is a high score of agreeableness and dichotomize Asum to 1 or 0 accordingly.\nbfi = @pipe transform(bfi, \n                      :Asum => ByRow(function(x) \n                                        if x >= 20 1 \n                                        else 0 \n                                        end \n                                     end) \n                               => :Abin);\ndescribe(select(bfi, :Abin))\n1×7 DataFrame\n Row │ variable  mean     min    median   max    nmissing  eltype\n     │ Symbol    Float64  Int64  Float64  Int64  Int64     DataType\n─────┼──────────────────────────────────────────────────────────────\n   1 │ Abin      0.70975      0      1.0      1         0  Int64\nNow because our outcome variable is a binary variable, it is more appropriate to model it with a binomial distribution and a probit link.\nlm_probit = glm(@formula(Abin ~ Age), bfi, Binomial(), ProbitLink())\nStatsModels.TableRegressionModel{GLM.GeneralizedLinearModel{GLM.GlmResp{Arr\nay{Float64,1},Distributions.Binomial{Float64},GLM.ProbitLink},GLM.DensePred\nChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float\n64,2}}\n\nAbin ~ 1 + Age\n\nCoefficients:\n──────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error     z  Pr(>|z|)   Lower 95%  Upper 95%\n──────────────────────────────────────────────────────────────────────────\n(Intercept)  0.319024    0.0832706   3.83    0.0001  0.155816    0.482231\nAge          0.00798517  0.00269249  2.97    0.0030  0.00270799  0.0132623\n──────────────────────────────────────────────────────────────────────────\nAn equivalent way to specify this probit model is\nfit(GeneralizedLinearModel, @formula(Abin ~ Age), bfi, Binomial(), ProbitLink())\nStatsModels.TableRegressionModel{GLM.GeneralizedLinearModel{GLM.GlmResp{Arr\nay{Float64,1},Distributions.Binomial{Float64},GLM.ProbitLink},GLM.DensePred\nChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float\n64,2}}\n\nAbin ~ 1 + Age\n\nCoefficients:\n──────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error     z  Pr(>|z|)   Lower 95%  Upper 95%\n──────────────────────────────────────────────────────────────────────────\n(Intercept)  0.319024    0.0832706   3.83    0.0001  0.155816    0.482231\nAge          0.00798517  0.00269249  2.97    0.0030  0.00270799  0.0132623\n──────────────────────────────────────────────────────────────────────────\nOur probit model is as follows. \\[\n\\Phi^{-1}(\\text{High_Agreeableness}) = 0.319 + 0.008 \\text{Age}\n\\]\nFor a person of age 20, the predicted probit of scoring high on agreeableness is 0.479.\nNote that the coefficients are probability z-scores and the prediction is a probit. To translate a probit back to a probability, we could use cdf() in the Distributions package.\nNormal() by default specifies a normal distribution with a mean of 0 and a standard deviation of 1.\nusing Distributions\ncdf(Normal(), .479)\n0.6840306856730872\nThe predicted probability of someone at age 20 scoring high on agreeablness is 68.4%.\nLogistic Regression\nAn alternative way to handle binary outcome variables is through logistic regression.\nlm_logit = glm(@formula(Abin ~ Age), bfi, Binomial(), LogitLink())\nStatsModels.TableRegressionModel{GLM.GeneralizedLinearModel{GLM.GlmResp{Arr\nay{Float64,1},Distributions.Binomial{Float64},GLM.LogitLink},GLM.DensePredC\nhol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float6\n4,2}}\n\nAbin ~ 1 + Age\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error     z  Pr(>|z|)   Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────\n(Intercept)  0.497558   0.139339    3.57    0.0004  0.224458    0.770657\nAge          0.0135925  0.00455195  2.99    0.0028  0.00467084  0.0225141\n─────────────────────────────────────────────────────────────────────────\nSimilarly, an equivalent way to specify this logistic model is\nfit(GeneralizedLinearModel, @formula(Abin ~ Age), bfi, Binomial(), LogitLink())\nStatsModels.TableRegressionModel{GLM.GeneralizedLinearModel{GLM.GlmResp{Arr\nay{Float64,1},Distributions.Binomial{Float64},GLM.LogitLink},GLM.DensePredC\nhol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float6\n4,2}}\n\nAbin ~ 1 + Age\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error     z  Pr(>|z|)   Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────\n(Intercept)  0.497558   0.139339    3.57    0.0004  0.224458    0.770657\nAge          0.0135925  0.00455195  2.99    0.0028  0.00467084  0.0225141\n─────────────────────────────────────────────────────────────────────────\nNow our logistic model is as follows. \\[\nlogit(\\text{High_Agreeableness}) = 0.498 + 0.014 \\text{Age}\n\\]\nFor a person of age 20, the predicted logit of scoring high on agreeableness is 0.778. Now the prediction is represented in terms of logit. To make it more interpretable, we can simply exponentiate the logit to get the odds ratio, or use the following formula to get the probability:\n\\[\n\\hat{\\pi} = \\frac{exp(0.498 + 0.014)}{1 + exp(0.498 + 0.014)}\n\\]\nwhere \\hat{\\pi} is the predicted probability of someone scoring high on agreeableness.\nexp(.778)/(1 + exp(.778))\n0.6852489081587367\nConsistent with the result from probit regression, the predicted probability of someone at age 20 scoring high on agreeablness is 68.5%.\nVisualization\nLet’s cover the answers of Follow-up Practice (1) and (2) here.\n# Practice (1)\nlm3 = fit(LinearModel, @formula(Csum ~ Education), bfi)\nStatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1\n}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2\n}}}},Array{Float64,2}}\n\nCsum ~ 1 + Education\n\nCoefficients:\n──────────────────────────────────────────────────────────────────────────\n                Coef.  Std. Error       t  Pr(>|t|)   Lower 95%  Upper 95%\n──────────────────────────────────────────────────────────────────────────\n(Intercept)  18.5942    0.176512   105.34    <1e-99  18.2481     18.9403\nEducation     0.14285   0.0522318    2.73    0.0063   0.0404219   0.245278\n──────────────────────────────────────────────────────────────────────────\n# Practice (2)\nlm4 = fit(LinearModel, @formula(Csum ~ Education*Gender_bin), bfi)\nStatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1\n}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2\n}}}},Array{Float64,2}}\n\nCsum ~ 1 + Education + Gender_bin + Education & Gender_bin\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n──────────────\n                                Coef.  Std. Error      t  Pr(>|t|)   Lower \n95%  Upper 95%\n───────────────────────────────────────────────────────────────────────────\n──────────────\n(Intercept)                18.9793      0.287144   66.10    <1e-99  18.4162\n     19.5424\nEducation                   0.0693464   0.0844421   0.82    0.4116  -0.0962\n469   0.23494\nGender_bin: 2              -0.607298    0.364028   -1.67    0.0954  -1.3211\n7     0.106571\nEducation & Gender_bin: 2   0.119855    0.10743     1.12    0.2647  -0.0908\n195   0.330529\n───────────────────────────────────────────────────────────────────────────\n──────────────\nNote that Gender_bin is dummy coded with male (1) as 0 and female (2) as 1. The default in GLM is dummy coding, but we can specify a different contrast coding. More details on contrast coding can be found [here] (https://juliastats.org/StatsModels.jl/stable/contrasts/).\n# extracting coefficients\nlm3_b0, lm3_b1 = coef(lm3);\nlm4_b0, lm4_b1, lm4_b2, lm4_b3 = coef(lm4);\nWe will use the package Plots for visualizing the relationship between variables in the fitted models. The package StatsPlots is an extension of Plots that supports plotting with DataFrame objects.\n@df bfi scatter(\n    :Education, \n    :Csum, \n    group = :Gender, \n    legend = :bottomright, \n    label = [\"Male\" \"Female\"], \n    xlabel = \"Education\", \n    ylabel = \"Contientiousness\") \nplot!((x) -> lm3_b0 + lm3_b1 * x, 1, 5, label = \"Linear Model\")\n\n@df bfi scatter(\n    :Education, \n    :Csum, \n    group = :Gender, \n    legend = :bottomright, \n    label = [\"Male\" \"Female\"], \n    xlabel = \"Education\", \n    ylabel = \"Contienciousness\", \n    size=(690,460)) \nplot!((x) -> lm4_b0 + lm4_b1 * x + lm4_b2 * 0 + lm4_b2 * x * 0, \n      1, 5, label = \"Male\", linecolor = \"blue\")\nplot!((x) -> lm4_b0 + lm4_b1 * x + lm4_b2 * 1 + lm4_b2 * x * 1, \n      1, 5, label = \"Female\", linecolor = \"red\")\n\nI also made reference to this webpage to create the plots above.\nPerhaps because the “research questions” I posed here were not so interesting, there was not much interesting to observe from the analysis outputs and the plots. But I hope you enjoy this tutorial in doing regression analyses with Julia :)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-14T15:04:58-08:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Measurement & Multilevel Modeling Lab",
    "description": "Welcome to our blog. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "MMM Lab",
        "url": {}
      }
    ],
    "date": "2020-12-17",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-14T15:04:58-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-25-part-1-covid-19/",
    "title": "Exploring the UAS Survey Part 1: Data exploration (Demography and Risk Items)",
    "description": "Analyze Covid-19 survey conducted by USC in March, 2020.",
    "author": [
      {
        "name": "Haley",
        "url": {}
      }
    ],
    "date": "2020-07-25",
    "categories": [
      "COVID",
      "Visualization",
      "Exploratory Data Analysis",
      "Applied"
    ],
    "contents": "\n\nContents\nIntroduction\nData preparation\nLoading libraries\nLoading data using here package\nData transformation\n\nGender\nAge\nResidence\nEducation\nIncome\nRace\nTime\nSummary\nPublication Statement\n\n[Edit by ML: The styler package was used to style the code. ]\nIntroduction\nAs people in the US learned about the COVID-19 pandemic, their perceived risks related to infection, unemployment, and other risk factors have changed during the early month of the outbreak. In this post, we are interpreting a survey published by the University of Southern California by analyzing and comparing some key features related to risk factors.\nData preparation\nLoading libraries\n\n\nlibrary(tidyverse)\nlibrary(Rmisc)\nlibrary(ggpubr)\nlibrary(ggsci)\n\n\n\nLoading data using here package\n\n\n# Imported the second wave of UAS230 Covid Survey Data.\ncovid19 <- readr::read_csv(here::here(\"data\", \"uas230_march_31_2020.csv\"))\n# The data was collected from a national survey during 10 March,2020 - 31 March,2020.\n\n\n\nData transformation\n\n\n# Create a new dataframe contains only key demographic information and risk items\ndemo <- covid19 %>%\n  select(\n    statereside, gender, end_date, age, education, hhincome, race,\n    cr005, cr009, cr006, cr008a, end_date\n  )\n\ndemo <- na_if(demo, \".a\") # question never seen by the respondents. \n                          # S/he may skip over the question, or never \n                          # view the question due to survey broke off.\ndemo <- na_if(demo, \".e\") # .e -> questions that were asked but not answered\ndemo <- na_if(demo, \".c\") # .c -> if respondent did not complete the survey\n\ndemo <- mutate_at(\n  demo, # convert data type to numeric data\n  vars(cr006, cr005, cr009, cr008a, age),\n  as.numeric\n)\ndemo <- na.omit(demo)\n\n\n\nGender\n58.6% of the respondents are female.\nMale and Female have a similar belief in how infectious Coronavirus is.\nMore male respondents are very optimistic about the risks related to Coronavirus.\n\n\ngen <- demo %>%\n  group_by(gender) %>%\n  dplyr::summarise(g = n())\n\nggplot(gen, aes(x = gender, y = g, fill = gender)) + # Barplot of gender distribution with annotation\n  geom_col() +\n  geom_label(aes(label = g)) +\n  labs(x = \"Gender\", y = \"Count\") +\n  scale_fill_jco() + # jco palette\n  coord_flip() +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\ngender_inf <- demo %>% # Plot perceived infection risk\n  select(gender, cr005) %>%\n  ggplot(aes(x = cr005, color = gender, fill = gender)) +\n  geom_density(alpha = 0.5) +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) +\n  labs(x = \"Perceived risk of infection\")\n\n\ngender_death <- demo %>% # Plot perceived risk of death\n  select(gender, cr006) %>%\n  ggplot(aes(x = cr006, color = gender, fill = gender)) +\n  geom_density(alpha = 0.5) +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) +\n  labs(x = \"Perceived risk of death\", y = \"\")\n\n\ngender_unemploy <- demo %>% # Plot perceived unemployment risk\n  select(gender, cr008a) %>%\n  ggplot(aes(x = cr008a, color = gender, fill = gender)) +\n  geom_density(alpha = 0.5) +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) +\n  labs(x = \"Perceived unemployment risk\", y = \"\")\n\n\ngender_finance <- demo %>% # Plott perceived risk of running out of money\n  select(gender, cr009) %>%\n  ggplot(aes(x = cr009, color = gender, fill = gender)) +\n  geom_density(alpha = 0.5) +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) +\n  labs(x = \"Perceived risk of running out of money\")\n\n\nggarrange(gender_inf, gender_death, gender_finance, gender_unemploy + rremove(\"x.text\"), # Combining gender and risk items in one plot\n  common.legend = TRUE, legend = \"right\",\n  ncol = 2, nrow = 2\n)\n\n\n\n\nAge\nMost respondents are between age 30 to age 60.\nYounger respondents have a higher estimate chance of infection, but a lower estimate chance of death related to Coronavirus.\nElderly respondents have a lower perceived unemployment risk and financial risk related to coronavirus.\n[Two graphs below are edited by ML]\n\n\nggplot(demo, aes(x = age)) +\n  geom_histogram(binwidth = 10, alpha = 0.85, fill = \"deepskyblue3\", color = \"white\") +\n  theme_minimal() +\n  labs(x = \"Age\", y = \"Count\") +\n  scale_x_continuous(breaks = seq(0, 100, by = 10)) +\n  scale_y_continuous(breaks = seq(0, 1400, by = 200))\n\n\n\nggplot(data = demo) + # combined plot 1\n  geom_point(\n    mapping = aes(x = age, y = cr005, color = \"Perceived Risk of Infection\"),\n    alpha = 0.1, size = 0.5\n  ) +\n  geom_point(\n    mapping = aes(x = age, y = cr006, color = \"Perceived Risk of Death\"),\n    alpha = 0.1, size = 0.5\n  ) +\n  geom_smooth(mapping = aes(x = age, y = cr005, color = \"Perceived Risk of Infection\")) +\n  geom_smooth(mapping = aes(x = age, y = cr006, color = \"Perceived Risk of Death\")) +\n  labs(y = \"Perceived Risk (%)\", x = \"Age\") +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) +\n  scale_x_continuous(breaks = seq(0, 100, by = 10))\n\n\n\nggplot(data = demo) + # combined plot 2\n  geom_point(\n    mapping = aes(x = age, y = cr008a, color = \"Perceived Unemployment Risk\"),\n    alpha = 0.1, size = 0.5\n  ) +\n  geom_point(\n    mapping = aes(x = age, y = cr009, color = \"Perceived Financial Risk\"),\n    alpha = 0.1, size = 0.5\n  ) +\n  geom_smooth(mapping = aes(x = age, y = cr008a, color = \"Perceived Unemployment Risk\")) +\n  geom_smooth(mapping = aes(x = age, y = cr009, color = \"Perceived Financial Risk\")) +\n  labs(y = \"Perceived Risk (%)\", x = \"Age\") +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) +\n  scale_x_continuous(breaks = seq(15, 95, by = 10))\n\n\n\n\nResidence\nThe number of respondents from California is significantly higher than number of respondents from any other states.\nRespondents in New York, Pennsylvania, and Michigan report a lower perceived fianancial risk related to Coronavirus.\nWhile respondents in California, Illnois, and Texas report a higher perceived unemployment risk and financial risk related to Coronavirus.\n[ML: Interestingly, it looks like Californian perceived higher risks consistently than those in other states.]\n\n\ndemo_state <- demo %>% # add counts for each state:\n  add_count(statereside, name = \"n\") %>% # use New York as cutline, filter out 8 states with n >= 115\n  filter(n >= 115)\n\n\nreside <- demo_state %>%\n  group_by(statereside) %>%\n  dplyr::summarise(re = n())\n\nggplot(reside, aes(x = statereside, y = re, fill = statereside)) + # Barplot of gender distribution with annotation\n  geom_col() +\n  geom_label(aes(label = re), fill = \"white\") +\n  labs(x = \"State Reside\", y = \"Count\") +\n  scale_fill_jco() +\n  coord_flip() +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\ndeath_state <- ggplot(demo_state, aes(x = cr006, fill = statereside)) + # Plot perceived risk of death\n  geom_density(alpha = 0.3) +\n  theme_minimal() +\n  labs(y = \"\", x = \"Perceived risk of death\") +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10))\n\ninf_state <- ggplot(demo_state, aes(x = cr005, fill = statereside)) + # Plot perceived risk of infection\n  geom_density(alpha = 0.3) +\n  theme_minimal() +\n  labs(x = \"Perceived risk of infection\") +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10))\n\nfinance_state <- ggplot(demo_state, aes(x = cr009, fill = statereside)) + # Plot perceived financial risk\n  geom_density(alpha = 0.3) +\n  theme_minimal() +\n  labs(y = \"\", x = \"Perceived financial risk\") +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10))\n\n\nunemploy_state <- ggplot(demo_state, aes(x = cr008a, fill = statereside)) + # Plot perceived unemployment risk\n  geom_density(alpha = 0.3) +\n  theme_minimal() +\n  labs(x = \"Perceived unemployment risk\") +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10))\n\n\nggarrange(inf_state, death_state, finance_state, unemploy_state, # combine the plots\n  common.legend = TRUE, legend = \"right\",\n  ncol = 2, nrow = 2\n)\n\n\n\nfinance_state_stat <- desc_statby(demo_state, measure.var = \"cr009\", grps = \"statereside\") # Summary table for perceived risk of financial hardship by states\nfinance_state_stat <- finance_state_stat[, c(\"statereside\", \"length\", \"mean\", \"sd\")] %>%\n  arrange(-mean) # sort by mean in decending order\n\nfinance_state_stat$mean <- round(finance_state_stat$mean, 2) # round to 2 decimal places\nfinance_state_stat$sd <- round(finance_state_stat$sd, 2)\n\n\nfinance_state_stat <- ggtexttable(finance_state_stat,\n  rows = NULL,\n  theme = ttheme(\"mOrange\")\n) # Summary table plot, medium orange theme\n\n\nunemploy_state_stat <- desc_statby(demo_state, measure.var = \"cr008a\", grps = \"statereside\") # Summary table for perceived risk of unemployment by states\nunemploy_state_stat <- unemploy_state_stat[, c(\"statereside\", \"length\", \"mean\", \"sd\")] %>%\n  arrange(-mean)\n\nunemploy_state_stat$mean <- round(unemploy_state_stat$mean, 2) # round to 2 decimal places\nunemploy_state_stat$sd <- round(unemploy_state_stat$sd, 2)\n\nunemploy_state_stat <- ggtexttable(unemploy_state_stat,\n  rows = NULL,\n  theme = ttheme(\"mOrange\")\n)\n\ntext_2 <- paste(\"Perceived risk of unemployment\", sep = \" \") # Create text to annotate summary tables\ntext_1 <- paste(\"Perceived risk of financial hardship\", sep = \" \")\ntext.p1 <- ggparagraph(text = text_1, face = \"italic\", size = 11, color = \"black\")\ntext.p2 <- ggparagraph(text = text_2, face = \"italic\", size = 11, color = \"black\")\n\nggarrange(text.p1, text.p2, finance_state_stat, unemploy_state_stat, # combine the plots\n  heights = c(0.1, 2),\n  ncol = 2, nrow = 2\n)\n\n\n\n\nEducation\nRespondents with a higher education degree report a higher perceived risk of infection, but lower perceived risk of death.\nRespondents with a lower educaton degree report a higher perceived unemployment risk and financial risk.\n\n\ndemo <- demo %>% # recode education level into more organzied groups\n  mutate(tidyEdu = recode(education,\n    `1 Less than 1st grade` = \"Less than HS\",\n    `10 Some college-no degree` = \"Some College\",\n    `11 Assoc. college degree-occ/voc prog` = \"Associate\",\n    `12 Assoc. college degree-academic prog` = \"Associate\",\n    `13 Bachelor's degree` = \"College/professional\",\n    `14 Master's degree` = \"Master/PhD\",\n    `15 Professional school degree` = \"College/professional\",\n    `16 Doctorate degree` = \"Master/PhD\",\n    `2 Up to 4th grade` = \"Less than HS\",\n    `3 5th or 6th grade` = \"Less than HS\",\n    `4 7th or 8th grade` = \"Less than HS\",\n    `5 9th grade` = \"Less than HS\",\n    `6 10th grade` = \"Less than HS\",\n    `7 11th grade` = \"Less than HS\",\n    `8 12th grade-no diploma` = \"Less than HS\",\n    `9 High school graduate or GED` = \"HS\"\n  ))\n\n\nedu <- demo %>%\n  group_by(tidyEdu) %>%\n  dplyr::summarise(e = n())\n\nggplot(edu, aes(x = tidyEdu, y = e, fill = tidyEdu)) + # Barplot of education distribution with annotation\n  geom_col() +\n  geom_label(aes(label = e), fill = \"white\") +\n  labs(x = \"Education\", y = \"Count\") +\n  scale_fill_jco() +\n  coord_flip() +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  scale_x_discrete(limits = c(\"Less than HS\", \"HS\", \"Some College\", \"Associate\", \"College/professional\", \"Master/PhD\")) # fixed order\n\n\n\n# summarize basic statistic using Rmisc package\nvis_death <- summarySE(demo, measurevar = \"cr006\", groupvars = c(\"tidyEdu\"))\n\nedu_2 <- ggplot(vis_death, aes(x = tidyEdu, y = cr006, fill = tidyEdu)) + # Plot Perceived risk of death\n  geom_bar(stat = \"identity\", width = .5) +\n  scale_fill_jco() +\n  theme_minimal() +\n  coord_flip() +\n  labs(x = \"\", y = \"Perceived risk of death (%)\") +\n  scale_x_discrete(limits = c(\"Less than HS\", \"HS\", \"Some College\", \"Associate\", \"College/professional\", \"Master/PhD\")) +\n  scale_y_continuous(breaks = seq(0, 30, by = 5))\n\n\nvis_inf <- summarySE(demo, measurevar = \"cr005\", groupvars = c(\"tidyEdu\")) # Plot Perceived risk of infection\n\nedu_1 <- ggplot(vis_inf, aes(x = tidyEdu, y = cr005, fill = tidyEdu)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  scale_fill_jco() +\n  coord_flip() +\n  labs(x = \"\", y = \"Perceived risk of infection (%)\") +\n  scale_x_discrete(limits = c(\"Less than HS\", \"HS\", \"Some College\", \"Associate\", \"College/professional\", \"Master/PhD\")) +\n  scale_y_continuous(breaks = seq(0, 30, by = 5))\n\n# Perceived unemployment risk (cr008a) by education\nvis_umemploy <- summarySE(demo, measurevar = \"cr008a\", groupvars = c(\"tidyEdu\"))\n\nedu_4 <- ggplot(vis_umemploy, aes(x = tidyEdu, y = cr008a, fill = tidyEdu)) + # Plot Perceived risk of unemployment\n  geom_bar(stat = \"identity\", width = .5) +\n  coord_flip() +\n  scale_fill_jco() +\n  theme_minimal() +\n  labs(x = \"\", y = \"Perceived risk of unemployment (%)\") +\n  scale_x_discrete(limits = c(\"Less than HS\", \"HS\", \"Some College\", \"Associate\", \"College/professional\", \"Master/PhD\")) +\n  scale_y_continuous(breaks = seq(0, 30, by = 5))\n\n# Perceived financial risk (cr009) by education\nvis_financial <- summarySE(demo, measurevar = \"cr009\", groupvars = c(\"tidyEdu\"))\n\nedu_3 <- ggplot(vis_financial, aes(x = tidyEdu, y = cr009, fill = tidyEdu)) + # Plot Perceived risk of running out of money\n  geom_bar(stat = \"identity\", width = .5) +\n  coord_flip() +\n  scale_fill_jco() +\n  theme_minimal() +\n  labs(x = \"\", y = \"Perceived risk of running out of money (%)\") +\n  scale_x_discrete(limits = c(\"Less than HS\", \"HS\", \"Some College\", \"Associate\", \"College/professional\", \"Master/PhD\")) +\n  scale_y_continuous(breaks = seq(0, 30, by = 5))\n\nggarrange(edu_1, edu_2, edu_3, edu_4, # Combine plots\n  common.legend = TRUE, legend = \"none\",\n  ncol = 2, nrow = 2\n)\n\n\n\n\nIncome\nHigh income respondents report a higher perceived risk of infection, but a lower perceived risk of death.\nLow income respondents report a higher perceived unemployment risk and financial risk.\n\n\ndemo <- demo %>%\n  mutate(incomeTidy = recode(hhincome,\n    `1 Less than $5,000` = \"under 20k\",\n    `3 7,500 to 9,999` = \"under 20k\",\n    `2 5,000 to 7,499` = \"under 20k\",\n    `4 10,000 to 12,499` = \"under 20k\",\n    `5 12,500 to 14,999` = \"under 20k\",\n    `6 15,000 to 19,999` = \"under 20k\",\n    `7 20,000 to 24,999` = \"20-39k\",\n    `8 25,000 to 29,999` = \"20-39k\",\n    `9 30,000 to 34,999` = \"20-39k\",\n    `10 35,000 to 39,999` = \"20-39k\",\n    `11 40,000 to 49,999` = \"40-59k\",\n    `12 50,000 to 59,999` = \"40-59k\",\n    `13 60,000 to 74,999` = \"60-74k\",\n    `14 75,000 to 99,999` = \"75-99k\",\n    `15 100,000 to 149,999` = \"100-149k\",\n    `16 150,000 or more` = \"150k plus\"\n  ))\n\ntemp_inc <- demo %>%\n  select(incomeTidy) %>%\n  group_by(incomeTidy) %>%\n  dplyr::summarise(cnt = n())\n\nggplot(temp_inc, aes(x = incomeTidy, y = cnt, fill = incomeTidy)) +\n  geom_col() +\n  geom_label(aes(label = cnt), fill = \"white\") +\n  labs(x = \"Income\", y = \"Count\") +\n  scale_fill_jco() +\n  theme_bw() +\n  coord_flip() +\n  theme(legend.position = \"none\") +\n  scale_x_discrete(limits = c(\"under 20k\", \"20-39k\", \"40-59k\", \"60-74k\", \"75-99k\", \"100-149k\", \"150k plus\"))\n\n\n\n# Perceived risk of infection (cr005) by Income\ninc_infection <- summarySE(demo, measurevar = \"cr005\", groupvars = c(\"incomeTidy\"))\n\ninc_1 <- ggplot(inc_infection, aes(x = incomeTidy, y = cr005, fill = incomeTidy)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  scale_fill_jco() +\n  coord_flip() +\n  labs(x = \"\", y = \"Perceived risk of infection (%)\") +\n  scale_x_discrete(limits = c(\"under 20k\", \"20-39k\", \"40-59k\", \"60-74k\", \"75-99k\", \"100-149k\", \"150k plus\"))\n\n# Perceived risk of death (cr006) by Education\ninc_death <- summarySE(demo, measurevar = \"cr006\", groupvars = c(\"incomeTidy\"))\n\ninc_2 <- ggplot(inc_death, aes(x = incomeTidy, y = cr006, fill = incomeTidy)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  coord_flip() +\n  scale_fill_jco() +\n  labs(x = \"\", y = \"Perceived risk of death (%)\") +\n  scale_x_discrete(limits = c(\"under 20k\", \"20-39k\", \"40-59k\", \"60-74k\", \"75-99k\", \"100-149k\", \"150k plus\"))\n\n# Perceived unemployment risk (cr008a) by Income\ninc_unemploy <- summarySE(demo, measurevar = \"cr008a\", groupvars = c(\"incomeTidy\"))\n\ninc_4 <- ggplot(inc_unemploy, aes(x = incomeTidy, y = cr008a, fill = incomeTidy)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  coord_flip() +\n  scale_fill_jco() +\n  labs(x = \"\", y = \"Perceived risk of unemployment (%)\") +\n  scale_x_discrete(limits = c(\"under 20k\", \"20-39k\", \"40-59k\", \"60-74k\", \"75-99k\", \"100-149k\", \"150k plus\"))\n\n# Perceived financial risk (cr009) by Income\ninc_unemploy <- summarySE(demo, measurevar = \"cr009\", groupvars = c(\"incomeTidy\"))\n\n# Create visualization, order the bar by education level\ninc_3 <- ggplot(inc_unemploy, aes(x = incomeTidy, y = cr009, fill = incomeTidy)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  coord_flip() +\n  scale_fill_jco() +\n  labs(x = \"\", y = \"Perceived risk of running out of money (%)\") +\n  scale_x_discrete(limits = c(\"under 20k\", \"20-39k\", \"40-59k\", \"60-74k\", \"75-99k\", \"100-149k\", \"150k plus\"))\n\nggarrange(inc_1, inc_2, inc_3, inc_4,\n  common.legend = TRUE, legend = \"none\",\n  ncol = 2, nrow = 2\n)\n\n\n\n\nRace\nCompare to other races, Black American reported a lower perceived risk of infection, but a higher perceived financial risk, and perceived risk of death.\n\n\nrace_total <- demo %>%\n  select(race) %>%\n  group_by(race) %>%\n  filter(race %in% c(\"1 White Only\", \"2 Black Only\", \"4 Asian Only\", \"6 Mixed\")) %>%\n  dplyr::summarise(r = n())\n\n# Plot race distribution with annotation\nggplot(race_total, aes(x = race, y = r, fill = race)) +\n  geom_col() +\n  geom_label(aes(label = r), fill = \"white\") +\n  labs(x = \"Race\", y = \"Count\") +\n  scale_fill_jco() +\n  coord_flip() +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n# filter out the racial groups n >200\ndemo_race <- demo %>%\n  select(race, cr005, cr006, cr008a, cr009) %>%\n  filter(race %in% c(\"1 White Only\", \"2 Black Only\", \"4 Asian Only\", \"6 Mixed\"))\n\n# Perceived risk of infection (cr005) by Race\nrace_infection <- summarySE(demo_race, measurevar = \"cr005\", groupvars = c(\"race\"))\n\nrace_1 <- ggplot(race_infection, aes(x = race, y = cr005, fill = race)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  coord_flip() +\n  scale_fill_jco() +\n  labs(x = \"\", y = \"Perceived risk of infection (%)\")\n\n# Perceived risk of death (cr006) by Race\nrace_death <- summarySE(demo_race, measurevar = \"cr006\", groupvars = c(\"race\"))\n\nrace_2 <- ggplot(race_death, aes(x = race, y = cr006, fill = race)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  coord_flip() +\n  scale_fill_jco() +\n  labs(x = \"\", y = \"Perceived risk of death (%)\")\n\n# Perceived unemployment risk (cr008a) by Race\nrace_unemploy <- summarySE(demo_race, measurevar = \"cr008a\", groupvars = c(\"race\"))\n\nrace_4 <- ggplot(race_unemploy, aes(x = race, y = cr008a, fill = race)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  coord_flip() +\n  scale_fill_jco() +\n  labs(x = \"\", y = \"Perceived risk of unemployment (%)\")\n\n# Perceived financial risk (cr009) by Race\nrace_finance <- summarySE(demo_race, measurevar = \"cr009\", groupvars = c(\"race\"))\n\nrace_3 <- ggplot(race_finance, aes(x = race, y = cr009, fill = race)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  coord_flip() +\n  scale_fill_jco() +\n  labs(x = \"\", y = \"Perceived risk of running out of money (%)\")\n\nggarrange(race_1, race_2, race_3, race_4,\n  common.legend = TRUE, legend = \"none\",\n  ncol = 2, nrow = 2\n)\n\n\n\n\nTime\nPerceived financial risk and unemployment risk appear to peak at the late March, within the timeframe of this survey.\nAn increasing perceived risk of infection and death related to Coronavirus were noted during March, 2020.\n\n\ndemo$end_date <- as.Date(demo$end_date, format = \"%d%b%Y\") # convert date to the right format\n\ntime_stat <- demo %>% # Calculate daily average value for each risk items to create line chart\n  select(cr005, cr006, cr009, cr008a, end_date) %>%\n  group_by(end_date) %>%\n  dplyr::summarise(n = n(), infection = mean(cr005), death = mean(cr006), unemployment = mean(cr008a), financial = mean(cr009))\n\nt1 <- ggplot(data = time_stat) + # Line chart 1\n  geom_line(aes(end_date, infection, color = \"Perceived risk of infection\")) +\n  geom_line(aes(end_date, death, color = \"Perceived risk of death\")) +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) + # hide legend text, and adjust legend font size\n  labs(y = \"Perceived Risk (%)\", x = \"Survey Date\")\n\n\nt2 <- ggplot(data = time_stat) +\n  geom_line(aes(end_date, financial, color = \"Perceived financial risk\")) + # Line chart2\n  geom_line(aes(end_date, unemployment, color = \"Perceived unemployment risk\")) +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) +\n  labs(y = \"Perceived Risk (%)\", x = \"Survey Date\")\n\n\nt3 <- ggplot(demo) +\n  geom_smooth(aes(end_date, cr005, color = \"Perceived financial risk\")) + # geom_smooth plot 1\n  geom_smooth(aes(end_date, cr006, color = \"Perceived risk of death\")) +\n  theme_bw() +\n  labs(y = \"\", x = \"Survey Date\")\n\nt4 <- ggplot(demo) +\n  geom_smooth(aes(end_date, cr008a, color = \"Perceived unemployment risk\")) + # geom_smooth plot 1\n  geom_smooth(aes(end_date, cr009, color = \"Perceived financial risk\")) +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) +\n  theme_bw() +\n  labs(y = \"\", x = \"Survey Date\")\n\n\nt5 <- ggplot(demo, aes(end_date, cr005)) +\n  geom_jitter(size = 1) +\n  theme_bw() +\n  geom_smooth() +\n  labs(x = \"\")\n\nt6 <- ggplot(demo, aes(end_date, cr006)) +\n  geom_jitter(size = 1) +\n  theme_bw() +\n  geom_smooth() +\n  labs(x = \"\")\n\nt7 <- ggplot(demo, aes(end_date, cr008a)) +\n  geom_jitter(size = 1) +\n  theme_bw() +\n  geom_smooth()\n\nt8 <- ggplot(demo, aes(end_date, cr009)) +\n  geom_jitter(size = 1) +\n  theme_bw() +\n  geom_smooth()\n\n\nggarrange(t1, t3,\n  ncol = 2, nrow = 1,\n  common.legend = TRUE, legend = \"top\"\n)\n\n\n\nggarrange(t2, t4,\n  ncol = 2, nrow = 1,\n  common.legend = TRUE, legend = \"top\"\n)\n\n\n\n# ggarrange(t5, t6,t7, t8,\n#           ncol = 2, nrow = 2,\n#           common.legend = TRUE, legend = \"top\")\n\n\n\n[The graph below is added by ML]\n\n\n# Edit by ML: Showing the variability in responses across time\nggplot(\n  demo,\n  aes(end_date, cr008a)\n) +\n  geom_jitter(width = 0.1, height = 0.5, alpha = 0.1, size = 0.5) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_bw() +\n  geom_smooth(linetype = \"dotted\") +\n  labs(y = \"Perceived Financial Risk (%)\")\n\n\n\n\nSummary\nThe survey was conducted in the early month of the COVID-19 outbreak in the US. There was a large variation in how people evaluate risks related to Coronavirus, but the findings suggested that people perception of risks experienced a significant increase compare to the beginning of the survey.\nPublication Statement\nThe analysis described in this page relies on data from survey(s) administered by the Understanding America Study, which is maintained by the Center for Economic and Social Research (CESR) at the University of Southern California. The content of this page is solely the responsibility of the authors and does not necessarily represent the official views of USC or UAS. Journalists and other reporters must refer readers to methodology information available at <uasdata.usc.edu> or election.uas.edu as appropriate.\n\n\n\n",
    "preview": "posts/2020-07-25-part-1-covid-19/covid_p1_files/figure-html5/gender-1.png",
    "last_modified": "2021-12-14T15:04:58-08:00",
    "input_file": {}
  }
]
