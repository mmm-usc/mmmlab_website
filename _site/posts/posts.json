[
  {
    "path": "posts/2023-04-19-correlation-attenuation-for-categorical-variables/",
    "title": "Correlation Attenuation for Categorical Variables",
    "description": "An illustration of correlation attenuation when discretizing a continuous variable to an ordered categorical variable.",
    "author": [
      {
        "name": "Gengrui (Jimmy) Zhang",
        "url": {}
      }
    ],
    "date": "2023-04-19",
    "categories": [
      "statistics",
      "correlation",
      "categorical"
    ],
    "contents": "\n\nContents\nAn Intro to Correlation Attenuation\nAn Example of Attenuated Correlation for Dichotomous Variable\nAn Example of Attenuated Correlation for Categorical Variable with Three Thresholds\n\nVerification with simulated data\nReasoning of Generalization to X and Y* with Any Means and Variances\nVerification with simulated data (random mean and variance)\n\nAn Intro to Correlation Attenuation\n          Correlation is the degree to which two variables associate with one another. The correlation formula between two random variables (i.e., X and Y) is:\n\\[\\rho(x,y) = \\frac{COV(X, Y)}{\\sigma_{X}\\sigma_{Y}},\\]\nwhere \\(\\sigma\\) is the standard deviation.\n          When one of the variable is categorized into dichotomous or categorical variables, the correlation \\(\\rho(X,Y)\\) will usually be attenuated due to loss of information.\nAn Example of Attenuated Correlation for Dichotomous Variable\n          Say \\(X\\) and \\(Y^*\\) have a correlation of .5 (i.e., \\(\\rho[X, Y^*] = .5\\)), \\(Y^*\\) is dichotomized into \\(Y\\) so that 30% of \\(Y\\) is 0 and 70% of \\(Y\\) is 1. What is the correlation between \\(X\\) and \\(Y\\) now?\n          Let’s simulate a dataset to see this attenuation:\n\n\n# Set correlation between X and Y* to 0.5\nrho <- 0.5\n\n# Assume X and Y*~N(0,1) for now\nsd_x <- 1\nsd_y <- 1\ncov_xy <- rho * sd_x * sd_y\n\n# Simulate correlated X and Y*\ndf <- as.data.frame(\n  mvrnorm(\n    n = 1e4,\n    mu = c(0, 0),\n    Sigma = matrix(\n      c(\n        sd_x^2, cov_xy,\n        cov_xy, sd_y^2\n      ),\n      ncol = 2\n    )\n  )\n)\nnames(df) <- c(\"X\", \"Y*\")\n\n# Manually dichotomize Y* to 0 and 1\ndf <- df %>%\n  mutate(Y = ifelse(`Y*` > qnorm(0.7, mean(`Y*`), sd(`Y*`)), 1, 0))\n\n# Show proportion of Y\nknitr::kable(table(df$Y) / nrow(df),\n  col.names = c(\"Label\", \"Proportion\"),\n  align = \"c\"\n)\n\nLabel\nProportion\n0\n0.6988\n1\n0.3012\n\n\n# Show correlations between X and Y*, and X and Y\nknitr::kable(\n  cbind(cor(df$X, df$`Y*`), cor(df$X, df$Y)),\n  col.names = c(\"$\\\\rho_{(X, Y*)}$\", \"$\\\\rho_{(X, Y)}$\"),\n  align = \"c\"\n)\n\n\\(\\rho_{(X, Y*)}\\)\n\\(\\rho_{(X, Y)}\\)\n0.5110921\n0.3819812\n\n          In this example, we can see the correlation is attenuated when one of the continuous variables is dichotomized. According to the correlation formula and expectation of covariance formula, we can derive the attenuation factor due to categorization. Note that the value of dichotomozing \\(Y^*\\) for desired proportion is called “threshold.”\n\\[\\text{Attenuation Factor} = \\frac{COV(X, Y)}{COV(X, Y^*)}*\\sqrt{\\frac{\\sigma^2_{Y^*}}{\\sigma^2_{Y}}},\\]\n          Now we can use the derived formula instead of simulated dataset to calculate the attenuated \\(R^2\\). (A quick review of how to compute variance, e.g., \\(\\sigma^2_{Y}\\), of a binary variable: \\(\\sigma^2_{Y} = \\mu_{Y}*[1 - \\mu _{Y}]\\)).\n\n\n# Analytic calculation\nthres <- qnorm(0.3)\nvar_ystar <- 1\nvar_y <- 0.7 * (1 - 0.7)\nattenuation_bi <- dnorm(thres) * sqrt(var_ystar / var_y)\ncor_xy_bi <- attenuation_bi * rho\n\n# Simulated results\nlat_cor <- cor(df$X, df$`Y*`)\nobs_cor <- cor(df$X, df$Y)\n\natt_fac <- (cov(df$X, df$Y) / cov(df$X, df$`Y*`)) * sqrt(var(df$`Y*`) / var(df$Y))\ncal_cor <- att_fac * lat_cor\n\n\n          Then we can compare the results from analytic calculation and simulated results:\n\n\nsummary_1 <- round(c(rho, attenuation_bi, cor_xy_bi, att_fac, cal_cor), 3)\nnames(summary_1) <- c(\n  \"Correlation_XY*\", \"Attenuation_Formula\",\n  \"Correlation_Formula\", \"Attenuation_Data\",\n  \"Correlation_Data\"\n)\nknitr::kable(\n  summary_1,\n  align = \"c\",\n  col.names = \" \"\n)\n\nCorrelation_XY*\n0.500\nAttenuation_Formula\n0.759\nCorrelation_Formula\n0.379\nAttenuation_Data\n0.747\nCorrelation_Data\n0.382\n\n          In the table, “Attenuation_Formula” and “Correlation_Formula” represent the attenuated amount and correlation from the analytic formula, wheres “Attenuation_Data” and “Correlation_Data” represent those from simulated data.\n          We can see that the covariances between \\(X\\) and \\(Y^*\\) are needed to compute the attenuation factor, which requires raw data. Sometimes, however, researchers may have difficulties obtaining raw data or they only have the correlation between \\(X\\) and \\(Y\\) reported in published articles.\n\\[\\text{Attenuation Factor} = \\frac{E(XY) - E(X)E(Y)}{E(XY^*) - E(X)E(Y^*)}*\\sqrt{\\frac{\\sigma^2_{Y^*}}{\\sigma^2_{Y}}},\\]\n          If we further expand the covariance terms, they can be computed as long as we are able to obtain the information of each part (e.g., \\(E[XY]\\)).\n          Let’s suppose \\(X\\) and \\(Y^*\\) both follow standard normal distribution (i.e., \\(N(0,1)\\)) for simplicity. Under this condition, \\(E(X)E(Y)\\) and \\(E(X)E(Y^*) = 0\\) because \\(E(X) = 0\\).\n          For \\(E(XY^*)\\), we can use the correlation and covariance formula to prove that:\n\\[\n\\begin{aligned}\n  \\rho(X, Y^*) &= \\frac{E(XY^*)}{\\sqrt{\\sigma^2_{X} \\sigma^2_{Y^*}}} \\\\\n               &= E\\left[\\left(\\frac{X - \\mu_{x}}{\\sigma_{X}}\\right)\\left(\\frac{Y^* -        \\mu_{Y^*}}{\\sigma_{Y^*}}\\right)\\right] \\\\\n               &= E(XY^*), \\\\\n\\end{aligned}\n\\]\ngiven that the correlation of \\(X\\) and \\(Y^*\\) is their standardized covariance. Then \\(E(XY)\\) can be calculated using thresholds of the categorical variable.\n          Before we move forward, I would like to discuss two ways of calculating the \\(E(XY)\\) in the example of binary variable. One important formula we need is the probability density function (p.d.f.) of a standard bivariate normal distribution:\n\\[f_{x,y^{*}}(x,y^{*}) = \\frac{1}{2\\pi\\sqrt{1 - \\rho^2}}*e^{-\\frac{1}{2(1 - \\rho^2)}*[x^2 + {y^*}^2 - 2\\rho x y^*]},\\]\nand the expected value of \\(XY^*\\) is:\n\\[E(XY^*) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xy^{*}f_{x,y^{*}}(x,y^{*}) d(x) d(y^*),\\]\n          When \\(Y^*\\) is dichotomized into \\(Y\\) with a threshold (i.e., \\(\\tau\\)), it means that \\(Y^*\\) is truncated because 30% of \\(Y^*\\) is 0. The expected value of \\(XY\\) becomes a conditional expected value when \\(Y^* > \\tau\\):\n\\[E(XY) = E(X | Y^* > \\tau),\\]\nwhile \\(E(XY) = E(X | Y^* < \\tau) = E(X*0) = 0\\).\nFurther, the formula can be expanded as:\n\n\\[\n\\begin{aligned}\n  E(X | Y^* > \\tau) &= \\int_{-\\infty}^{\\infty}\\int_{\\tau}^{\\infty}xy^{*}f_{x,y^{*}}(x,y^{*}) d(x) d(y^*) \\\\\n  &= \\int_{-\\infty}^{\\infty}\\int_{\\tau}^{\\infty}x,y^{*}\\frac{1}{2\\pi\\sqrt{1 - \\rho^2}}*e^{-\\frac{1}{2(1 -              \\rho^2)}*[x^2 + y^{*^2} - 2\\rho x y^*]} d(x) d(y^*), \\\\\n\\end{aligned}\n\\]\n          We can see that the cumulative probability is determined by the density value, or marginal distribution, of \\(Y^*\\). Specifically, it is determined by the threshold \\(\\tau\\). Note that the marginal distribution of one variable in a bivariate normal distribution is a normal distribution. Now we think of \\(E(X | Y^* = a)\\). It can be thought of the marginal distribution of X at \\(Y^* = a\\) and \\(E(X)\\) does not depend on \\(\\rho\\).\n\\[\n\\begin{aligned}\n  E(X | Y^* = a) &= \\int_{-\\infty}^{\\infty} xf_{X|Y^*}(x|Y^* = y^*)d(x) \\\\\n  &= \\rho*a, \\\\\n\\end{aligned}\n\\]\ngiven that \\(f_{X|Y^*}(x|Y^* = y^*) = \\frac{f_{X,Y^*}(x, y^*)}{f_{Y^*}(y^*)}\\), and \\(\\rho\\) is the correlation between \\(X\\) and \\(Y^*\\).\nNow we think of \\(E(X | Y^* > a)\\):\n\\[\n\\begin{aligned}\n  E(X | Y^* > a) &= E(X - rY^* + rY^* | Y^* > a) \\\\\n  COV(X - rY^*, Y^*) &= E[(X - rY^*)Y^*] - E(X - rY^*)E(Y^*) \\\\\n  COV(X - rY^*, Y^*) &= E(XY^*) - rE(Y^*Y^*) - E(X)E(Y^*) + rE(Y^*)E(Y^*), \\\\\n\\end{aligned}\n\\]\nsince we assume that \\(X\\) and \\(Y^*\\) both follow \\(N ~ (0,1)\\), therefore \\(COV(X, Y^*) = E(XY^*) - E(X)E(Y^*) = r\\) and \\(VAR(Y^*) = E(Y^*)E(Y^*) - E(Y^*Y^*) = 1\\). Then,\n\\[\n\\begin{aligned}\n  COV(X - rY^*, Y^*) &= COV(X, Y^*) - r*1 \\\\\n  &= r - r \\\\\n  &= 0, \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  E(X | Y^* > a) &= E(X - rY^* + rY^* | Y^* > a) \\\\\n  &= E(X - rY^*|Y^* > a) + r*E(Y^*|Y^* > a), \\\\\n\\end{aligned}\n\\]\nWe have shown that the correlation between \\(X - rY^*\\) and \\(Y^*\\) is 0, and thus \\(E(X - rY^*)\\) is independent of \\(Y^*\\). Furthermore,\n\\[\n\\begin{aligned}\n  E(X | Y^* > a) &= E(X - rY^*) + r*E(Y^*|Y^* > a) \\\\\n  &= E(X) - r*E(Y^*) + r*E(Y^*|Y^* > a) \\\\\n  &= r*E(Y^*|Y^* > a), \\\\\n\\end{aligned}\n\\]\nAccording to the definition of conditional expectation,\n\\[\n\\begin{aligned}\n  E(Y^*|Y^* > a) &= \\int_{a}^{\\infty} y^*\\phi_{0}(y^*)d(y^*) \\\\\n  &= \\frac{e^{-\\frac{a^2}{2}}}{\\sqrt{2\\pi}}, \\\\\n\\end{aligned}\n\\]\nnote that \\(\\phi_{0}\\) is the p.d.f. of standard normal distribution.\n          The dnorm() function in R calculates the p.d.f. of the normal distribution. For standard normal distribution, the R function dnorm(a) would return \\(\\frac{e^{-\\frac{a^2}{2}}}{\\sqrt{2\\pi}}\\), which is the value of \\(E(Y^*|Y^* > a)\\). Thus, we are able to show that:\n\n\\[E(X | Y^* > a) = \\rho*\\phi_0(a)\\]\n          In the following sections, I’ll use an example of 4-category variable to show how \\(E(XY)\\) can be computed for each response category using the probability density function of bivariate normal random variables and their cumulative probabilities.\n          The information of other parts are available with threshold values if \\(X\\) and \\(Y^*\\) follow normal distribution. Thus, we are able to calculate the attenuated correlation without the need of covariances but only the correlation \\(\\rho(X, Y^*)\\).\nAn Example of Attenuated Correlation for Categorical Variable with Three Thresholds\n          Given \\(Y^*\\) is discretized into \\(Y\\) with 4 categories (ie., 50% is 0, 30% is 1, 10% is 2, 10% is 3), what is the correlation between \\(X\\) and \\(Y\\)?\n\n\n# Assuming X and Y* ~ N(0,1)\n# for standard bivariate normal distribution, E(XY*) = rho\nrho <- 0.5\nvar_ystar <- 1\n\nthres_1 <- qnorm(0.5)\nthres_2 <- qnorm(0.5 + 0.3)\nthres_3 <- qnorm(0.5 + 0.3 + 0.1)\n\np_less_than_thres1 <- pnorm(thres_1)\np_thres1_thres2 <- pnorm(thres_2) - pnorm(thres_1)\np_thres2_thres3 <- pnorm(thres_3) - pnorm(thres_2)\np_larger_than_thres3 <- pnorm(thres_3, lower.tail = F)\n\ne_y2 <- 0 * p_less_than_thres1 +\n  1^2 * p_thres1_thres2 +\n  2^2 * p_thres2_thres3 +\n  3^2 * p_larger_than_thres3\ne_y <- 1 * p_thres1_thres2 + 2 * p_thres2_thres3 + 3 * p_larger_than_thres3\nvar_y <- e_y2 - e_y^2\nattenuation_cat <- 1 * (dnorm(thres_1) - dnorm(thres_2)) +\n  2 * (dnorm(thres_2) - dnorm(thres_3)) +\n  3 * dnorm(thres_3) * sqrt(var_ystar / var_y)\n# attenuation_cat <- (0*pbnorm(-Inf, Inf, -Inf, thres_1, 0, 0, 1, 1, 0.5) +\n#   1*pbnorm(-Inf, Inf, thres_1, thres_2, 0, 0, 1, 1, 0.5) +\n#   2*pbnorm(-Inf, Inf, thres_2, thres_3, 0, 0, 1, 1, 0.5) +\n#   3*pbnorm(-Inf, Inf, thres_3, Inf, 0, 0, 1, 1, 0.5))/rho * sqrt(var_ystar/var_y)\ncor_xy_cat <- attenuation_cat * rho\n\n\nVerification with simulated data\n\n\nrho <- 0.5\nsd_x <- 1\nsd_y <- 1\ncov_xy <- rho * sd_x * sd_y\n\ndf3 <- as.data.frame(mvrnorm(\n  n = 1e4,\n  mu = c(0, 0),\n  Sigma = matrix(\n    c(\n      sd_x^2, cov_xy,\n      cov_xy, sd_y^2\n    ),\n    ncol = 2\n  )\n))\n\n\nnames(df3) <- c(\"y1\", \"y2\")\n\n# HL: An easier way to do the categorization:\n# findInterval(\n#   df3$y2,\n#   rightmost.closed = TRUE,\n#   quantile(df3$y2, c(0, 0.5, 0.5 + 0.3, 0.5 + 0.3 + 0.1, 1))\n# ) - 1  # if starting from 0\n# The above is based on the sample quantiles without\n# assuming normality. If you want to assume normality, try\n# findInterval(\n#   df3$y2,\n#   rightmost.closed = TRUE,\n#   qnorm(c(0, 0.5, 0.5 + 0.3, 0.5 + 0.3 + 0.1, 1),\n#         mean = mean(df3$y2), sd = sd(df3$y2))\n# ) - 1\n# Could you update the following accordingly? Thanks.\ndf3$y2_mul <- findInterval(\n             df3$y2,\n             rightmost.closed = TRUE,\n             qnorm(c(0, 0.5, 0.5 + 0.3, 0.5 + 0.3 + 0.1, 1),\n                   mean = mean(df3$y2), sd = sd(df3$y2))\n           ) - 1\n\nlat_cor <- cor(df3$y1, df3$y2)\nobs_cor <- cor(df3$y1, df3$y2_mul)\n\natt_fac <- (cov(df3$y1, df3$y2_mul) / cov(df3$y1, df3$y2)) * sqrt(var(df3$y2) / var(df3$y2_mul))\ncal_cor <- att_fac * lat_cor\n\n\n\n\nsummary_2 <- round(c(rho, attenuation_cat, cor_xy_cat, att_fac, cal_cor), 3)\nnames(summary_2) <- c(\n  \"Correlation_XY*\", \"Attenuation_Formula\",\n  \"Correlation_Formula\", \"Attenuation_Data\",\n  \"Correlation_Data\"\n)\nknitr::kable(summary_2,\n  align = \"c\",\n  col.names = \" \"\n)\n\nCorrelation_XY*\n0.500\nAttenuation_Formula\n0.865\nCorrelation_Formula\n0.433\nAttenuation_Data\n0.876\nCorrelation_Data\n0.430\n\nReasoning of Generalization to X and Y* with Any Means and Variances\n          We would like to prove that the attenuation of correlation based on standard normal \\(X\\) and \\(Y^*\\) is generalizeable to \\(X\\) and \\(Y^*\\) with any means and variances when \\(Y^*\\) is categorized with any number of categories.\n\n          Let’s say \\(Y^{*}\\) is categorized to \\(Y\\) with \\(c\\) categories (c = 4; [0, 1, 2, 3]),\n\\[\n\\begin{aligned}\nE(X,Y) =\n  \\begin{cases}\n    0, & \\text{if } Y^{*} \\le \\tau_{1} \\\\  \n    E(X | Y = 1), & \\text{if } \\tau_{1} \\le Y^{*} \\le \\tau_{2} \\\\\n    E(X | Y = 2), & \\text{if } \\tau_{2} \\le Y^{*} \\le \\tau_{3} \\\\\n    E(X | Y = 3), & \\text{if } Y^{*} > \\tau_{3}\n  \\end{cases}\n\\end{aligned}\n\\]\nTake one category as one example, for \\(E(X | Y = 1)\\) with \\(\\tau_{1} \\le Y^{*} \\le \\tau_{2}\\) and \\(c = 1\\): \\(E(X | Y = 1) = \\int_{-\\infty}^{\\infty} \\int_{\\tau_{1}}^{\\tau_{2}} \\text{x} y^{*} f_{(x, y^{\\ast})} d_{x} d_{y^{*}}\\)\n          Now the distributions of X and Y are dependent on their mean and variance. We can use the z-scores to substitute limits of integrals.\n          Let \\(z_{x} = \\frac{x - \\mu_{x}}{\\sigma_{x}}\\) and \\(z_{y^{*}} = \\frac{y^{*} - \\mu_{y^{*}}}{\\sigma_{y^{*}}}\\), then,\n\\[f_{x,y^{*}}(x,y^{*}) = \\frac{1}{2\\pi\\sigma_{x}\\sigma_{y^{*}}\\sqrt{1 - \\rho^2}}*e^{-\\frac{1}{2(1 - \\rho^2)}*[z_{x}^2 + z_{y^{*}}^2 - 2\\rho z_{x} z_{y^{*}}]},\\]\nand transform the formula to:\n\\[E(X | Y = 1) = \\int_{-\\infty}^{\\infty}\\int_{\\tau_{1}}^{\\tau_{2}}xy^{*}\\frac{d_{x}d_{y^{*}}}{2\\pi\\sigma_{x}\\sigma_{y^{*}}\\sqrt{1 - \\rho^2}}*e^{-\\frac{1}{2(1 - \\rho^2)}*[z_{x}^2 + z_{y^{*}}^2 - 2\\rho z_{x} z_{y^{*}}]} d(x) d(y^*),\\]\nBecause of the property of derivation,\n\\[\n\\begin{aligned}\n  \\frac{d(x)}{\\sigma_{x}} &= d(\\frac{x - \\mu_{x}}{\\sigma_{x}}) \\\\\n  &= d(z_{x}),\n\\end{aligned}\n\\]\nand it is the same for \\(d(z_{y^{*}})\\).\n          Thus, the equation of \\(E(X | Y = 1)\\) becomes:\n\\[E(X | Y = 1) = \\int_{-\\infty}^{\\infty}\\int_{\\frac{\\tau_{1} - \\mu_{y^{*}}}{\\sigma_{y^{*}}}}^{\\frac{\\tau_{2} - \\mu_{y^{*}}}{\\sigma_{y^{*}}}}xy^{*}\\frac{1}{2\\pi\\sqrt{1 - \\rho^2}}*e^{-\\frac{1}{2(1 - \\rho^2)}*[z_{x}^2 + z_{y^{*}}^2 - 2\\rho z_{x} z_{y^{*}}]}d(z_{x})d(z_{y^{*}})\\]\n          The “new” values of limits, e.g., \\(\\frac{\\tau_{1} - \\mu_{y^{*}}}{\\sigma_{y^{*}}}\\), are linear tranformed using the mean and variance of \\(Y^{*}\\). It means that no matter how threshold values change due to mean and variance of \\(Y^{*}\\), we can always z-tranform them back so that X and \\(Y^{*}\\) always follow a standard bivariate normal distribution. In other words, as long as we know the threshold values and proportion of categories of the categorized variable, and X and \\(Y^{*}\\) follow normal distributions, we should be able to compute the attenuated \\(R^2\\) no matter the mean and variance of \\(Y^{*}\\).\nVerification with simulated data (random mean and variance)\n          Now it’s time to verify if our reasoning works with any means and variances for dichotomous \\(Y\\) and categorical \\(Y\\).\n\n\nrho <- 0.5\nsd_x <- rnorm(1, 1, 0.5)\nsd_y <- rnorm(1, 1.5, 0.3)\ncov_xy <- rho * sd_x * sd_y\n\ndf2 <- as.data.frame(mvrnorm(\n  n = 1e7,\n  mu = c(rnorm(1, 10, 2.1), rnorm(1, 8, 1.1)),\n  Sigma = matrix(\n    c(\n      sd_x^2, cov_xy,\n      cov_xy, sd_y^2\n    ),\n    ncol = 2\n  )\n))\n\nnames(df2) <- c(\"y1\", \"y2\")\ndf2 <- df2 %>%\n  mutate(y2_cat = ifelse(y2 > qnorm(0.7, mean(df2$y2), sd(df2$y2)), 1, 0))\n\nlat_cor <- cor(df2$y1, df2$y2)\nobs_cor <- cor(df2$y1, df2$y2_cat)\n\natt_fac <- (cov(df2$y1, df2$y2_cat) / cov(df2$y1, df2$y2)) * sqrt(var(df2$y2) / var(df2$y2_cat))\ncal_cor <- att_fac * lat_cor\n\n\n\n\nsummary_3 <- round(c(rho, attenuation_bi, cor_xy_bi, att_fac, cal_cor), 3)\nnames(summary_3) <- c(\n  \"Correlation_XY*\", \"Attenuation_Formula\",\n  \"Correlation_Formula\", \"Attenuation_Data\",\n  \"Correlation_Data\"\n)\nknitr::kable(summary_3,\n  align = \"c\",\n  col.names = \" \"\n)\n\nCorrelation_XY*\n0.500\nAttenuation_Formula\n0.759\nCorrelation_Formula\n0.379\nAttenuation_Data\n0.759\nCorrelation_Data\n0.379\n\n\n\nrho <- 0.5\nsd_x <- rnorm(1, 1, 0.5)\nsd_y <- rnorm(1, 1.5, 0.3)\ncov_xy <- rho * sd_x * sd_y\n\ndf3 <- as.data.frame(mvrnorm(\n  n = 1e7,\n  mu = c(rnorm(1, 10, 2.1), rnorm(1, 8, 1.1)),\n  Sigma = matrix(\n    c(\n      sd_x^2, cov_xy,\n      cov_xy, sd_y^2\n    ),\n    ncol = 2\n  )\n))\n\n\nnames(df3) <- c(\"y1\", \"y2\")\n\ndf3 <- df3 %>%\n  mutate(y2_mul = ifelse(y2 < qnorm(0.5, mean(df3$y2), sd(df3$y2)), 0,\n    ifelse(qnorm(0.5, mean(df3$y2), sd(df3$y2)) < y2 & y2 < qnorm(0.5 + 0.3, mean(df3$y2), sd(df3$y2)), 1,\n      ifelse(qnorm(0.5 + 0.3, mean(df3$y2), sd(df3$y2)) < y2 & y2 < qnorm(0.5 + 0.3 + 0.1, mean(df3$y2), sd(df3$y2)), 2,\n        ifelse(y2 > qnorm(0.5 + 0.3 + 0.1, mean(df3$y2), sd(df3$y2)), 3, NA)\n      )\n    )\n  ))\n\nlat_cor <- cor(df3$y1, df3$y2)\nobs_cor <- cor(df3$y1, df3$y2_mul)\n\natt_fac <- (cov(df3$y1, df3$y2_mul) / cov(df3$y1, df3$y2)) * sqrt(var(df3$y2) / var(df3$y2_mul))\ncal_cor <- att_fac * lat_cor\n\n\n\n\nsummary_4 <- round(c(rho, attenuation_cat, cor_xy_cat, att_fac, cal_cor), 3)\nnames(summary_4) <- c(\n  \"Correlation_XY*\", \"Attenuation_Formula\",\n  \"Correlation_Formula\", \"Attenuation_Data\",\n  \"Correlation_Data\"\n)\nknitr::kable(summary_4,\n  align = \"c\",\n  col.names = \" \"\n)\n\nCorrelation_XY*\n0.500\nAttenuation_Formula\n0.865\nCorrelation_Formula\n0.433\nAttenuation_Data\n0.872\nCorrelation_Data\n0.436\n\n          It seems that the comparison of attenuated \\(R^2\\) values calculated by the formula and from the simulated results are highly similar.\n\n\n\n",
    "preview": {},
    "last_modified": "2024-02-19T23:30:40-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-12-05-git-workflow/",
    "title": "Git Workflow",
    "description": "A brief overview of the git workflow and a demonstration of the git workflow for collaboration.",
    "author": [
      {
        "name": "Meltem Ozcan",
        "url": "https://quantscience.rbind.io"
      }
    ],
    "date": "2022-11-28",
    "categories": [
      "git",
      "version control"
    ],
    "contents": "\n\nContents\nGit concepts\nCommits\nBranches\nPull Requests\nProjects\n\nDemo\nReferences and helpful Git resources:\n\n\nThe goal for this demonstration is to equip the readers with a basic understanding\nof concepts relevant to version control with git, and to demonstrate how git can be\nused for collaboration among multiple team members.\nGit concepts\nGit allows multiple users to synchronously write and edit code locally, fix bugs\nand resolve any conflicts with others’ work before accepting (or rejecting) and\npropagating the changes. Users can go back to previous versions of their work as\nneeded, allowing for an relatively risk free coding environment. Version control\nis made possible through the use of tree-like data structures, and the distinction\nbetween the local and remote repositories. The remote repository contains the\nversion(s) of the project hosted on the internet and can be accessed by collaborators.\nAll changes to code remain local until they are pushed to the remote. The\nlocal repository refers to versions of the project on someone’s local\ncomputer, and consists of 3 ‘trees’: 1. working directory (the files), 2. index\n(the staging area), 3. HEAD (the pointer to the most recent commit).\nCommits\nA commit is a snapshot/copy/state of the local project at a specific point\nin time, and commits can be thought of as periodic checkpoints that make it easier\nto backtrack/understand previous work and to troubleshoot. See Git best practices about commits\nfor more details and tips. Committing early and often is strongly recommended.\nFirst, the file is staged for commit and added to\nthe index by calling git add <filename> in the terminal. Multiple files can be\nstaged at once by specifying file names with spaces in between:\ngit add <filename1> <filename2>, and all files can be staged using git *.\nOnce files are staged, the changes can be committed as below:\ngit commit -m “<meaningful message here>”.\nOnce the commit is completed, HEAD moves up to match the newest commit. A history\nof commits and commit messages can be viewed using git log, which pulls up the\nlatest commits in chronological order. A more readable list of commits can be\nproduced using: git log --pretty=format:\"%h - %an, %ar : %s\".\nBranches\nA branch is a pointer to a specific commit. Commonly, three types of branches\nare used:\nMain/master branch: the primary branch automatically created when the\nrepository is cloned from the origin, which is a remote repository. The main/master\nbranch can be thought of as “production-ready” and is only updated when the develop branch\nis stable with new version updates. As such, it contains an abridged history of commits.\nDevelop (dev) branch: the secondary branch that contains full history of\ncommits. Supporting branches are created from and merged into dev. The separation\nbetween main/master and dev branches functions as a check against unstable/buggy commits\nbeing prematurely pushed to the main/master branch.\nSupporting branches: the short-lived snapshots created to build or test new\nfeatures, fix glitches etc. and are deleted after a merge. Ideally each\nsupporting branch is used for an isolated task or feature.\nThe current branch and its status (in terms of modified files and in comparison\nwith origin/) can be checked with git status. Any staged files will\nappear in green.\nThe user can list all branches with git branch -a, and all remote branches\nwith git branch -r. The current branch is marked with an asterisk.\nIn order to create a new branch, the user can navigate to the branch to build from\n(e.g., dev), and use\ngit checkout -b <new branch name>\nAs with commits, a new branch is local until it is pushed to the remote repository.\nWhen pushing a branch for the first time, git push will give an error as the\nnew local branch does not have an upstream branch. This error is solvable by using\ngit push --set-upstream origin <name of the new branch>\nOr, the user can directly push to the origin by\ngit push origin <name of the new branch>\nAfter the first commit, git push is sufficient to push commits to the remote.\ngit branch -d <branch name> can be used to delete a local branch that has been\npushed and merged with the remote branch. Use -D to force delete a branch that\nhasn’t been merged into the remote.\ngit push <remote name> -- delete <branch name> deletes the remote branch.\ngit fetch --all --prune grabs all changes from remotes and locally deletes the\nfiles/branches that were deleted remotely.\ngit fetch grabs all the changes from remotes. This command does not delete\nlocal files.\ngit pull downloads the most recent version of the branch from the remote.\nRemember to pull first before any edits, as there may have been modifications to\ncode by others.\ngit diff illustrates changes since the last commit, git diff <filename>\nshows changes to a particular file.\nPull Requests\nWhen changes from a supporting branch are ready to be merged into dev, a pull\nrequest can be put in from the browser. The user can specify a topic and an\noptional description to explain to the reader what the PR is about. It is important\nto double check which branch the PR is set to merge the supporting branch into,\nas the browser might default to main rather than dev. The following summarizes\nthe main steps of a pull request: pull -> edit -> commit with message -> push -> put in a pull request (PR) through the browser.\nIf there are no conflicts and the user has no suggestions/edits, the user can\nautomatically merge and delete the supporting branch.\nIf there are conflicts, the user can view and solve these either via the browser\nor in RStudio. Through the browser, the user can check which files have conflicts\nunder the ‘files changed’ tab for the pull request.\nThe browser has two helpful viewing options that make it easier to sort through\nmultiple changes: split view or unified view.\nIf a user would like a code review on the changes made or would like collaborators\nto review and accept/reject their changes, a pull review can be requested from\nspecific individuals who will receive a notification to view, comment on, or\nmerge the pull request. If multiple individuals need to weigh in before the merge,\neach individual can click ‘approve’ to show that they are comfortable with the\nchanges/edits (or request edits/reject otherwise).\nWhile reviewing a PR with multiple files, progress can be tracked by clicking ‘Viewed’.\nIf the user sees errors/has suggestions, they can leave comments on specific lines or chunks of code by\nclicking on the blue comment icon on the left side. If the task is substantial,\nan issue can be created for it and specific individuals can be assigned to the task.\nProjects\nEach issue can be assigned to a project, which makes it easier to keep track of different ongoing projects’ progress.\nDepending on the priority and status of the issues (‘cards’) within a project,\nthey can be moved under different headings for easier management as well as\ntracking of ideas: backlog, to-do, in progress, on hold/blocked, done, won’t do, etc.\nFor bigger tasks, it is helpful to write a description/comment for the card to\nprovide some scaffolding/reminders for the assignee.\nIf setting up new repository:\nNavigate to Github profile -> Repositories tab -> New -> fill in repository name and click ‘add a README file’ -> Create repository\nNavigate to the new repository -> copy the HTTPS or SSH link from under the Code tab -> pull up RStudio (or VSCode) terminal -> clone repository to local machine by typing <username>$ git clone <URL>\nin the terminal.\nNote that the project will be cloned into the current working directory, which\ncan be checked in the terminal with the pwd command.\nIn the terminal, navigate to the repository with command cd <name or path to repo>.\nDemo\nThis is a quick demo that can be worked on as a group to get familiar with the\ngit workflow for collaboration. It was created with four collaborators in mind\nbut can be modified as needed.\nFirst, one person creates a new repo and creates a dev branch from main.\nEveryone else clones the new repo to their local machine.\nOne person creates a git project in the browser for the demo with tabs “to do”,\n“done”, “won’t do”.\nEach person adds a card for a task from the list below to the “to do” column of\nthe new project. The fifth task should alsp be added to the “to do” column, but\nwill not be worked on.\nTask 1. Create an R file “string.R” which has one function “split_list” that\ntakes in a single argument. split_list splits a given long string into a list\nof words (e.g., “heavy rain” to list(“heavy”, “rain”) and returns this list.\nTask 2. Create an R file “main.R” that sources an R file “string.R”. Assign the\nreturned list from split_list(“Measurement and Multilevel Modeling Lab”) to an\nobject. Write a for loop that loops over each element in the object and prints it.\nTask 3. Create an R file “and.R” which has one function “checkifand” which takes\nin one argument. Returns TRUE if the argument provided is “and” and FALSE if\nthere is no match.\nTask 4. Modify main.R to also source add.R. Edit the for loop such that any\nmatches to “and” are not printed.\nTask 5. Create an R file “reorder.R” that takes in a list and reorders the\nelements such that the first element is now the last element and so on.\nOnce the cards have been created on the browser project, the first four tasks\nare split between the individuals. The cards can be converted into issues and\nassigned to specific individuals.\nEach person creates a supporting branch from dev with a suitable name and\nchecks out the branch on their local machine.\nEveryone takes 5 minutes to make some progress on their designated task and\npull, commit, pushes their changes to the remote following the best practices\ndiscussed above. When a task is complete and the change has been pushed to the\nremote, each person can create a pull request from their branch to dev, and assign\none other person from the group to review their pull request (PR).\nWhile the updates are being made, experiment with git fetch, git status,\ngit branch -a and git branch -r to see how each commit changes the repositories.\nEach person reviews a PR, either approves edits or suggests a simple change.\nIf everything works as expected, the branch can be merged into dev from the browser\nand the supporting branch can be deleted.\nAgain, everyone experiments with commands like git fetch, git status,\ngit branch -a and git branch -r.\nFinally, fetch with pruning to delete local versions of deleted branches.\nEach person moves their git project card to the appropriate tab, and one\nperson moves the fifth task card to the “won’t do” tab.\nReferences and helpful Git resources:\ngit - the simple guide\nThink like (a) Git - a guide for the perplexed\nCheatsheet\nHello world tutorial\n\n\n\n",
    "preview": {},
    "last_modified": "2024-02-19T23:30:40-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-10-confidence-intervals-for-multilevel-r-squared/",
    "title": "Confidence Intervals for Multilevel R-Squared",
    "description": "A demonstration of obtaining confidence intervals for multilevel R-squared effect size using parametric and residual multilevel bootstrapping.",
    "author": [
      {
        "name": "Hok Chio (Mark) Lai",
        "url": "https://quantscience.rbind.io"
      }
    ],
    "date": "2022-05-10",
    "categories": [
      "Bootstrap",
      "Multilevel Modeling",
      "Statistics"
    ],
    "contents": "\n\nContents\nLoad Packages\nAn Example Multilevel\nModel\nNakagawa-Johnson-Schielzeth \\(R^2\\)\nRight-Sterba \\(R^2\\)\n\nConfidence Intervals for\n\\(R^2\\)\nParametric Bootstrap\nBias-corrected estimate\nConfidence intervals\n\nResidual Bootstrap\nConfidence Intervals\n\nBootstrap CI With\nTransformation\nConclusion\n\nLoad Packages\n\n\nlibrary(lme4)\nlibrary(MuMIn)  # for computing multilevel R-squared\nlibrary(r2mlm)  # another package for R-squared\nlibrary(bootmlm)  # for multilevel bootstrapping\nlibrary(boot)  # for bootstrap CIs\n\n\n\nAn Example Multilevel Model\n\n\nfm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)\n\n\n\nNakagawa-Johnson-Schielzeth\n\\(R^2\\)\n\n\nr.squaredGLMM(fm1)\n\n\n           R2m       R2c\n[1,] 0.2786511 0.7992199\n\nThe marginal \\(R^2\\) considers the\ntotal variance accounted for due to the fixed effect associated with the\npredictors (Days in this example). See Nakagawa, Johnson, &\nSchielzeth (2017) for more information.\nRight-Sterba \\(R^2\\)\nMore fine-grained partitioning, as described in Rights & Sterba\n(2019)\n\n\nr2mlm(fm1)\n\n\n\n$Decompositions\n                     total\nfixed           0.27851304\nslope variation 0.08915267\nmean variation  0.43165365\nsigma2          0.20068063\n\n$R2s\n         total\nf   0.27851304\nv   0.08915267\nm   0.43165365\nfv  0.36766572\nfvm 0.79931937\n\nThe fixed part is the same as the marginal \\(R^2\\).\nConfidence Intervals for \\(R^2\\)\nNeither MuMIn::r.squaredGLMM() nor\nr2mlm::r2mlm() provided confidence intervals (CIs) for the\n\\(R^2\\), but general guidelines for\neffect size reporting would suggest always reporting CIs for point\nestimates of effect size, just like for any point estimates in\nstatistics. We can use multilevel bootstrapping to get CIs.\nTo do bootstrap, first defines an R function that gives the target\n\\(R^2\\) statistics. We can do it for\nthe marginal \\(R^2\\):\n\n\nmarginal_r2 <- function(object) {\n  r.squaredGLMM(object)[[1]]\n}\nmarginal_r2(fm1)\n\n\n[1] 0.2786511\n\nParametric Bootstrap\nThe lme4::bootMer() supports basic parametric multilevel\nbootstrapping\n\n\n# This takes about 30 sec on my computer\nboo01 <- bootMer(fm1, FUN = marginal_r2, nsim = 999)\nboo01\n\n\n\nPARAMETRIC BOOTSTRAP\n\n\nCall:\nbootMer(x = fm1, FUN = marginal_r2, nsim = 999)\n\n\nBootstrap Statistics :\n     original      bias    std. error\nt1* 0.2786511 0.005759548  0.07545455\n\nHere is the bootstrap distribution\n\n\nplot(boo01)\n\n\n\n\nBias-corrected estimate\nThe above shows that the sample estimate of \\(R^2\\) was upwardly biased. To correct for\nthe bias, we can use the bootstrap bias-corrected estimate\n\n\n2 * boo01$t0 - mean(boo01$t)\n\n\n[1] 0.2728915\n\nConfidence intervals\nYou can get three types of bootstrap CIs (\"norm\",\n\"basic\", \"perc\") with\nbootMer:\n\n\nboot::boot.ci(boo01, index = 1, type = c(\"norm\", \"basic\", \"perc\"))\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 999 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = boo01, type = c(\"norm\", \"basic\", \"perc\"), \n    index = 1)\n\nIntervals : \nLevel      Normal              Basic              Percentile     \n95%   ( 0.1250,  0.4208 )   ( 0.1088,  0.4114 )   ( 0.1460,  0.4485 )  \nCalculations and Intervals on Original Scale\n\nResidual Bootstrap\nThe bootmlm::bootstrap_mer() implements the residual\nbootstrap, which is robust to non-normality.\n\n\n# This takes about 30 sec on my computer\nboo02 <- bootstrap_mer(fm1, FUN = marginal_r2, nsim = 999, type = \"residual\")\nboo02\n\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nbootstrap_mer(x = fm1, FUN = marginal_r2, nsim = 999, type = \"residual\")\n\n\nBootstrap Statistics :\n     original      bias    std. error\nt1* 0.2786511 0.005397786  0.08418536\n\nIn this example, the results are similar. The boostrap bias-corrected\nestimate of \\(R^2\\), and the three\nbasic CIs, can similarly be computed as in parametric bootstrap.\nConfidence Intervals\nIn addition to the three CIs previously discussed, which are\nfirst-order accurate, we can also obtain CIs that are second-order\naccurate: (a) bias-corrected and accelerated (BCa) CI and (b)\nstudentized CI (also called the bootstrap-\\(t\\) CI). For (a), it requires the influence\nvalue of the \\(R^2\\) function, whereas\nfor (b), it requires an estimate of the sampling variance of the \\(R^2\\) estimate.\nInfluence value\n\n\n# Based on the group jackknife\ninf_val <- bootmlm::empinf_mer(fm1, marginal_r2, index = 1)\n\n\n\nSampling\nvariance with the numerical delta method\n\nThis part is a bit more technical; skip this if you’re not interested\nin the studentized CI.\n\nTo obtain an approximate sampling variance of the \\(R^2\\), it would be easier to use the\nr2mlm::r2mlm_manual() function to compute \\(R^2\\). We first write a function that\ncomputes \\(R^2\\) using input of the\nfixed and random effects:\n\n\nmanual_r2 <- function(theta, data,\n                      # The following are model-specific\n                      wc = 2, bc = NULL, rc = 2,\n                      cmc = FALSE) {\n  n_wc <- length(wc)\n  n_bc <- length(bc) + 1\n  dim_rc <- length(rc) + 1\n  n_rc <- dim_rc * (dim_rc + 1) / 2\n  gam_w <- theta[seq_len(n_wc)]\n  gam_b <- theta[n_wc + seq_len(n_bc)]\n  tau <- matrix(NA, nrow = dim_rc, ncol = dim_rc)\n  tau[lower.tri(tau, diag = TRUE)] <- theta[n_wc + n_bc + seq_len(n_rc)]\n  tau2 <- t(tau)\n  tau2[lower.tri(tau2)] <- tau[lower.tri(tau)]\n  s2 <- tail(theta, n = 1)\n  r2mlm_manual(data,\n               within_covs = wc,\n               between_covs = bc,\n               random_covs = 2,\n               gamma_w = gam_w,\n               gamma_b = gam_b,\n               Tau = tau2,\n               sigma2 = s2,\n               clustermeancentered = cmc,\n               bargraph = FALSE)$R2s[1, 1]\n}\ntheta_fm1 <- c(fixef(fm1)[2], fixef(fm1)[1],\n               VarCorr(fm1)[[1]][c(1, 2, 4)], sigma(fm1)^2)\nmanual_r2(theta_fm1, data = fm1@frame)\n\n\n[1] 0.278513\n\nNow computes the numerical gradient\n\n\ngrad_fm1 <- numDeriv::grad(manual_r2, x = theta_fm1, data = fm1@frame)\n\n\n\nWe also need the asymptotic covariance matrix of the fixed and random\neffects\n\n\nvcov_fixed <- vcov(fm1)\nvcov_random <- vcov_vc(fm1, sd_cor = FALSE, print_names = FALSE)\nvcov_fm1 <- bdiag(vcov_fixed, vcov_random)\n# Need to re-arrange the first two columns\nvcov_fm1 <- vcov_fm1[c(2, 1, 3:6), c(2, 1, 3:6)]\n\n\n\nNow apply the multivariate delta method\n\n\ncrossprod(grad_fm1, vcov_fm1) %*% grad_fm1\n\n\n1 x 1 Matrix of class \"dgeMatrix\"\n            [,1]\n[1,] 0.005919918\n\nWe now need a function that computes both \\(R^2\\) and the asymptotic sampling variance\nof it.\n\n\nmarginal_r2_with_var <- function(object,\n                                 wc = 2, bc = NULL, rc = 2) {\n  dim_rc <- length(rc) + 1\n  vc_mat <- matrix(seq_len(dim_rc^2), nrow = dim_rc, ncol = dim_rc)\n  vc_index <- vc_mat[lower.tri(vc_mat, diag = TRUE)]\n  theta_obj <- c(fixef(object)[wc], fixef(object)[c(1, bc)],\n                 VarCorr(object)[[1]][vc_index], sigma(object)^2)\n  r2 <- manual_r2(theta_obj, data = object@frame)\n  grad_obj <- numDeriv::grad(manual_r2, x = theta_obj, data = object@frame)\n  # Need to re-arrange the order of the fixed effects\n  names_wc <- names(object@frame)[wc]\n  names_bc <- c(\"(Intercept)\", names(object@frame)[bc])\n  vcov_fixed <- vcov(object)[c(names_wc, names_bc), c(names_wc, names_bc)]\n  vcov_random <- vcov_vc(object, sd_cor = FALSE, print_names = FALSE)\n  vcov_obj <- bdiag(vcov_fixed, vcov_random)\n  v_r2 <- crossprod(grad_obj, vcov_obj) %*% grad_obj\n  c(r2, as.numeric(v_r2))\n}\nmarginal_r2_with_var(fm1)\n\n\n[1] 0.278513044 0.005919918\n\nFive Bootstrap CIs\nNow, we can do bootstrap again, using the new function that computes\nboth the estimate and the asymptotic sampling variance\n\n\n# This takes quite a bit longer due to the need to compute variances\nboo03 <- bootstrap_mer(fm1, FUN = marginal_r2_with_var, nsim = 999,\n                       type = \"residual\")\nboo03\n\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nbootstrap_mer(x = fm1, FUN = marginal_r2_with_var, nsim = 999, \n    type = \"residual\")\n\n\nBootstrap Statistics :\n       original        bias    std. error\nt1* 0.278513044  0.0100127766 0.083704256\nt2* 0.005919918 -0.0002646922 0.001150678\n\nAnd then obtain five types of CIs\n\n\nboot::boot.ci(boo03, L = inf_val)\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 998 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = boo03, L = inf_val)\n\nIntervals : \nLevel      Normal              Basic             Studentized     \n95%   ( 0.1044,  0.4326 )   ( 0.0890,  0.4067 )   ( 0.0822,  0.4385 )  \n\nLevel     Percentile            BCa          \n95%   ( 0.1503,  0.4680 )   ( 0.1408,  0.4466 )  \nCalculations and Intervals on Original Scale\n\nBootstrap CI With\nTransformation\nGiven that \\(R^2\\) is bounded, it\nmay be more accurate to first transform the \\(R^2\\) estimates to an unbounded scale,\nobtain the CIs on the transformed scale, and then back transform it to\nbetween 0 and 1. This can be done in boot::boot.ci() as\nwell with the logistic transformation:\n\n\nboot::boot.ci(boo03, L = inf_val, h = qlogis,\n              # Need the derivative of the transformation\n              hdot = function(x) 1 / (x - x^2),\n              hinv = plogis)\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 998 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = boo03, L = inf_val, h = qlogis, hdot = function(x) 1/(x - \n    x^2), hinv = plogis)\n\nIntervals : \nLevel      Normal              Basic             Studentized     \n95%   ( 0.1440,  0.4629 )   ( 0.1449,  0.4572 )   ( 0.1184,  0.4187 )  \n\nLevel     Percentile            BCa          \n95%   ( 0.1503,  0.4680 )   ( 0.1408,  0.4466 )  \nCalculations on Transformed Scale;  Intervals on Original Scale\n\nNote that the transformation only affects the Normal, Basic, and\nStudentized CIs.\nConclusion\nThis post demonstrates how to use multilevel bootstrapping to obtain\nCIs for \\(R^2\\). The post only focuses\non marginal \\(R^2\\), but CIs for other\n\\(R^2\\) measures can be similarly\nobtained. The studentized CI is the most complex as it requires\nobtaining the sampling variance of \\(R^2\\) for each bootstrap sample. So far, to\nmy knowledge, there has not been studies on which CI(s) perform best, so\nsimulation studies are needed.\nFurther readings on multilevel bootstrap:\nChapter\nby van der Leeden et al.\nPaper by\nLai\nBook by Davison\n& Hinkley\n\n\n\n",
    "preview": "posts/2022-05-10-confidence-intervals-for-multilevel-r-squared/confidence-intervals-for-multilevel-r-squared_files/figure-html5/r2mlm-fm1-1.png",
    "last_modified": "2024-02-19T23:30:40-08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-09-julia-mle/",
    "title": "Maximum Likelihood Estimation with Julia",
    "description": "A demonstration of doing maximum likelihood estimation using Julia",
    "author": [
      {
        "name": "Hok Chio (Mark) Lai",
        "url": {}
      }
    ],
    "date": "2021-03-09",
    "categories": [
      "Julia",
      "Statistics"
    ],
    "contents": "\n\nContents\nLoading Packages\nSimple Bernoulli Example\nGrid Search\nGradient Descent/Ascent\nLog-Likelihood\n\nNewton’s Method\nUsing\nEstablished Algorithms with Optim.jl\nFit MLE\n\nRegression\nSample Data\nWith Optim.jl\nUsing JuMP.jl\nSufficient Statistics\n\n\n\nFor a statistical model of data \\(\\mathbf{y}\\) written as \\(f(\\mathbf{y};\n\\boldsymbol{\\mathbf{\\theta}})\\) with parameters \\(\\boldsymbol{\\mathbf{\\theta}}\\), the\nlikelihood function, \\(\\mathcal{L}(\\boldsymbol{\\mathbf{\\theta}};\n\\mathbf{y})\\), is the probability of the data given the parameter\nvalues.\nLoading Packages\nusing Plots, Distributions, Random\nusing Optim\nusing NLSolversBase, ForwardDiff, FiniteDifferences\nusing HypothesisTests\nusing Printf\nusing LinearAlgebra\nSimple Bernoulli Example\nAs a simple example, consider a model where \\(Y\\) is assumed to follow a Bernoulli\ndistribution with probability of success equals \\(\\theta\\). Assume that there are three\nobserved values \\(\\mathbf{y} = [0, 1,\n0]'\\). You can think about flipping a coin (which may or may\nnot be fair) three times, and call it a success if the coin shows a\nhead. The model further assumes that each coin flip is independent.\nTherefore, if the success probability is \\(\\theta\\), then the probability of observing\n\\(\\mathbf{y} = [0, 1, 0]'\\) is\n\\[\\mathcal{L}(\\theta; y_1 = 0, y_2 = 1, y_3 =\n0) = (1 - \\theta) \\times \\theta \\times (1 - \\theta) = \\theta(1 -\n\\theta)^2,\\] which is our likelihood function. Note that \\(\\theta \\in [0, 1]\\) We can write the\nfunction in Julia\n# Note that it returns zero when theta is out of range\nlik(θ) = (0 <= θ <= 1) ? θ * (1 - θ)^2 : 0\n# Try theta = 0.5\nlik(0.5)\n# Try theta = 0.3\nlik(0.3)\n# Try theta = -0.1\nlik(-0.1)\n0\nWe can plot the likelihood function [need help to annotate it]\nplot(lik, 0, 1)\n\nThe maximum point of the curve is the maximum likelihood estimate\n(MLE), usually denoted as \\(\\hat\n\\theta\\). From eyeballing we can see \\(\\hat \\theta \\approx 1 / 3\\). As you\nprobably can guess, the MLE is the sample mean.\nGrid Search\nLet’s write a Julia program to find the MLE. One brute force method\nis to try discrete values, say 0, 0.001, 0.002, up to 1:\n# Write a function that returns the maximum likelihood value and the estimate\n# Need to make the lower bound, the upper bound, and the grid size as parameters\nmy_find_mle = function(lik, grid_size)\n    thetas = 0:grid_size:1\n    lik_vals = [lik(i) for i in thetas]\n    max_lik, max_idx = findmax(lik_vals)\n    return (lik = max_lik, mle = thetas[max_idx])\nend\nmy_find_mle(lik, .00001)\n(lik = 0.148148148137037, mle = 0.33333)\nGradient Descent/Ascent\nAn algorithm to minimize a function \\(f\\) is to iteratively do \\[\\theta_{s + 1} = \\theta_s + \\gamma\nf'(\\theta_s),\\] where \\(\\gamma\\) is a step size/learning rate. We\nneed to first to find the derivative. It can be done numerically,\nlike\nlik_prime(θ; h = 1e-5) = (lik(θ + h) - lik(θ)) / h\n# lik_prime(0.5)\nlik_prime (generic function with 1 method)\n\nNote: One can also use the central\ndifference method, which is more accurate\n\nstep_size = 0.6\nθ₀ = 0.5\nlik0 = lik(θ₀)\ndel0 = lik_prime(θ₀)\nθ₁ = θ₀ + step_size * del0\ndel1 = lik_prime(θ₁)\nθ₂ = θ₁ + step_size * del1\n# TODO: Write a loop/function to find the MLE\nfunction ga(f, θ; step_size = 0.6, tol = 1e-10, max_iter = 100)\n    f_prime(θ; h = 1e-5) = (f(θ + h) - f(θ)) / h\n    del = f_prime(θ)\n    iter = 0\n    while (iter < max_iter && abs(del) > tol)\n        θ += step_size * del\n        del = f_prime(θ)\n        iter += 1\n    end\n    println(\"converged after \", iter, \" iterations.\")\n    θ\nend\nga (generic function with 1 method)\nLog-Likelihood\nThe log-likelihood function is \\[\\ell(\\theta; y_1 = 0, y_2 = 1, y_3 = 0) = \\log\n\\theta + 2 \\log(1 - \\theta).\\]\nThe second derivative (or Hessian if \\(\\theta\\) is a vector), is useful to\nobtaining approximate standard errors for the estimate. From the theory\nof maximum likelihood estimation, it can be shown that the standard\nerror of the MLE is approximately \\(\\sqrt{- 1\n/ \\ell''(\\theta)}\\). This is usually called the\nasymptotic standard error, or ase.\nll(θ) = log(lik(θ))\n# Numerical first derivative\nll_prime(θ; h = 1e-5) = (ll(θ + h) - ll(θ)) / h\n# Numerical second-order derivative (2nd order forward)\nll_prime2(θ; h = 1e-5) = (ll(θ + 2h) - 2ll(θ + h) + ll(θ)) / h^2\n# Asymptotic standard error\nase(θ) = sqrt(- 1 / ll_prime2(θ))\nase (generic function with 1 method)\nSo the ase is 0.27.\nNewton’s Method\nSee https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization\nθ₀ = 0.5\nll0 = ll(θ₀)\ndel0 = ll_prime(θ₀)\ndeldel0 = ll_prime2(θ₀)\nθ₁ = θ₀ - del0 / deldel0\ndel1 = ll_prime(θ₁)\ndeldel1 = ll_prime2(θ₁)\nθ₂ = θ₁ + del1 / deldel1\n# TODO: Write a loop/function to find the MLE (credit to Winnie Tse)\nfunction newt(θ; step_size = .6, tol = 1e-10, max_iter = 10)\n    del = ll_prime(θ)\n    deldel = ll_prime2(θ)\n    iter = 0\n    while (iter < max_iter && abs(del) > tol)\n        θ += (-1)^(iter+1) * del / deldel\n        del = ll_prime(θ)\n        deldel = ll_prime2(θ)\n        iter += 1\n    end\n    println(\"converged after \", iter, \" iterations.\")\n    θ\nend\nnewt(.5)\nconverged after 5 iterations.\n0.33332833334790385\nUsing Established\nAlgorithms with Optim.jl\nSee https://julianlsolvers.github.io/Optim.jl/stable/\n# Find minimum on -LL\nopt = optimize(x -> -ll(x), 0, 1)  # Brent's Method\n# Second-derivative (5th order central method)\nFiniteDifferences.central_fdm(5, 2)(ll, opt.minimizer)\n-13.499999822735278\nFit MLE\n# From Distributions.jl\nfit_mle(Bernoulli, [0, 1, 0])\nDistributions.Bernoulli{Float64}(p=0.3333333333333333)\nRegression\n\\[Y = \\beta_0 + \\beta_1 X + e\\]\n\\[e \\sim N(0, \\sigma)\\]\n\\[f(Y; \\beta_0, \\beta_1) \\overset{d}{=}\nN(\\beta_0 + \\beta_1 X, \\sigma)\\]\n\\[f(Y_1, Y_2, \\ldots; \\beta_0, \\beta_1)\n\\overset{d}{=} \\Pi_{i = 1}^n N(\\beta_0 + \\beta_1 X_i,\n\\sigma)\\]\n\\[\\ell(\\beta_0, \\beta_1, \\sigma; y_1, y_2,\n\\ldots, y_n) = \\sum_{i = 1}^n \\ell(\\beta_0, \\beta_1, \\sigma;\ny_i)\\]\n\\[\\ell(\\beta_0, \\beta_1, \\sigma; y_1, y_2,\n\\ldots, y_n) =  -\\frac{n}{2} \\log(2 \\pi) - n \\log \\sigma - \\frac{\\sum_{i\n= 1}^n(y_i - \\beta_0 - \\beta_1 x_i)^2}{2 \\sigma^2}\\]\n\nSample Data\n# From the `lm()` example in R\nctl = [4.17, 5.58, 5.18, 6.11, 4.50, 4.61, 5.17, 4.53, 5.33, 5.14]\ntrt = [4.81, 4.17, 4.41, 3.59, 5.87, 3.83, 6.03, 4.89, 4.32, 4.69]\n# Likelihood function\nfunction ll(θ; yc = ctl, yt = trt)\n    σ = θ[3]\n    if σ < 0 \n        return -Inf\n    end\n    β₀ = θ[1]\n    β₁ = θ[2]\n    sum(vcat(logpdf(Normal(β₀, σ), yc), \n             logpdf(Normal(β₀ + β₁, σ), yt)))\nend\nll (generic function with 1 method)\nPlot the conditional log-likelihood functions\n# Just beta1\nplot(x -> ll([5, x, 1]), -2, 2)\n# Both beta1 and sigma\ncontour(-1:0.01:0, 0.5:0.01:1, (x, y) -> ll([5, x, y]), fill = true)\n\nWith Optim.jl\nopt = optimize(x -> -ll(x), [4.0, 1.0, 1.0])    # Nelder-Mead\nopt.minimizer\n3-element Vector{Float64}:\n  5.03198824016666\n -0.37098861906446434\n  0.6606616771567159\nOr use Optim.maximize\nresult = maximize(ll, [4.0, 1.0, 1.0])\nresult.res.minimizer\n3-element Vector{Float64}:\n  5.03198824016666\n -0.37098861906446434\n  0.6606616771567159\nBox Constraints\nWhen some of the parameter(s) are constrained in just a subset of the\nreal line.\nfunction ll2(θ; yc = ctl, yt = trt)\n    sum(vcat(logpdf(Normal(θ[1], θ[3]), yc), \n             logpdf(Normal(θ[1] + θ[2], θ[3]), yt)))\nend\n# Unconstrained for beta, [0, infinity) for sigma\nopt_box = optimize(x -> -ll2(x), \n                   [-Inf, -Inf, 0], [Inf, Inf, Inf], \n                   [4.0, 1.0, 1.0])    # Fminbox with L-BFGS\nopt_box.minimizer\n3-element Vector{Float64}:\n  5.032000000048675\n -0.3710000000898218\n  0.6606530860112547\nTo obtain the Hessian (to be used to compute standard errors, use\nhess = ForwardDiff.hessian(x -> ll(x), opt.minimizer)\n# Asymptotic covariance matrix\n- inv(hess)\n3×3 Matrix{Float64}:\n  0.0436474   -0.0436474   -3.88479e-7\n -0.0436474    0.0872948    3.75962e-7\n -3.88479e-7   3.75962e-7   0.0109123\nCompare to \\(t\\) test\nEqualVarianceTTest(ctl, trt)\nTwo sample t-test (equal variance)\n----------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          0.371\n    95% confidence interval: (-0.2833, 1.0253)\n\nTest summary:\n    outcome with 95% confidence: fail to reject h_0\n    two-sided p-value:           0.2490\n\nDetails:\n    number of observations:   [10,10]\n    t-statistic:              1.191260381848704\n    degrees of freedom:       18\n    empirical standard error: 0.3114348514002032\nWith Analytic Derivatives\nfunction g!(G, θ; yc = ctl, yt = trt)\n    n = size(ctl, 1) + size(trt, 1)\n    G[1] = - (sum(yc .- θ[1]) + sum(yt .- θ[1] .- θ[2])) / θ[3]^2\n    G[2] = - sum(yt .- θ[1] .- θ[2]) / θ[3]^2\n    G[3] = n / θ[3] - sum(vcat(yc .- θ[1], yt .- θ[1] .- θ[2]) .^ 2) / θ[3]^3\nend\ng! (generic function with 1 method)\nopt2 = optimize(x -> -ll(x), g!, [4.0, 1.0, 1.0], GradientDescent())\n* Status: success\n\n * Candidate solution\n    Final objective value:     2.008824e+01\n\n * Found with\n    Algorithm:     Gradient Descent\n\n * Convergence measures\n    |x - x'|               = 1.92e-09 ≰ 0.0e+00\n    |x - x'|/|x'|          = 3.82e-10 ≰ 0.0e+00\n    |f(x) - f(x')|         = 0.00e+00 ≤ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 0.00e+00 ≤ 0.0e+00\n    |g(x)|                 = 6.96e-08 ≰ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    88\n    f(x) calls:    224\n    ∇f(x) calls:   224\nWith Automatic\nDifferentiation (AD)\nobj_func = TwiceDifferentiable(x -> -ll(x), [4.0, 1.0, 1.0]; \n                               autodiff = :forward)\nopt3 = optimize(obj_func, [4.0, 1.0, 1.0])  # Newton's method\nNLSolversBase.hessian!(obj_func, opt3.minimizer)\n3×3 Matrix{Float64}:\n 45.823        22.9115       -3.52496e-14\n 22.9115       22.9115       -3.34732e-14\n -3.52496e-14  -3.34732e-14  91.6459\nUsing JuMP.jl\nusing JuMP\nimport Ipopt\nmodel = Model(Ipopt.Optimizer)\n@variable(model, β[1:2])\nsetvalue(β[1], 4.0)\nsetvalue(β[2], 1.0)\n@variable(model, σ >= 0.0, start = 1.0)\n@NLobjective(\n    model,\n    Max,\n    -10 * log(σ) - sum((ctl[i] - β[1])^2 for i = 1:10) / (2 * σ^2) + \n    -10 * log(σ) - sum((trt[i] - β[1] - β[2])^2 for i = 1:10) / (2 * σ^2)\n)\n# @NLconstraint(model, β₀ == 10σ)\noptimize!(model)\nJuMP.value.(β)\nJuMP.value.(σ)\n***************************************************************************\n***\nThis program contains Ipopt, a library for large-scale nonlinear optimizati\non.\n Ipopt is released as open source code under the Eclipse Public License (EP\nL).\n         For more information visit https://github.com/coin-or/Ipopt\n***************************************************************************\n***\n\nThis is Ipopt version 3.13.4, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation\n).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:        6\n\nTotal number of variables............................:        3\n                     variables with only lower bounds:        1\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_p\nr  ls\n   0 -1.0264350e+01 0.00e+00 6.93e+00  -1.0 0.00e+00    -  0.00e+00 0.00e+0\n0   0\n   1 -3.6504365e+00 0.00e+00 3.23e+01  -1.0 7.91e+00   0.0 1.00e+00 1.55e-0\n1f  2\n   2 -2.0124548e+00 0.00e+00 8.88e+00  -1.0 9.54e-02    -  8.95e-01 1.00e+0\n0f  1\n   3 -1.7225159e+00 0.00e+00 1.75e+00  -1.0 5.56e-02    -  1.00e+00 1.00e+0\n0f  1\n   4 -1.7094835e+00 0.00e+00 1.07e-01  -1.0 1.67e-02    -  1.00e+00 1.00e+0\n0f  1\n   5 -1.7094719e+00 0.00e+00 6.82e-05  -2.5 4.64e-04    -  1.00e+00 1.00e+0\n0f  1\n   6 -1.7094718e+00 0.00e+00 6.89e-07  -3.8 4.45e-05    -  1.00e+00 1.00e+0\n0f  1\n   7 -1.7094718e+00 0.00e+00 2.10e-09  -5.7 2.45e-06    -  1.00e+00 1.00e+0\n0f  1\n   8 -1.7094718e+00 0.00e+00 3.50e-13  -8.6 3.04e-08    -  1.00e+00 1.00e+0\n0f  1\n\nNumber of Iterations....: 8\n\n                                   (scaled)                 (unscaled)\nObjective...............:   1.7094718195406751e+00   -1.7094718195406751e+0\n0\nDual infeasibility......:   3.4991254363551114e-13    3.4991254363551114e-1\n3\nConstraint violation....:   0.0000000000000000e+00    0.0000000000000000e+0\n0\nComplementarity.........:   2.5059884083328424e-09   -2.5059884083328424e-0\n9\nOverall NLP error.......:   2.5059884083328424e-09    3.4991254363551114e-1\n3\n\n\nNumber of objective function evaluations             = 14\nNumber of objective gradient evaluations             = 9\nNumber of equality constraint evaluations            = 0\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 0\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 8\nTotal CPU secs in IPOPT (w/o function evaluations)   =      2.174\nTotal CPU secs in NLP function evaluations           =      1.901\n\nEXIT: Optimal Solution Found.\n0.6606530860101113\nFor more discussion on these packages and other packages in Julia,\nthis page https://julia.quantecon.org/more_julia/optimization_solver_packages.html\nwould be helpful. Also check out the documentation of JuMP.jl and Optim.jl.\nSufficient Statistics\nIt can be shown that the likelihood function depends only on \\(\\sum_{i = 1}^n y_i^2\\) and \\(\\sum_{i = 1}^n y_i\\). This helps speed up\nthe optimization.\n# Likelihood function with sufficient statistics\nfunction ll_suff(θ; sum_yc = sum(ctl), sum_yt = sum(trt), \n                    sum_ycsq = sum(ctl .^ 2), sum_ytsq = sum(trt .^ 2))\n    β₀ = θ[1]\n    β₁ = θ[2]\n    σ = θ[3]\n    μₜ = β₀ + β₁\n    - 10 * log(2pi) - 20 * log(σ) - \n    (sum_ycsq + sum_ytsq - 2 * (β₀ * sum_yc + μₜ * sum_yt) + \n     10 * (β₀^2 + μₜ^2)) / 2σ^2\nend\n# Unconstrained for beta, [0, infinity) for sigma\nopt_box = optimize(x -> -ll_suff(x), \n                   [-Inf, -Inf, 0], [Inf, Inf, Inf], \n                   [4.0, 1.0, 1.0])    # Fminbox with L-BFGS\nopt_box.minimizer\n3-element Vector{Float64}:\n  5.032000000229699\n -0.3710000002832418\n  0.6606530859921219\n\n\n\n",
    "preview": {},
    "last_modified": "2024-02-19T23:30:40-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-23-multilevel-models-in-julia/",
    "title": "Multilevel Models in Julia",
    "description": "Multilevel Models in Julia",
    "author": [
      {
        "name": "Yichi Zhang",
        "url": {}
      }
    ],
    "date": "2021-02-23",
    "categories": [
      "Julia",
      "Multilevel Modeling",
      "Statistics"
    ],
    "contents": "\n\nContents\nSet up\nInstall Packages\nUsing Packages\nDataset\n\nResearch Questions\nDescriptive Analysis\nLinear Regression\nFitting the linear\nregression model\nEquations\n\nMultilevel Modeling\nRandom Intercept Model\nAdd Level 2 Predictor\n\nAdd Level 1 predictor\nCross level Interaction\n\nConclusion\nResources\n\nToday We will have a fun session doing regression and multilevel\nmodels(MLMs) using Julia. First, we will have a small exercise fitting a\nlinear regression model to a sample dataset, and then we will fit the\nMLM to the same dataset and compare the difference in results. Please\nnote the examples come from Dr.Mark Lai’s multilevel modeling course,\nwhich can be assessed at https://quantscience.rbind.io/courses/psyc575/homework-3/.\nCan’t wait? Let’s get started! Yay!\nSet up\nInstall Packages\nIn Julia, we can add package by typing “]” in the Terminal to enter\nthe Pkg mode and exit using Ctrl + C. Or, we\ncan use [Pkg] package and add multiple packages at\nonce.\n#= uncomment below to install packages\nusing Pkg\nPkg.add([\"Pipe\", \"DataFrames\", \"StatFiles\", \"GLM\",\"MixedModels\",\"Plots\", \"StatsPlots\"])\n=#\nUsing Packages\nJust like we use library to load packages in R, we use\nusing in Julia. It’s better to have a separate session in\nthe beginning of the file to load all packages you will need in the\nanalysis. Otherwise it will take a long time to precompile each\ntime.\nPipe for pipe operator. DataFrames for\nworking with dataframes. StatFiles for reading datasets\nfrom Stata, SPSS and SAS. If you have a txt file, can try CSV.read\nfunction from package CSV. GLM for linear\nregression. MixedModels for working with multilevel models.\nSimilar to lme4. Plots for making graphs.\nStatsPlots for making graphs.\nusing Pipe, DataFrames, StatFiles, GLM, MixedModels, Plots, StatsPlots, Statistics\nDataset\nToday we are going to use the dataset from the World Value\nSurvey-1990-93 data (World Values Study Group, 1994). There are five\nvariables in the data set:\nCountryID: CountryIDcountry: Country’s namegm_GNP: Grand-mean centered Gross National Productincome: Income level(0-least income to 9-most income)happy:Feel happy(1-not happy to 4-very happy)\nThis data set and set of practice problem come from Mark’s Multilvel\nModeling class, so thanks Mark!\ndata_happy = DataFrame(load(\"happy_combined.sav\"))\n#= Or we can use pipe operator\ndata_happy = load(\"happy_combined.sav\") |> DataFrame\n=#\ndata_happy = dropmissing(data_happy)\nResearch Questions\nAre people with higher individual level income happier? Is the\nrelation similar across countries? How is the result of linear\nregression different from the result of multilevel models?\nDescriptive Analysis\n## Summary of all variables in the dataset\ndescribe(data_happy)\n## list first five rows of data\nfirst(data_happy,5)\n## list names of all variables\nnames(data_happy)\n## get size of the data set\nsize(data_happy)\nsize(data_happy,1)\nsize(data_happy,2)\n## check a specific column \ndata_happy[:,\"country\"]\nunique(data_happy[:,\"country\"])\n# data_happy[!,:2]; data_happy[:,2] also work for extracting the second columns\n# data_happy[2,:] can extract the second row, data_happy[2:5,:] extract the second to fifth row.\n@df data_happy scatter(\n    :country, \n    :happy,\n    group = :country)\nLinear Regression\n??? Exercise Time: We are familiar with our dataset, so let’s do some\nexercise! Recall Winnie did a great presentation last time on linear\nregression, so please go ahead to fit a linear regression model and\nwrite out the equation.\nHint: Model_name = lm(@formula(DV ~ IV), data_set)\nFitting the linear\nregression model\nlm1 = lm(@formula(happy~ gm_GNP),data_happy)\n# extract coefficients\ncoef(lm1)\n# extract standard errors\nstderror(lm1)\n# extract variance covariance matrix\nvcov(lm1)\n# obtain R^2\nr2(lm1)\n# get the deviance \ndeviance(lm1)\nThis model explains 4.72% of variance in happy. Note the\nstandard error estimates for gm_GNP is 0.010, t =17.13, 95%\nCI [0.155 0.195], p < 0.0001.\nEquations\n\\[\\text{happy} = 2.992 + 0.175\n\\text{gm_GNP}\\]\nMultilevel Modeling\nRandom Intercept Model\nWe first fit a random intercept model and calculate the intraclass\ncorrelation. Recall\nEquations\nLevel 1:\n\\[\\text{happy}_{ij} = \\beta_{0j} +\ne_{ij}\\]\nLevel 2:\n\\[\\beta_{0j} = \\gamma_{00} +\nu_{0j}\\]\n\\[\\text{ICC} = \\frac{\\tau_0^2}{\\tau_0^2 +\n\\sigma^2}\\]\n## Fitting MLMs\nmm1 = fit(LinearMixedModel, @formula(happy ~ (1|country)), data_happy)\n## Create a vector and store the model fit statistics\nmodel_fit= Vector{Float64}()\npush!(model_fit,aic(mm1))\n??? Exercise Time: What is the value of ICC?\nThe first part of the result prints out estimation method and the\nmodel fit statistics, such as AIC,BIC, etc. The second part is the table\nof estimates of parameters associated with the random effects. The third\npart is the fixed effects point estimates and standard errors.\nICC = 0.0649/(0.0649 + 0.4842) = 0.118, so there is evidence that\npeople’s happiness level varies across countries. Variability at the\ncountry level accounts for 11.8% of the total variability of happiness\nlevel.\nDesign effect = 1 +(average cluster size - 1) x ICC. We have 5926\nobservations and 38 groups, so design effect = 1 + (5926/38 -1) x 0.118\n= 19.28.\nAdd Level 2 Predictor\nIt is reasonable to think gm_GNP is a cluster level\npredictor, so let’s add it to our model.\nEquations\nLevel 1:\n\\[\\text{happy}_{ij} = \\beta_{0j} +\ne_{ij}\\] Level 2:\n\\[\\beta_{0j} = \\gamma_{00} + \\gamma_{01}\n\\text{gm_GNP}_{j} + u_{0j}\\]\n## Fitting MLMs\nmm2 = fit(LinearMixedModel, @formula(happy ~ gm_GNP + (1|country)), data_happy)\npush!(model_fit, aic(mm2))\n# extract log likelihood\nloglikelihood(mm2)\n# extract Akaike's Information Criterion\naic(mm2)\n# extract Bayesian Information Criterion\nbic(mm2)\n# extract degrees of freedom\ndof(mm2)\n# extract coefficient\ncoef(mm2)\n# extract fixed effects\nfixef(mm2)\nvcov(mm2)\nstderror(mm2)\n# extract coefficients table\ncoeftable(mm2)\n# extract variance components\nVarCorr(mm2)\n# return sigma^2\nvarest(mm2)\n# return tau\nVarCorr(mm2).σρ[1][1][1]\n# return elements in the components\ndump(VarCorr(mm1))\n# return sigma\nsdest(mm2)\n# extract random effects\nranef(mm2)\nNote that the result is slightly different from the R output, it’s\nbecause the default estimation method in [MixedModels] in\nJulia is maximum likelihood estimation, whereas the default estimation\nmethod for [lme4] in R is Restricted maximum likelihood\nmethod.\nThis time standard error estimates for gm_GNP is 0.0337,\nz = 5.33, p < 0.0001. The SE for OLS is smaller than the SE for MLM,\nindicating OLS underestimates the SE.\nAdd Level 1 predictor\nBecause relationships at one level are not necessarily the same at\nthe other level, we need to be careful adding the level 1 predictor. Two\napproaches to decompose the impact of level 1 predictor on outcome\nvariables: cluter mean centering + cluster mean and the raw predictor +\nclutster mean. Here we will use the first approach.\nEquations\nLevel 1:\n\\[\\text{happy}_{ij} = \\beta_{0j} +\n\\beta_{1j} \\text{income_cmc}_{ij} + e_{ij}\\]\nLevel 2:\n\\[\n  \\begin{aligned}\n  \\beta_{0j} & = \\gamma_{00} + \\gamma_{01} \\text{income_mean}_{j} +\nu_{0j} \\\\\n  \\beta_{1j} & = \\gamma_{10} + u_{1j}\n  \\end{aligned}\n\\]\n## Cluster mean centering\ndata_happy2 = @pipe data_happy |> \n            groupby(_,:country) |> # group by country\n            transform(_, :income => mean => :income_mean)|> # add a new variable income_mean\n            transform(_, [:income, :income_mean]=> ByRow(-) => :income_cmc) # add a new variable the centered variable\n\n## Fitting MLMs\nmm3 = fit(LinearMixedModel, @formula(happy ~ income_cmc + income_mean + (income_cmc|country)), data_happy2)\npush!(model_fit, aic(mm3))\nBoth income_cmc and income are significant\npredictors of happiness level. For a person from a country with\nincome_mean = 0 and this person has average country level\nincome, the predicted happiness level is 2.66, SE = 0.16. The average\nwithin country slope is 0.047(SE = 0.008), meaning a one unit increase\nin income_cmc is associated with a 0.047 unit increase in\nhappiness. This slope varies across countries, with a standard deviation\nof 0.38. The average between country level slope is 0.08, SE = 0.04,\nsuggesting a one unit increase in income_mean is associated\nwith a 0.08 unit increase in the average happiness level.\nCross level Interaction\nIs the relation between happy and income\nmoderated by gm_GNP? We can answer this question by adding\ngm_GNP to the above model.\nmm4 = fit(LinearMixedModel, @formula(happy ~ income_cmc * gm_GNP + income_mean + (income_cmc|country)), data_happy2)\npush!(model_fit, aic(mm4))\nAfter adding gm_GNP, the impact of\nincome_mean on happy is not significant.\nEquations\nLevel 1:\n\\[\n\\text{happy}_{ij} = \\beta_{0j} + \\beta_{1j} \\text{income_cmc}_{ij} +\ne_{ij}\n\\]\nLevel 2:\n\\[\n  \\begin{aligned}\n  \\beta_{0j} & = \\gamma_{00} + \\gamma_{01} \\text{income_mean}_{j} +\n\\gamma_{02} \\text{gm_GNP}_{j} + u_{0j}\\\\\n\\beta_{1j} & = \\gamma_{10} + \\gamma_{11} \\text{gm_GNP}_{j} + u_{1j}\n  \\end{aligned}\n\\]\n??? Exercise time: 1. Which model fits data better?\n# print out the AIC\nprint(model_fit)\n# Likelihood Ratio Test\nMixedModels.likelihoodratiotest(mm1,mm2,mm3,mm4)\nConclusion\nThe results of [MixedModels] in Julia and\n[lme4] are slightly different due to the estimation methods\nthat are used. The cross-level interaction model fits best to our data,\nsuggesting the country level GNP gm_GNP moderated the\nrelation between individual’s happiness level happy and\nincome income.\nResources\nMark’s MLM class and HW\nDataFrames package https://dataframes.juliadata.org/stable/\nMixedModels Package (similar to lme4 in R) documentation https://juliastats.org/MixedModels.jl/stable/\nMaybe useful for plotting week:\nGadfly Package (similar to ggplot in R) documentation https://gadflyjl.org/stable/\n\n\n\n",
    "preview": {},
    "last_modified": "2024-02-19T23:30:40-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-09-julia-regression/",
    "title": "Regression with Julia!",
    "description": "Running regression with GLM.jl in Julia",
    "author": [
      {
        "name": "Winnie Wing-Yee Tse",
        "url": {}
      }
    ],
    "date": "2021-02-11",
    "categories": [
      "Julia",
      "Regression",
      "Visualization"
    ],
    "contents": "\n\nContents\nSetup\nBrief Introduction\nto the Package Pipe\nBrief\nIntroduction to the Package RDatasets\nBrief\nIntroduction to the Package DataFrames\n\nLinear Regression\nExtracting Model\nInformation\n\nMultiple Regression\nProbit Regression\nLogistic Regression\nVisualization\n\nSetup\nBefore we start, I encourage you to uncomment the codes below and\ninstall the required packages that you do not have for this\ntutorial.\nusing Plots, StatsPlots\n#= uncomment below to install packages\nusing Pkg\nPkg.add([\"RDatasets\", \"Pipe\", \"DataFrames\", \"GLM\",\n        \"Plots\", \"StatPlots\"])\n=#\nPrior to running regression analyses, let me briefly go over three\nessential packages, Pipe, RDatasets,\nDataFrames, in handling data frames in Julia.\nBrief Introduction to\nthe Package Pipe\nThe package Pipe improves the base pipe operator\n(|>) in Julia. It allows you to represent the object\nthat is piped from with an underscore _ in the functions\nthat follow. For example, sqrt(sum([1, 2, 3])) involve two\ncommands – one applied to [1, 2, 3] and the other one\napplied to sum([1, 2, 3]). We can pipe these commands,\nstemming from the [1, 2, 3] object, using\n@pipe together with |>, as illustrated\nbelow.\nusing Pipe\n@pipe [1, 2, 3] |> sum(_) |> sqrt(_)\n2.449489742783178\nThe above code might have overkilled this simple task of taking a\nsqaure root of a sum, but piping will become very useful in handling a\nlong series of commands. For more details of Pipe, please\nread the julia tutorial on [pipe] (https://syl1.gitbook.io/julia-language-a-concise-tutorial/useful-packages/pipe).\nBrief Introduction\nto the Package RDatasets\nThe package RDatasets\nprovides most of the base R datasets for Julia users to play around. We\nwill be using the dataset bfi from the R package\npsych in this tutorial to explore regression analyses in\nJulia.\nLet’s read in the dataset bfi and drop out missing\nvalues.\nusing RDatasets\n# show(RDatasets.datasets(\"psych\"), allrows=true)\nbfi = @pipe dataset(\"psych\", \"bfi\") |> dropmissing(_)\nprintln(names(bfi))\n[\"Variable\", \"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"E\n1\", \"E2\", \"E3\", \"E4\", \"E5\", \"N1\", \"N2\", \"N3\", \"N4\", \"N5\", \"O1\", \"O2\", \"O3\",\n \"O4\", \"O5\", \"Gender\", \"Education\", \"Age\"]\nFor a complete R documentation of this dataset, please refer to\n[this] (https://www.personality-project.org/r/html/bfi.html).\nBrief Introduction\nto the Package DataFrames\nFor simplicity, let’s wrangle our data and sum over the scores on the\ncolumns for agreeableness (A1 to A5) and for\ncontienciousness (C1 to C5).\nGender is a numeric variable in the original dataset, while\nit should better be treated as a binary variable. Let’s also convert it\ninto a categorical array using categorical.\ntransform() has similar function as\nmutate() in R, with which we can transform a column and\nsave it into a new column in a dataframe. The operator\n=> is pretty intuitive. In the first transform statment\ndown below, we are basically asking Julia to take the columns between\nA1 and A5 in bfi, transform them\nby summing scores on the same row (=> ByRow(+)), and\nthen save these transformed data into a new column called\nAsum.\nNote that to call a column in a dataframe, we need to place an colon\n: before the column name.\nusing DataFrames\nbfi = @pipe bfi |> \n            transform(_, Between(:A1, :A5) => ByRow(+) => :Asum) |> \n            transform(_, Between(:C1, :C5) => ByRow(+) => :Csum) |>\n            transform(_, :Gender => categorical => :Gender_bin)\ndescribe(select(bfi, Between(:Asum, :Gender_bin)))\n3×7 DataFrame\n Row │ variable    mean     min  median  max  nmissing  eltype             \n    ⋯\n     │ Symbol      Union…   Any  Union…  Any  Int64     DataType           \n    ⋯\n─────┼─────────────────────────────────────────────────────────────────────\n─────\n   1 │ Asum        21.1632  5    22.0    30          0  Int64              \n    ⋯\n   2 │ Csum        19.0501  5    19.0    30          0  Int64\n   3 │ Gender_bin           1            2           0  CategoricalValue{In\nt64\n                                                                1 column om\nitted\ndescribe() provides summary statistics of the selected\ncolumns of our newly created variables from Asum to\nGender_bin. As can be seen in the summary table,\nGender_bin has a data type of\nCategoricalValue{Int64,UInt32}. Through that way in our\nfollowing regression analysis, gender will be treated as a categorical\nvariable and automatically dummy coded.\nFor data wrangling in Julia using the package\nDataframes, a [cheatsheet] (https://ahsmart.com/assets/pages/data-wrangling-with-data-frames-jl-cheat-sheet/DataFramesCheatSheet_v0.22_rev1.pdf)\nmay come in handy here. I am a beginner Julia and\nDataframes user. If you have a better way to optimize my\ncodes, please let me know and I will be more than happy to learn it!\nLinear Regression\nBelow are some summary statistics and a snapshot of the variables of\ninterest for the following analyses.\ndescribe(select(bfi, Between(:Education, :Gender_bin)))\n5×7 DataFrame\n Row │ variable    mean     min  median  max  nmissing  eltype             \n    ⋯\n     │ Symbol      Union…   Any  Union…  Any  Int64     DataType           \n    ⋯\n─────┼─────────────────────────────────────────────────────────────────────\n─────\n   1 │ Education   3.19141  1    3.0     5           0  Int64              \n    ⋯\n   2 │ Age         29.5103  3    26.0    86          0  Int64\n   3 │ Asum        21.1632  5    22.0    30          0  Int64\n   4 │ Csum        19.0501  5    19.0    30          0  Int64\n   5 │ Gender_bin           1            2           0  CategoricalValue{In\nt64 ⋯\n                                                                1 column om\nitted\nfirst(select(bfi, Between(:Education, :Gender_bin)), 5)\n5×5 DataFrame\n Row │ Education  Age    Asum   Csum   Gender_bin\n     │ Int64      Int64  Int64  Int64  Cat…\n─────┼────────────────────────────────────────────\n   1 │         3     21     28     22  2\n   2 │         2     19     14     15  1\n   3 │         1     21     24     17  1\n   4 │         1     17     14     19  1\n   5 │         5     68     23     18  1\nIn our sample, do older people tend to be more agreeable than younger\npeople? We can fit a linear model with Age as the predictor\nand Asum, the sum scores of agreeableness, as the outcome\nvariable, and determine whether there is a positive association between\nthese two variables.\nThe package GLM\nprovides convenient ways that are analogous to the R functions\nlm() and glm() to specify linear models in\nJulia.\nTo build a linear model, we use lm(), specify the model\nequation within @formula(), and indicate the dataset after\nthe formula.\nusing GLM\nlm1 = lm(@formula(Asum ~ Age), bfi)\nStatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1\n}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2\n}}}},Array{Float64,2}}\n\nAsum ~ 1 + Age\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(>|t|)  Lower 95%   Upper 95%\n───────────────────────────────────────────────────────────────────────────\n(Intercept)  20.4382     0.225823    90.51    <1e-99  19.9954    20.8811\nAge           0.0245685  0.00719702   3.41    0.0007   0.010455   0.0386821\n───────────────────────────────────────────────────────────────────────────\nUnlike R, we do not need to additionally summarize lm1\nto get a summary table of the outputs.\nEquivalent to lm(), we can use the fit()\nfunction with the LinearModel argument.\nfit1 = fit(LinearModel, @formula(Asum ~ Age), bfi)\nStatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1\n}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2\n}}}},Array{Float64,2}}\n\nAsum ~ 1 + Age\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(>|t|)  Lower 95%   Upper 95%\n───────────────────────────────────────────────────────────────────────────\n(Intercept)  20.4382     0.225823    90.51    <1e-99  19.9954    20.8811\nAge           0.0245685  0.00719702   3.41    0.0007   0.010455   0.0386821\n───────────────────────────────────────────────────────────────────────────\nlm(...) and fit(LinearModel, ...) do not\ndiffer in their type, output, and functionality. A quick check on their\ntype:\ntypeof(lm1)\nStatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1\n}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2\n}}}},Array{Float64,2}}\ntypeof(fit1)\nStatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1\n}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2\n}}}},Array{Float64,2}}\nwhich are essentially the same.\nExtracting Model Information\nWe can use coef(), stderror(), and\nvcov() to extract coefficient estimates and standard errors\nof the coefficents, and the estimated variance-covariance matrix of the\ncoefficient estimates, respectively.\ncoef(lm1)\n2-element Array{Float64,1}:\n 20.43821402633636\n  0.024568514618654096\nstderror(lm1)\n2-element Array{Float64,1}:\n 0.22582269777999403\n 0.007197020298763872\nvcov(lm1)\n2×2 Array{Float64,2}:\n  0.0509959   -0.00152855\n -0.00152855   5.17971e-5\nTo obtain statistics for model fit, we can use r2 for\nR^2 and deviance() for the weighted residual\nsum of squares.\nr2(lm1)\n0.005189310057134855\ndeviance(lm1)\n29411.99403563621\npredict() computes the predicted values of each\nindividual in our sample.\npredict(lm1)\n2236-element Array{Float64,1}:\n 20.9541528333281\n 20.905015804090787\n 20.9541528333281\n 20.85587877485348\n 22.10887302040484\n 21.10156392104002\n 20.880447289472134\n 20.92958431870944\n 21.69120827188772\n 21.248975008751945\n  ⋮\n 21.15070095027733\n 21.248975008751945\n 21.003289862565406\n 20.978721347946752\n 20.978721347946752\n 21.02785837718406\n 21.15070095027733\n 21.199837979514637\n 21.666639757269067\n!!! note “Follow-up Practice (1)” Let’s try to run a linear model to\ninvestigate whether contiouenciousness is predicted by education. How\nmuch variance is explained by this model?\nMultiple Regression\nWe may suspect that the difference in agreeableness over years of age\ndepend on someone’s education level. To investigate this, we can add an\ninteration between Age and Education on\nAsum by Age & Education. Typically when we\nadd an interaction term to a model, we include also the main effects of\nthe variables. A shorthand of specifying both the main effects and\ninteraction between the variables is Age * Education.\n# the models below are equivalent\nlm(@formula(Asum ~ Age + Education + Age & Education), bfi)\nStatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1\n}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2\n}}}},Array{Float64,2}}\n\nAsum ~ 1 + Age + Education + Age & Education\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n──────\n                      Coef.  Std. Error      t  Pr(>|t|)   Lower 95%    Upp\ner 95%\n───────────────────────────────────────────────────────────────────────────\n──────\n(Intercept)      19.4241      0.651563   29.81    <1e-99  18.1463     20.70\n18\nAge               0.0831532   0.0211002   3.94    <1e-4    0.0417752   0.12\n4531\nEducation         0.273547    0.200684    1.36    0.1730  -0.119999    0.66\n7093\nAge & Education  -0.016346    0.0061294  -2.67    0.0077  -0.0283659  -0.00\n432607\n───────────────────────────────────────────────────────────────────────────\n──────\nlm(@formula(Asum ~ Age * Education), bfi)\nStatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1\n}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2\n}}}},Array{Float64,2}}\n\nAsum ~ 1 + Age + Education + Age & Education\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n──────\n                      Coef.  Std. Error      t  Pr(>|t|)   Lower 95%    Upp\ner 95%\n───────────────────────────────────────────────────────────────────────────\n──────\n(Intercept)      19.4241      0.651563   29.81    <1e-99  18.1463     20.70\n18\nAge               0.0831532   0.0211002   3.94    <1e-4    0.0417752   0.12\n4531\nEducation         0.273547    0.200684    1.36    0.1730  -0.119999    0.66\n7093\nAge & Education  -0.016346    0.0061294  -2.67    0.0077  -0.0283659  -0.00\n432607\n───────────────────────────────────────────────────────────────────────────\n──────\n!!! note “Follow-up Practice (2)” Let’s fit a model to explore\nwhether there is an interaction between education and gender\n(Gender_bin) on contienciousness. How much did the variance\nexplained by the model increasecompared to the model without the\ninteraction (in reference to Practice (1)) increase?\nProbit Regression\nFor the illustrative purpose, I arbitrarily define that a sum score\nabove 20 out of 25 is a high score of agreeableness and dichotomize\nAsum to 1 or 0 accordingly.\nbfi = @pipe transform(bfi, \n                      :Asum => ByRow(function(x) \n                                        if x >= 20 1 \n                                        else 0 \n                                        end \n                                     end) \n                               => :Abin);\ndescribe(select(bfi, :Abin))\n1×7 DataFrame\n Row │ variable  mean     min    median   max    nmissing  eltype\n     │ Symbol    Float64  Int64  Float64  Int64  Int64     DataType\n─────┼──────────────────────────────────────────────────────────────\n   1 │ Abin      0.70975      0      1.0      1         0  Int64\nNow because our outcome variable is a binary variable, it is more\nappropriate to model it with a binomial distribution and a probit\nlink.\nlm_probit = glm(@formula(Abin ~ Age), bfi, Binomial(), ProbitLink())\nStatsModels.TableRegressionModel{GLM.GeneralizedLinearModel{GLM.GlmResp{Arr\nay{Float64,1},Distributions.Binomial{Float64},GLM.ProbitLink},GLM.DensePred\nChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float\n64,2}}\n\nAbin ~ 1 + Age\n\nCoefficients:\n──────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error     z  Pr(>|z|)   Lower 95%  Upper 95%\n──────────────────────────────────────────────────────────────────────────\n(Intercept)  0.319024    0.0832706   3.83    0.0001  0.155816    0.482231\nAge          0.00798517  0.00269249  2.97    0.0030  0.00270799  0.0132623\n──────────────────────────────────────────────────────────────────────────\nAn equivalent way to specify this probit model is\nfit(GeneralizedLinearModel, @formula(Abin ~ Age), bfi, Binomial(), ProbitLink())\nStatsModels.TableRegressionModel{GLM.GeneralizedLinearModel{GLM.GlmResp{Arr\nay{Float64,1},Distributions.Binomial{Float64},GLM.ProbitLink},GLM.DensePred\nChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float\n64,2}}\n\nAbin ~ 1 + Age\n\nCoefficients:\n──────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error     z  Pr(>|z|)   Lower 95%  Upper 95%\n──────────────────────────────────────────────────────────────────────────\n(Intercept)  0.319024    0.0832706   3.83    0.0001  0.155816    0.482231\nAge          0.00798517  0.00269249  2.97    0.0030  0.00270799  0.0132623\n──────────────────────────────────────────────────────────────────────────\nOur probit model is as follows. \\[\n\\Phi^{-1}(\\text{High_Agreeableness}) = 0.319 + 0.008 \\text{Age}\n\\]\nFor a person of age 20, the predicted probit of scoring high on\nagreeableness is 0.479.\nNote that the coefficients are probability z-scores and the\nprediction is a probit. To translate a probit back to a probability, we\ncould use cdf() in the Distributions\npackage.\nNormal() by default specifies a normal distribution with\na mean of 0 and a standard deviation of 1.\nusing Distributions\ncdf(Normal(), .479)\n0.6840306856730872\nThe predicted probability of someone at age 20 scoring high on\nagreeablness is 68.4%.\nLogistic Regression\nAn alternative way to handle binary outcome variables is through\nlogistic regression.\nlm_logit = glm(@formula(Abin ~ Age), bfi, Binomial(), LogitLink())\nStatsModels.TableRegressionModel{GLM.GeneralizedLinearModel{GLM.GlmResp{Arr\nay{Float64,1},Distributions.Binomial{Float64},GLM.LogitLink},GLM.DensePredC\nhol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float6\n4,2}}\n\nAbin ~ 1 + Age\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error     z  Pr(>|z|)   Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────\n(Intercept)  0.497558   0.139339    3.57    0.0004  0.224458    0.770657\nAge          0.0135925  0.00455195  2.99    0.0028  0.00467084  0.0225141\n─────────────────────────────────────────────────────────────────────────\nSimilarly, an equivalent way to specify this logistic model is\nfit(GeneralizedLinearModel, @formula(Abin ~ Age), bfi, Binomial(), LogitLink())\nStatsModels.TableRegressionModel{GLM.GeneralizedLinearModel{GLM.GlmResp{Arr\nay{Float64,1},Distributions.Binomial{Float64},GLM.LogitLink},GLM.DensePredC\nhol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2}}}},Array{Float6\n4,2}}\n\nAbin ~ 1 + Age\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error     z  Pr(>|z|)   Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────\n(Intercept)  0.497558   0.139339    3.57    0.0004  0.224458    0.770657\nAge          0.0135925  0.00455195  2.99    0.0028  0.00467084  0.0225141\n─────────────────────────────────────────────────────────────────────────\nNow our logistic model is as follows. \\[\nlogit(\\text{High_Agreeableness}) = 0.498 + 0.014 \\text{Age}\n\\]\nFor a person of age 20, the predicted logit of scoring high on\nagreeableness is 0.778. Now the prediction is represented in terms of\nlogit. To make it more interpretable, we can simply exponentiate the\nlogit to get the odds ratio, or use the following formula to get the\nprobability:\n\\[\n\\hat{\\pi} = \\frac{exp(0.498 + 0.014)}{1 + exp(0.498 + 0.014)}\n\\]\nwhere \\hat{\\pi} is the predicted probability of someone\nscoring high on agreeableness.\nexp(.778)/(1 + exp(.778))\n0.6852489081587367\nConsistent with the result from probit regression, the predicted\nprobability of someone at age 20 scoring high on agreeablness is\n68.5%.\nVisualization\nLet’s cover the answers of Follow-up Practice (1) and (2) here.\n# Practice (1)\nlm3 = fit(LinearModel, @formula(Csum ~ Education), bfi)\nStatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1\n}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2\n}}}},Array{Float64,2}}\n\nCsum ~ 1 + Education\n\nCoefficients:\n──────────────────────────────────────────────────────────────────────────\n                Coef.  Std. Error       t  Pr(>|t|)   Lower 95%  Upper 95%\n──────────────────────────────────────────────────────────────────────────\n(Intercept)  18.5942    0.176512   105.34    <1e-99  18.2481     18.9403\nEducation     0.14285   0.0522318    2.73    0.0063   0.0404219   0.245278\n──────────────────────────────────────────────────────────────────────────\n# Practice (2)\nlm4 = fit(LinearModel, @formula(Csum ~ Education*Gender_bin), bfi)\nStatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1\n}},GLM.DensePredChol{Float64,LinearAlgebra.Cholesky{Float64,Array{Float64,2\n}}}},Array{Float64,2}}\n\nCsum ~ 1 + Education + Gender_bin + Education & Gender_bin\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n──────────────\n                                Coef.  Std. Error      t  Pr(>|t|)   Lower \n95%  Upper 95%\n───────────────────────────────────────────────────────────────────────────\n──────────────\n(Intercept)                18.9793      0.287144   66.10    <1e-99  18.4162\n     19.5424\nEducation                   0.0693464   0.0844421   0.82    0.4116  -0.0962\n469   0.23494\nGender_bin: 2              -0.607298    0.364028   -1.67    0.0954  -1.3211\n7     0.106571\nEducation & Gender_bin: 2   0.119855    0.10743     1.12    0.2647  -0.0908\n195   0.330529\n───────────────────────────────────────────────────────────────────────────\n──────────────\nNote that Gender_bin is dummy coded with male (1) as 0\nand female (2) as 1. The default in GLM is dummy coding,\nbut we can specify a different contrast coding. More details on contrast\ncoding can be found [here] (https://juliastats.org/StatsModels.jl/stable/contrasts/).\n# extracting coefficients\nlm3_b0, lm3_b1 = coef(lm3);\nlm4_b0, lm4_b1, lm4_b2, lm4_b3 = coef(lm4);\nWe will use the package Plots for\nvisualizing the relationship between variables in the fitted models. The\npackage StatsPlots\nis an extension of Plots that supports plotting with\nDataFrame objects.\n@df bfi scatter(\n    :Education, \n    :Csum, \n    group = :Gender, \n    legend = :bottomright, \n    label = [\"Male\" \"Female\"], \n    xlabel = \"Education\", \n    ylabel = \"Contientiousness\") \nplot!((x) -> lm3_b0 + lm3_b1 * x, 1, 5, label = \"Linear Model\")\n\n@df bfi scatter(\n    :Education, \n    :Csum, \n    group = :Gender, \n    legend = :bottomright, \n    label = [\"Male\" \"Female\"], \n    xlabel = \"Education\", \n    ylabel = \"Contienciousness\", \n    size=(690,460)) \nplot!((x) -> lm4_b0 + lm4_b1 * x + lm4_b2 * 0 + lm4_b2 * x * 0, \n      1, 5, label = \"Male\", linecolor = \"blue\")\nplot!((x) -> lm4_b0 + lm4_b1 * x + lm4_b2 * 1 + lm4_b2 * x * 1, \n      1, 5, label = \"Female\", linecolor = \"red\")\n\nI also made reference to this webpage\nto create the plots above.\nPerhaps because the “research questions” I posed here were not so\ninteresting, there was not much interesting to observe from the analysis\noutputs and the plots. But I hope you enjoy this tutorial in doing\nregression analyses with Julia :)\n\n\n\n",
    "preview": {},
    "last_modified": "2024-02-19T23:30:40-08:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Measurement & Multilevel Modeling Lab",
    "description": "Welcome to our blog. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "MMM Lab",
        "url": {}
      }
    ],
    "date": "2020-12-17",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2024-02-19T23:30:40-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-25-part-1-covid-19/",
    "title": "Exploring the UAS Survey Part 1: Data exploration (Demography and Risk Items)",
    "description": "Analyze Covid-19 survey conducted by USC in March, 2020.",
    "author": [
      {
        "name": "Haley",
        "url": {}
      }
    ],
    "date": "2020-07-25",
    "categories": [
      "COVID",
      "Visualization",
      "Exploratory Data Analysis",
      "Applied"
    ],
    "contents": "\n\nContents\nIntroduction\nData preparation\nLoading libraries\nLoading data using here package\nData transformation\n\nGender\nAge\nResidence\nEducation\nIncome\nRace\nTime\nSummary\nPublication Statement\n\n[Edit by ML: The styler package was used to style the code. ]\nIntroduction\nAs people in the US learned about the COVID-19 pandemic, their perceived risks related to infection, unemployment, and other risk factors have changed during the early month of the outbreak. In this post, we are interpreting a survey published by the University of Southern California by analyzing and comparing some key features related to risk factors.\nData preparation\nLoading libraries\n\n\nlibrary(tidyverse)\nlibrary(Rmisc)\nlibrary(ggpubr)\nlibrary(ggsci)\n\n\n\nLoading data using here package\n\n\n# Imported the second wave of UAS230 Covid Survey Data.\ncovid19 <- readr::read_csv(here::here(\"data\", \"uas230_march_31_2020.csv\"))\n# The data was collected from a national survey during 10 March,2020 - 31 March,2020.\n\n\n\nData transformation\n\n\n# Create a new dataframe contains only key demographic information and risk items\ndemo <- covid19 %>%\n  select(\n    statereside, gender, end_date, age, education, hhincome, race,\n    cr005, cr009, cr006, cr008a, end_date\n  )\n\ndemo <- na_if(demo, \".a\") # question never seen by the respondents. \n                          # S/he may skip over the question, or never \n                          # view the question due to survey broke off.\ndemo <- na_if(demo, \".e\") # .e -> questions that were asked but not answered\ndemo <- na_if(demo, \".c\") # .c -> if respondent did not complete the survey\n\ndemo <- mutate_at(\n  demo, # convert data type to numeric data\n  vars(cr006, cr005, cr009, cr008a, age),\n  as.numeric\n)\ndemo <- na.omit(demo)\n\n\n\nGender\n58.6% of the respondents are female.\nMale and Female have a similar belief in how infectious Coronavirus is.\nMore male respondents are very optimistic about the risks related to Coronavirus.\n\n\ngen <- demo %>%\n  group_by(gender) %>%\n  dplyr::summarise(g = n())\n\nggplot(gen, aes(x = gender, y = g, fill = gender)) + # Barplot of gender distribution with annotation\n  geom_col() +\n  geom_label(aes(label = g)) +\n  labs(x = \"Gender\", y = \"Count\") +\n  scale_fill_jco() + # jco palette\n  coord_flip() +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\ngender_inf <- demo %>% # Plot perceived infection risk\n  select(gender, cr005) %>%\n  ggplot(aes(x = cr005, color = gender, fill = gender)) +\n  geom_density(alpha = 0.5) +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) +\n  labs(x = \"Perceived risk of infection\")\n\n\ngender_death <- demo %>% # Plot perceived risk of death\n  select(gender, cr006) %>%\n  ggplot(aes(x = cr006, color = gender, fill = gender)) +\n  geom_density(alpha = 0.5) +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) +\n  labs(x = \"Perceived risk of death\", y = \"\")\n\n\ngender_unemploy <- demo %>% # Plot perceived unemployment risk\n  select(gender, cr008a) %>%\n  ggplot(aes(x = cr008a, color = gender, fill = gender)) +\n  geom_density(alpha = 0.5) +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) +\n  labs(x = \"Perceived unemployment risk\", y = \"\")\n\n\ngender_finance <- demo %>% # Plott perceived risk of running out of money\n  select(gender, cr009) %>%\n  ggplot(aes(x = cr009, color = gender, fill = gender)) +\n  geom_density(alpha = 0.5) +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) +\n  labs(x = \"Perceived risk of running out of money\")\n\n\nggarrange(gender_inf, gender_death, gender_finance, gender_unemploy + rremove(\"x.text\"), # Combining gender and risk items in one plot\n  common.legend = TRUE, legend = \"right\",\n  ncol = 2, nrow = 2\n)\n\n\n\n\nAge\nMost respondents are between age 30 to age 60.\nYounger respondents have a higher estimate chance of infection, but a lower estimate chance of death related to Coronavirus.\nElderly respondents have a lower perceived unemployment risk and financial risk related to coronavirus.\n[Two graphs below are edited by ML]\n\n\nggplot(demo, aes(x = age)) +\n  geom_histogram(binwidth = 10, alpha = 0.85, fill = \"deepskyblue3\", color = \"white\") +\n  theme_minimal() +\n  labs(x = \"Age\", y = \"Count\") +\n  scale_x_continuous(breaks = seq(0, 100, by = 10)) +\n  scale_y_continuous(breaks = seq(0, 1400, by = 200))\n\n\n\nggplot(data = demo) + # combined plot 1\n  geom_point(\n    mapping = aes(x = age, y = cr005, color = \"Perceived Risk of Infection\"),\n    alpha = 0.1, size = 0.5\n  ) +\n  geom_point(\n    mapping = aes(x = age, y = cr006, color = \"Perceived Risk of Death\"),\n    alpha = 0.1, size = 0.5\n  ) +\n  geom_smooth(mapping = aes(x = age, y = cr005, color = \"Perceived Risk of Infection\")) +\n  geom_smooth(mapping = aes(x = age, y = cr006, color = \"Perceived Risk of Death\")) +\n  labs(y = \"Perceived Risk (%)\", x = \"Age\") +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) +\n  scale_x_continuous(breaks = seq(0, 100, by = 10))\n\n\n\nggplot(data = demo) + # combined plot 2\n  geom_point(\n    mapping = aes(x = age, y = cr008a, color = \"Perceived Unemployment Risk\"),\n    alpha = 0.1, size = 0.5\n  ) +\n  geom_point(\n    mapping = aes(x = age, y = cr009, color = \"Perceived Financial Risk\"),\n    alpha = 0.1, size = 0.5\n  ) +\n  geom_smooth(mapping = aes(x = age, y = cr008a, color = \"Perceived Unemployment Risk\")) +\n  geom_smooth(mapping = aes(x = age, y = cr009, color = \"Perceived Financial Risk\")) +\n  labs(y = \"Perceived Risk (%)\", x = \"Age\") +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) +\n  scale_x_continuous(breaks = seq(15, 95, by = 10))\n\n\n\n\nResidence\nThe number of respondents from California is significantly higher than number of respondents from any other states.\nRespondents in New York, Pennsylvania, and Michigan report a lower perceived fianancial risk related to Coronavirus.\nWhile respondents in California, Illnois, and Texas report a higher perceived unemployment risk and financial risk related to Coronavirus.\n[ML: Interestingly, it looks like Californian perceived higher risks consistently than those in other states.]\n\n\ndemo_state <- demo %>% # add counts for each state:\n  add_count(statereside, name = \"n\") %>% # use New York as cutline, filter out 8 states with n >= 115\n  filter(n >= 115)\n\n\nreside <- demo_state %>%\n  group_by(statereside) %>%\n  dplyr::summarise(re = n())\n\nggplot(reside, aes(x = statereside, y = re, fill = statereside)) + # Barplot of gender distribution with annotation\n  geom_col() +\n  geom_label(aes(label = re), fill = \"white\") +\n  labs(x = \"State Reside\", y = \"Count\") +\n  scale_fill_jco() +\n  coord_flip() +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\ndeath_state <- ggplot(demo_state, aes(x = cr006, fill = statereside)) + # Plot perceived risk of death\n  geom_density(alpha = 0.3) +\n  theme_minimal() +\n  labs(y = \"\", x = \"Perceived risk of death\") +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10))\n\ninf_state <- ggplot(demo_state, aes(x = cr005, fill = statereside)) + # Plot perceived risk of infection\n  geom_density(alpha = 0.3) +\n  theme_minimal() +\n  labs(x = \"Perceived risk of infection\") +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10))\n\nfinance_state <- ggplot(demo_state, aes(x = cr009, fill = statereside)) + # Plot perceived financial risk\n  geom_density(alpha = 0.3) +\n  theme_minimal() +\n  labs(y = \"\", x = \"Perceived financial risk\") +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10))\n\n\nunemploy_state <- ggplot(demo_state, aes(x = cr008a, fill = statereside)) + # Plot perceived unemployment risk\n  geom_density(alpha = 0.3) +\n  theme_minimal() +\n  labs(x = \"Perceived unemployment risk\") +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10))\n\n\nggarrange(inf_state, death_state, finance_state, unemploy_state, # combine the plots\n  common.legend = TRUE, legend = \"right\",\n  ncol = 2, nrow = 2\n)\n\n\n\nfinance_state_stat <- desc_statby(demo_state, measure.var = \"cr009\", grps = \"statereside\") # Summary table for perceived risk of financial hardship by states\nfinance_state_stat <- finance_state_stat[, c(\"statereside\", \"length\", \"mean\", \"sd\")] %>%\n  arrange(-mean) # sort by mean in decending order\n\nfinance_state_stat$mean <- round(finance_state_stat$mean, 2) # round to 2 decimal places\nfinance_state_stat$sd <- round(finance_state_stat$sd, 2)\n\n\nfinance_state_stat <- ggtexttable(finance_state_stat,\n  rows = NULL,\n  theme = ttheme(\"mOrange\")\n) # Summary table plot, medium orange theme\n\n\nunemploy_state_stat <- desc_statby(demo_state, measure.var = \"cr008a\", grps = \"statereside\") # Summary table for perceived risk of unemployment by states\nunemploy_state_stat <- unemploy_state_stat[, c(\"statereside\", \"length\", \"mean\", \"sd\")] %>%\n  arrange(-mean)\n\nunemploy_state_stat$mean <- round(unemploy_state_stat$mean, 2) # round to 2 decimal places\nunemploy_state_stat$sd <- round(unemploy_state_stat$sd, 2)\n\nunemploy_state_stat <- ggtexttable(unemploy_state_stat,\n  rows = NULL,\n  theme = ttheme(\"mOrange\")\n)\n\ntext_2 <- paste(\"Perceived risk of unemployment\", sep = \" \") # Create text to annotate summary tables\ntext_1 <- paste(\"Perceived risk of financial hardship\", sep = \" \")\ntext.p1 <- ggparagraph(text = text_1, face = \"italic\", size = 11, color = \"black\")\ntext.p2 <- ggparagraph(text = text_2, face = \"italic\", size = 11, color = \"black\")\n\nggarrange(text.p1, text.p2, finance_state_stat, unemploy_state_stat, # combine the plots\n  heights = c(0.1, 2),\n  ncol = 2, nrow = 2\n)\n\n\n\n\nEducation\nRespondents with a higher education degree report a higher perceived risk of infection, but lower perceived risk of death.\nRespondents with a lower educaton degree report a higher perceived unemployment risk and financial risk.\n\n\ndemo <- demo %>% # recode education level into more organzied groups\n  mutate(tidyEdu = recode(education,\n    `1 Less than 1st grade` = \"Less than HS\",\n    `10 Some college-no degree` = \"Some College\",\n    `11 Assoc. college degree-occ/voc prog` = \"Associate\",\n    `12 Assoc. college degree-academic prog` = \"Associate\",\n    `13 Bachelor's degree` = \"College/professional\",\n    `14 Master's degree` = \"Master/PhD\",\n    `15 Professional school degree` = \"College/professional\",\n    `16 Doctorate degree` = \"Master/PhD\",\n    `2 Up to 4th grade` = \"Less than HS\",\n    `3 5th or 6th grade` = \"Less than HS\",\n    `4 7th or 8th grade` = \"Less than HS\",\n    `5 9th grade` = \"Less than HS\",\n    `6 10th grade` = \"Less than HS\",\n    `7 11th grade` = \"Less than HS\",\n    `8 12th grade-no diploma` = \"Less than HS\",\n    `9 High school graduate or GED` = \"HS\"\n  ))\n\n\nedu <- demo %>%\n  group_by(tidyEdu) %>%\n  dplyr::summarise(e = n())\n\nggplot(edu, aes(x = tidyEdu, y = e, fill = tidyEdu)) + # Barplot of education distribution with annotation\n  geom_col() +\n  geom_label(aes(label = e), fill = \"white\") +\n  labs(x = \"Education\", y = \"Count\") +\n  scale_fill_jco() +\n  coord_flip() +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  scale_x_discrete(limits = c(\"Less than HS\", \"HS\", \"Some College\", \"Associate\", \"College/professional\", \"Master/PhD\")) # fixed order\n\n\n\n# summarize basic statistic using Rmisc package\nvis_death <- summarySE(demo, measurevar = \"cr006\", groupvars = c(\"tidyEdu\"))\n\nedu_2 <- ggplot(vis_death, aes(x = tidyEdu, y = cr006, fill = tidyEdu)) + # Plot Perceived risk of death\n  geom_bar(stat = \"identity\", width = .5) +\n  scale_fill_jco() +\n  theme_minimal() +\n  coord_flip() +\n  labs(x = \"\", y = \"Perceived risk of death (%)\") +\n  scale_x_discrete(limits = c(\"Less than HS\", \"HS\", \"Some College\", \"Associate\", \"College/professional\", \"Master/PhD\")) +\n  scale_y_continuous(breaks = seq(0, 30, by = 5))\n\n\nvis_inf <- summarySE(demo, measurevar = \"cr005\", groupvars = c(\"tidyEdu\")) # Plot Perceived risk of infection\n\nedu_1 <- ggplot(vis_inf, aes(x = tidyEdu, y = cr005, fill = tidyEdu)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  scale_fill_jco() +\n  coord_flip() +\n  labs(x = \"\", y = \"Perceived risk of infection (%)\") +\n  scale_x_discrete(limits = c(\"Less than HS\", \"HS\", \"Some College\", \"Associate\", \"College/professional\", \"Master/PhD\")) +\n  scale_y_continuous(breaks = seq(0, 30, by = 5))\n\n# Perceived unemployment risk (cr008a) by education\nvis_umemploy <- summarySE(demo, measurevar = \"cr008a\", groupvars = c(\"tidyEdu\"))\n\nedu_4 <- ggplot(vis_umemploy, aes(x = tidyEdu, y = cr008a, fill = tidyEdu)) + # Plot Perceived risk of unemployment\n  geom_bar(stat = \"identity\", width = .5) +\n  coord_flip() +\n  scale_fill_jco() +\n  theme_minimal() +\n  labs(x = \"\", y = \"Perceived risk of unemployment (%)\") +\n  scale_x_discrete(limits = c(\"Less than HS\", \"HS\", \"Some College\", \"Associate\", \"College/professional\", \"Master/PhD\")) +\n  scale_y_continuous(breaks = seq(0, 30, by = 5))\n\n# Perceived financial risk (cr009) by education\nvis_financial <- summarySE(demo, measurevar = \"cr009\", groupvars = c(\"tidyEdu\"))\n\nedu_3 <- ggplot(vis_financial, aes(x = tidyEdu, y = cr009, fill = tidyEdu)) + # Plot Perceived risk of running out of money\n  geom_bar(stat = \"identity\", width = .5) +\n  coord_flip() +\n  scale_fill_jco() +\n  theme_minimal() +\n  labs(x = \"\", y = \"Perceived risk of running out of money (%)\") +\n  scale_x_discrete(limits = c(\"Less than HS\", \"HS\", \"Some College\", \"Associate\", \"College/professional\", \"Master/PhD\")) +\n  scale_y_continuous(breaks = seq(0, 30, by = 5))\n\nggarrange(edu_1, edu_2, edu_3, edu_4, # Combine plots\n  common.legend = TRUE, legend = \"none\",\n  ncol = 2, nrow = 2\n)\n\n\n\n\nIncome\nHigh income respondents report a higher perceived risk of infection, but a lower perceived risk of death.\nLow income respondents report a higher perceived unemployment risk and financial risk.\n\n\ndemo <- demo %>%\n  mutate(incomeTidy = recode(hhincome,\n    `1 Less than $5,000` = \"under 20k\",\n    `3 7,500 to 9,999` = \"under 20k\",\n    `2 5,000 to 7,499` = \"under 20k\",\n    `4 10,000 to 12,499` = \"under 20k\",\n    `5 12,500 to 14,999` = \"under 20k\",\n    `6 15,000 to 19,999` = \"under 20k\",\n    `7 20,000 to 24,999` = \"20-39k\",\n    `8 25,000 to 29,999` = \"20-39k\",\n    `9 30,000 to 34,999` = \"20-39k\",\n    `10 35,000 to 39,999` = \"20-39k\",\n    `11 40,000 to 49,999` = \"40-59k\",\n    `12 50,000 to 59,999` = \"40-59k\",\n    `13 60,000 to 74,999` = \"60-74k\",\n    `14 75,000 to 99,999` = \"75-99k\",\n    `15 100,000 to 149,999` = \"100-149k\",\n    `16 150,000 or more` = \"150k plus\"\n  ))\n\ntemp_inc <- demo %>%\n  select(incomeTidy) %>%\n  group_by(incomeTidy) %>%\n  dplyr::summarise(cnt = n())\n\nggplot(temp_inc, aes(x = incomeTidy, y = cnt, fill = incomeTidy)) +\n  geom_col() +\n  geom_label(aes(label = cnt), fill = \"white\") +\n  labs(x = \"Income\", y = \"Count\") +\n  scale_fill_jco() +\n  theme_bw() +\n  coord_flip() +\n  theme(legend.position = \"none\") +\n  scale_x_discrete(limits = c(\"under 20k\", \"20-39k\", \"40-59k\", \"60-74k\", \"75-99k\", \"100-149k\", \"150k plus\"))\n\n\n\n# Perceived risk of infection (cr005) by Income\ninc_infection <- summarySE(demo, measurevar = \"cr005\", groupvars = c(\"incomeTidy\"))\n\ninc_1 <- ggplot(inc_infection, aes(x = incomeTidy, y = cr005, fill = incomeTidy)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  scale_fill_jco() +\n  coord_flip() +\n  labs(x = \"\", y = \"Perceived risk of infection (%)\") +\n  scale_x_discrete(limits = c(\"under 20k\", \"20-39k\", \"40-59k\", \"60-74k\", \"75-99k\", \"100-149k\", \"150k plus\"))\n\n# Perceived risk of death (cr006) by Education\ninc_death <- summarySE(demo, measurevar = \"cr006\", groupvars = c(\"incomeTidy\"))\n\ninc_2 <- ggplot(inc_death, aes(x = incomeTidy, y = cr006, fill = incomeTidy)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  coord_flip() +\n  scale_fill_jco() +\n  labs(x = \"\", y = \"Perceived risk of death (%)\") +\n  scale_x_discrete(limits = c(\"under 20k\", \"20-39k\", \"40-59k\", \"60-74k\", \"75-99k\", \"100-149k\", \"150k plus\"))\n\n# Perceived unemployment risk (cr008a) by Income\ninc_unemploy <- summarySE(demo, measurevar = \"cr008a\", groupvars = c(\"incomeTidy\"))\n\ninc_4 <- ggplot(inc_unemploy, aes(x = incomeTidy, y = cr008a, fill = incomeTidy)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  coord_flip() +\n  scale_fill_jco() +\n  labs(x = \"\", y = \"Perceived risk of unemployment (%)\") +\n  scale_x_discrete(limits = c(\"under 20k\", \"20-39k\", \"40-59k\", \"60-74k\", \"75-99k\", \"100-149k\", \"150k plus\"))\n\n# Perceived financial risk (cr009) by Income\ninc_unemploy <- summarySE(demo, measurevar = \"cr009\", groupvars = c(\"incomeTidy\"))\n\n# Create visualization, order the bar by education level\ninc_3 <- ggplot(inc_unemploy, aes(x = incomeTidy, y = cr009, fill = incomeTidy)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  coord_flip() +\n  scale_fill_jco() +\n  labs(x = \"\", y = \"Perceived risk of running out of money (%)\") +\n  scale_x_discrete(limits = c(\"under 20k\", \"20-39k\", \"40-59k\", \"60-74k\", \"75-99k\", \"100-149k\", \"150k plus\"))\n\nggarrange(inc_1, inc_2, inc_3, inc_4,\n  common.legend = TRUE, legend = \"none\",\n  ncol = 2, nrow = 2\n)\n\n\n\n\nRace\nCompare to other races, Black American reported a lower perceived risk of infection, but a higher perceived financial risk, and perceived risk of death.\n\n\nrace_total <- demo %>%\n  select(race) %>%\n  group_by(race) %>%\n  filter(race %in% c(\"1 White Only\", \"2 Black Only\", \"4 Asian Only\", \"6 Mixed\")) %>%\n  dplyr::summarise(r = n())\n\n# Plot race distribution with annotation\nggplot(race_total, aes(x = race, y = r, fill = race)) +\n  geom_col() +\n  geom_label(aes(label = r), fill = \"white\") +\n  labs(x = \"Race\", y = \"Count\") +\n  scale_fill_jco() +\n  coord_flip() +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\n# filter out the racial groups n >200\ndemo_race <- demo %>%\n  select(race, cr005, cr006, cr008a, cr009) %>%\n  filter(race %in% c(\"1 White Only\", \"2 Black Only\", \"4 Asian Only\", \"6 Mixed\"))\n\n# Perceived risk of infection (cr005) by Race\nrace_infection <- summarySE(demo_race, measurevar = \"cr005\", groupvars = c(\"race\"))\n\nrace_1 <- ggplot(race_infection, aes(x = race, y = cr005, fill = race)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  coord_flip() +\n  scale_fill_jco() +\n  labs(x = \"\", y = \"Perceived risk of infection (%)\")\n\n# Perceived risk of death (cr006) by Race\nrace_death <- summarySE(demo_race, measurevar = \"cr006\", groupvars = c(\"race\"))\n\nrace_2 <- ggplot(race_death, aes(x = race, y = cr006, fill = race)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  coord_flip() +\n  scale_fill_jco() +\n  labs(x = \"\", y = \"Perceived risk of death (%)\")\n\n# Perceived unemployment risk (cr008a) by Race\nrace_unemploy <- summarySE(demo_race, measurevar = \"cr008a\", groupvars = c(\"race\"))\n\nrace_4 <- ggplot(race_unemploy, aes(x = race, y = cr008a, fill = race)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  coord_flip() +\n  scale_fill_jco() +\n  labs(x = \"\", y = \"Perceived risk of unemployment (%)\")\n\n# Perceived financial risk (cr009) by Race\nrace_finance <- summarySE(demo_race, measurevar = \"cr009\", groupvars = c(\"race\"))\n\nrace_3 <- ggplot(race_finance, aes(x = race, y = cr009, fill = race)) +\n  geom_bar(stat = \"identity\", width = .5) +\n  theme_minimal() +\n  coord_flip() +\n  scale_fill_jco() +\n  labs(x = \"\", y = \"Perceived risk of running out of money (%)\")\n\nggarrange(race_1, race_2, race_3, race_4,\n  common.legend = TRUE, legend = \"none\",\n  ncol = 2, nrow = 2\n)\n\n\n\n\nTime\nPerceived financial risk and unemployment risk appear to peak at the late March, within the timeframe of this survey.\nAn increasing perceived risk of infection and death related to Coronavirus were noted during March, 2020.\n\n\ndemo$end_date <- as.Date(demo$end_date, format = \"%d%b%Y\") # convert date to the right format\n\ntime_stat <- demo %>% # Calculate daily average value for each risk items to create line chart\n  select(cr005, cr006, cr009, cr008a, end_date) %>%\n  group_by(end_date) %>%\n  dplyr::summarise(n = n(), infection = mean(cr005), death = mean(cr006), unemployment = mean(cr008a), financial = mean(cr009))\n\nt1 <- ggplot(data = time_stat) + # Line chart 1\n  geom_line(aes(end_date, infection, color = \"Perceived risk of infection\")) +\n  geom_line(aes(end_date, death, color = \"Perceived risk of death\")) +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) + # hide legend text, and adjust legend font size\n  labs(y = \"Perceived Risk (%)\", x = \"Survey Date\")\n\n\nt2 <- ggplot(data = time_stat) +\n  geom_line(aes(end_date, financial, color = \"Perceived financial risk\")) + # Line chart2\n  geom_line(aes(end_date, unemployment, color = \"Perceived unemployment risk\")) +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) +\n  labs(y = \"Perceived Risk (%)\", x = \"Survey Date\")\n\n\nt3 <- ggplot(demo) +\n  geom_smooth(aes(end_date, cr005, color = \"Perceived financial risk\")) + # geom_smooth plot 1\n  geom_smooth(aes(end_date, cr006, color = \"Perceived risk of death\")) +\n  theme_bw() +\n  labs(y = \"\", x = \"Survey Date\")\n\nt4 <- ggplot(demo) +\n  geom_smooth(aes(end_date, cr008a, color = \"Perceived unemployment risk\")) + # geom_smooth plot 1\n  geom_smooth(aes(end_date, cr009, color = \"Perceived financial risk\")) +\n  theme(legend.title = element_blank(), legend.text = element_text(size = 10)) +\n  theme_bw() +\n  labs(y = \"\", x = \"Survey Date\")\n\n\nt5 <- ggplot(demo, aes(end_date, cr005)) +\n  geom_jitter(size = 1) +\n  theme_bw() +\n  geom_smooth() +\n  labs(x = \"\")\n\nt6 <- ggplot(demo, aes(end_date, cr006)) +\n  geom_jitter(size = 1) +\n  theme_bw() +\n  geom_smooth() +\n  labs(x = \"\")\n\nt7 <- ggplot(demo, aes(end_date, cr008a)) +\n  geom_jitter(size = 1) +\n  theme_bw() +\n  geom_smooth()\n\nt8 <- ggplot(demo, aes(end_date, cr009)) +\n  geom_jitter(size = 1) +\n  theme_bw() +\n  geom_smooth()\n\n\nggarrange(t1, t3,\n  ncol = 2, nrow = 1,\n  common.legend = TRUE, legend = \"top\"\n)\n\n\n\nggarrange(t2, t4,\n  ncol = 2, nrow = 1,\n  common.legend = TRUE, legend = \"top\"\n)\n\n\n\n# ggarrange(t5, t6,t7, t8,\n#           ncol = 2, nrow = 2,\n#           common.legend = TRUE, legend = \"top\")\n\n\n\n[The graph below is added by ML]\n\n\n# Edit by ML: Showing the variability in responses across time\nggplot(\n  demo,\n  aes(end_date, cr008a)\n) +\n  geom_jitter(width = 0.1, height = 0.5, alpha = 0.1, size = 0.5) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_bw() +\n  geom_smooth(linetype = \"dotted\") +\n  labs(y = \"Perceived Financial Risk (%)\")\n\n\n\n\nSummary\nThe survey was conducted in the early month of the COVID-19 outbreak in the US. There was a large variation in how people evaluate risks related to Coronavirus, but the findings suggested that people perception of risks experienced a significant increase compare to the beginning of the survey.\nPublication Statement\nThe analysis described in this page relies on data from survey(s) administered by the Understanding America Study, which is maintained by the Center for Economic and Social Research (CESR) at the University of Southern California. The content of this page is solely the responsibility of the authors and does not necessarily represent the official views of USC or UAS. Journalists and other reporters must refer readers to methodology information available at <uasdata.usc.edu> or election.uas.edu as appropriate.\n\n\n\n",
    "preview": "posts/2020-07-25-part-1-covid-19/covid_p1_files/figure-html5/gender-1.png",
    "last_modified": "2024-02-19T23:30:40-08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
