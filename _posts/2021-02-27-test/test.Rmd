---
title: "test"
description: |
  A short description of the post.
author:
  - name: Winnie Wing-Yee Tse
    url: {}
date: 02-27-2021
output:
  distill::distill_article:
    self_contained: false
---

# Setup

Before we start, I encourage you to uncomment the codes below and install the 
required packages that you do not have for this tutorial. 

```{julia}
# uncomment below to install packages
#=using Pkg
Pkg.add(["RDatasets", "Pipe", "DataFrames", "GLM",
        "Plots", "StatPlots"])
=#
  1+1
```

Prior to running regression analyses, let me briefly go over three essential packages, 
`Pipe`, `RDatasets`, `DataFrames`, in handling data frames in Julia. 


### Brief Introduction to the Package `Pipe`

The package `Pipe` improves the base pipe operator (`|>`) in Julia. It allows
you to represent the object that is piped from with an underscore `_` in the
functions that follow. For example, `sqrt(sum([1, 2, 3]))` involve two commands -- 
one applied to `[1, 2, 3]` and the other one applied to `sum([1, 2, 3])`. We can
pipe these commands, stemming from the `[1, 2, 3]` object, using `@pipe` together
with `|>`, as illustrated below. 

```{julia}
using Pipe
@pipe [1, 2, 3] |> sum(_) |> sqrt(_)
```

The above code might have overkilled this simple task of taking a sqaure root of a sum, 
but piping will become very useful in handling a long series of commands. 
For more details of `Pipe`, please read the julia tutorial on [pipe]
(https://syl1.gitbook.io/julia-language-a-concise-tutorial/useful-packages/pipe). 

### Brief Introduction to the Package `RDatasets`

The package [`RDatasets`](https://github.com/JuliaStats/RDatasets.jl) 
provides most of the base R datasets for Julia users
to play around. We will be using the dataset `bfi` from the R package `psych`
in this tutorial to explore regression analyses in Julia. 

Let's read in the dataset `bfi` and drop out missing values. 

```{julia}
using RDatasets
# show(RDatasets.datasets("psych"), allrows=true)
bfi = @pipe dataset("psych", "bfi") |> dropmissing(_)
println(names(bfi))
```

For a complete R documentation of this dataset, please refer to [this]
(https://www.personality-project.org/r/html/bfi.html). 


### Brief Introduction to the Package `DataFrames`

For simplicity, let's wrangle our data and sum over the scores on the columns 
for agreeableness (`A1` to `A5`) and for contienciousness (`C1` to `C5`). 
`Gender` is a numeric variable in the original dataset, while it should
better be treated as a binary variable. Let's also convert it into a categorical
array using `categorical`. 

`transform()` has similar function as `mutate()` in R, with which we can transform
a column and save it into a new column in a dataframe. The operator `=>` is pretty
intuitive. In the first transform statment down below, we are basically asking Julia to 
take the columns between `A1` and `A5` in `bfi`, transform them by summing scores 
on the same row (`=> ByRow(+)`), and then save these transformed data into a new column 
called `Asum`. 

Note that to call a column in a dataframe, we need to place an colon `:` before
the column name. 

```julia
using DataFrames
bfi = @pipe bfi |> 
            transform(_, Between(:A1, :A5) => ByRow(+) => :Asum) |> 
            transform(_, Between(:C1, :C5) => ByRow(+) => :Csum) |>
            transform(_, :Gender => categorical => :Gender_bin)
describe(select(bfi, Between(:Asum, :Gender_bin)))
```

```julia
@pipe bfi |>
      select(_, Between(:Asum, :Gender_bin)) |>
      describe(_)
```

`describe()` provides summary statistics of the selected columns of our newly 
created variables from `Asum` to `Gender_bin`. As can be seen in the summary
table, `Gender_bin` has a data type of `CategoricalValue{Int64,UInt32}`. 
Through that way in our following regression analysis, gender will be treated as 
a categorical variable and automatically dummy coded. 

For data wrangling in Julia using the package `Dataframes`, a [cheatsheet]
(https://ahsmart.com/assets/pages/data-wrangling-with-data-frames-jl-cheat-sheet/DataFramesCheatSheet_v0.22_rev1.pdf) 
may come in handy here. 
I am a beginner Julia and `Dataframes` user. If you have a better way to
optimize my codes, please let me know and I will be more than happy to learn it!

---

# Linear Regression

Below are some summary statistics and a snapshot of the variables of interest for 
the following analyses.

```julia
describe(select(bfi, Between(:Education, :Gender_bin)))
```

```julia
first(select(bfi, Between(:Education, :Gender_bin)), 5)
```

In our sample, do older people tend to be more agreeable than younger people?
We can fit a linear model with `Age` as the predictor and `Asum`, the sum scores
of agreeableness, as the outcome variable, and determine whether there is a 
positive association between these two variables. 

The package [`GLM`](https://juliastats.org/GLM.jl/stable/) provides convenient 
ways that are analogous to the R functions `lm()` and `glm()` to specify linear 
models in Julia. 

To build a linear model, we use `lm()`, specify the model equation within
`@formula()`, and indicate the dataset after the formula. 

```julia
using GLM
lm1 = lm(@formula(Asum ~ Age), bfi)
```

Unlike R, we do not need to additionally summarize `lm1` to get a summary table
of the outputs. 

Equivalent to `lm()`, we can use the `fit()` function with the `LinearModel` argument. 

```julia
fit1 = fit(LinearModel, @formula(Asum ~ Age), bfi)
```

`lm(...)` and `fit(LinearModel, ...)` do not differ in their 
type, output, and functionality. A quick check on their type:

```julia
typeof(lm1)
```

```julia
typeof(fit1)
```

which are essentially the same. 

## Extracting Model Information

We can use `coef()`, `stderror()`, and `vcov()` to extract coefficient estimates and 
standard errors of the coefficents, and the estimated variance-covariance matrix
of the coefficient estimates, respectively. 

```julia
coef(lm1)
```

```julia
stderror(lm1)
```

```julia
vcov(lm1)
```

To obtain statistics for model fit, we can use `r2` for ``R^2`` and
`deviance()` for the weighted residual sum of squares. 

```julia
r2(lm1)
```

```julia
deviance(lm1)
```

`predict()` computes the predicted values of each individual in our sample. 

```julia
predict(lm1)
```

!!! note "Follow-up Practice (1)"
Let's try to run a linear model to investigate whether contiouenciousness 
is predicted by education. How much variance is explained by this model?

```julia
lm3 = fit(LinearModel, @formula(Csum ~ Education), bfi)
r2(lm3)
```

---

# Multiple Regression

We may suspect that the difference in agreeableness over years of age depend on
someone's education level. To investigate this, we can add an interation between 
`Age` and `Education` on `Asum` by `Age & Education`. Typically when we add an 
interaction term to a model, we include also the main effects of the variables. 
A shorthand of specifying both the main effects and interaction between the 
variables is `Age * Education`. 

```julia
# the models below are equivalent
lm2a = lm(@formula(Asum ~ Age + Education + Age & Education), bfi)
```

```julia
lm2b = lm(@formula(Asum ~ Age * Education), bfi)
```

!!! note "Follow-up Practice (2)"
Let's fit a model to explore whether there is an interaction between education
and gender (`Gender_bin`) on contienciousness. How much did the variance explained by 
the model increasecompared to the model without the interaction (in reference 
to Practice (1)) increase?  

---

# Probit Regression

For the illustrative purpose, I arbitrarily define that a sum score above 20 out of 30
is a high score of agreeableness and dichotomize `Asum` to 1 or 0 accordingly. 

```julia
bfi = @pipe transform(bfi, 
                      :Asum => ByRow(function(x) 
                                        if x >= 20 1 
                                        else 0 
                                        end 
                                     end) 
                               => :Abin);
describe(select(bfi, :Abin))
```

Now because our outcome variable is a binary variable, it is more appropriate
to model it with a binomial distribution and a probit link. 

```julia
lm_probit = glm(@formula(Abin ~ Age), bfi, Binomial(), ProbitLink())
```

An equivalent way to specify this probit model is 

```julia
fit(GeneralizedLinearModel, @formula(Abin ~ Age), bfi, Binomial(), ProbitLink())
```

Our probit model is as follows. 
```math
\Phi^{-1}(\text{High_Agreeableness}) = 0.319 + 0.008 \text{Age}
```

For a person of age 20, the predicted probit of scoring high on agreeableness is
0.479. 

Note that the coefficients are probability z-scores and the prediction is
a probit. To translate a probit back to a probability, we could use `cdf()` in the 
`Distributions` package. 

`Normal()` by default specifies a normal distribution with a mean of 0 and a standard 
deviation of 1. 

```julia
using Distributions
cdf(Normal(), .479)
```

The predicted probability of someone at age 20 scoring high on agreeablness is 68.4%. 

---

# Logistic Regression

An alternative way to handle binary outcome variables is through logistic regression. 

```julia
lm_logit = glm(@formula(Abin ~ Age), bfi, Binomial(), LogitLink())
```

Similarly, an equivalent way to specify this logistic model is 

```julia
fit(GeneralizedLinearModel, @formula(Abin ~ Age), bfi, Binomial(), LogitLink())
```

Now our logistic model is as follows. 
```math
logit(\text{High_Agreeableness}) = 0.498 + 0.014 \text{Age}
```

For a person of age 20, the predicted logit of scoring high on agreeableness is
0.778. Now the prediction is represented in terms of logit. To make it more interpretable, 
we can simply exponentiate the logit to get the odds ratio, or use the following formula
to get the probability:

```math
\hat{\pi} = \frac{exp(0.498 + 0.014)}{1 + exp(0.498 + 0.014)}
```

where ``\hat{\pi}`` is the predicted probability of someone scoring high on agreeableness. 

```julia
exp(.778)/(1 + exp(.778))
```

Consistent with the result from probit regression, the predicted probability of someone 
at age 20 scoring high on agreeablness is 68.5%. 

---

# Visualization

Let's cover the answers of Follow-up Practice (1) and (2) here. 

```julia
# Practice (1)
lm3 = fit(LinearModel, @formula(Csum ~ Education), bfi)
```

```julia
# Practice (2)
lm4 = fit(LinearModel, @formula(Csum ~ Education*Gender_bin), bfi)
```

Note that `Gender_bin` is dummy coded with male (1) as 0 and female (2) as 1. 
The default in `GLM` is dummy coding, but we can specify a different 
contrast coding. More details on contrast coding can be found [here]
(https://juliastats.org/StatsModels.jl/stable/contrasts/).  

```julia
# extracting coefficients
lm3_b0, lm3_b1 = coef(lm3);
lm4_b0, lm4_b1, lm4_b2, lm4_b3 = coef(lm4);
```

We will use the package [`Plots`](http://docs.juliaplots.org/latest/)
for visualizing the relationship between variables in the fitted models. 
The package [`StatsPlots`](http://docs.juliaplots.org/latest/contributing/#StatsPlots) 
is an extension of `Plots` that supports plotting with `DataFrame` objects. 

```julia
using Plots, StatsPlots
@df bfi scatter(
    :Education, 
    :Csum, 
    group = :Gender, 
    legend = :bottomright, 
    label = ["Male" "Female"], 
    xlabel = "Education", 
    ylabel = "Contientiousness") 
plot!((x) -> lm3_b0 + lm3_b1 * x, 1, 5, label = "Linear Model")
```

```julia
@df bfi scatter(
    :Education, 
    :Csum, 
    group = :Gender, 
    legend = :bottomright, 
    label = ["Male" "Female"], 
    xlabel = "Education", 
    ylabel = "Contienciousness", 
    size = (690,460)) 
plot!((x) -> lm4_b0 + lm4_b1 * x + lm4_b2 * 0 + lm4_b2 * x * 0, 
      1, 5, label = "Male", linecolor = "blue")
plot!((x) -> lm4_b0 + lm4_b1 * x + lm4_b2 * 1 + lm4_b2 * x * 1, 
      1, 5, label = "Female", linecolor = "red")   
```

I also made reference to this [webpage](https://nextjournal.com/DeepLearningNotes/Ch03LinearRegression)
to create the plots above. 

Perhaps because the "research questions" I posed here were not so interesting, 
there was not much interesting to observe from the analysis outputs and the
plots. But I hope you enjoy this tutorial in doing regression analyses with Julia :)